{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">      Training Configuration      </span>\n",
       "┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Parameter        </span>┃<span style=\"font-weight: bold\"> Value       </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> num_batches      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 100000      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> batch_size       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 4           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> grad_accum_every </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 4           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> learning_rate    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.001       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> validate_every   </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 100         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> prime_length     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 128         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> generate_every   </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 500         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> generate_length  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 512         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> seq_len          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 512         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> use_amp          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> True        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> use_parametrize  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> True        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> checkpoint_dir   </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> checkpoints </span>│\n",
       "└──────────────────┴─────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m      Training Configuration      \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mParameter       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mnum_batches     \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m100000     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mbatch_size      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m4          \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mgrad_accum_every\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m4          \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mlearning_rate   \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.001      \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mvalidate_every  \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m100        \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mprime_length    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m128        \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mgenerate_every  \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m500        \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mgenerate_length \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m512        \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mseq_len         \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m512        \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36muse_amp         \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrue       \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36muse_parametrize \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrue       \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mcheckpoint_dir  \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mcheckpoints\u001b[0m\u001b[32m \u001b[0m│\n",
       "└──────────────────┴─────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Start training with these parameters? <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">[y/n]</span>: </pre>\n"
      ],
      "text/plain": [
       "Start training with these parameters? \u001b[1;35m[y/n]\u001b[0m: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/ath/miniconda3/envs/nGPT/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/ath/miniconda3/envs/nGPT/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Validation loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.3751</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Validation loss: \u001b[1;36m5.3751\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Saved new best checkpoint!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mSaved new best checkpoint!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import gzip\n",
    "import random\n",
    "import numpy as np\n",
    "from contextlib import nullcontext\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch import Tensor\n",
    "from torch.amp import GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "from nGPT_pytorch import nGPT\n",
    "from rich.console import Console\n",
    "from rich.progress import Progress, SpinnerColumn, TimeElapsedColumn\n",
    "from rich.prompt import Confirm\n",
    "from rich.table import Table\n",
    "from rich.live import Live\n",
    "from rich.panel import Panel\n",
    "\n",
    "# Training configuration\n",
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        self.num_batches = int(1e5)\n",
    "        self.batch_size = 4\n",
    "        self.grad_accum_every = 4\n",
    "        self.learning_rate = 1e-3\n",
    "        self.validate_every = 100\n",
    "        self.prime_length = 128\n",
    "        self.generate_every = 500\n",
    "        self.generate_length = 512\n",
    "        self.seq_len = 512\n",
    "        self.use_amp = True\n",
    "        self.use_parametrize = True\n",
    "        self.checkpoint_dir = \"checkpoints\"\n",
    "        \n",
    "    def save(self, filename: str):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.__dict__, f, indent=2)\n",
    "            \n",
    "    @classmethod\n",
    "    def load(cls, filename: str) -> 'TrainingConfig':\n",
    "        config = cls()\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                config.__dict__.update(json.load(f))\n",
    "        return config\n",
    "\n",
    "# Helper functions\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return \"\".join(list(map(decode_token, tokens)))\n",
    "\n",
    "def format_time(seconds: float) -> str:\n",
    "    return str(timedelta(seconds=int(seconds)))\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len, device):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) // self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))\n",
    "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
    "        return full_seq.to(self.device)\n",
    "\n",
    "class TrainingManager:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.console = Console()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Create checkpoint directory\n",
    "        os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        self.setup_model()\n",
    "        self.setup_data()\n",
    "        self.setup_training()\n",
    "        \n",
    "    def setup_model(self):\n",
    "        self.model = nGPT(\n",
    "            num_tokens=256,\n",
    "            dim=512,\n",
    "            depth=8,\n",
    "            tied_embedding=True,\n",
    "            add_value_residual=True,\n",
    "            attn_norm_qk=False,\n",
    "            manual_norm_weights=not self.config.use_parametrize\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.scaler = GradScaler(enabled=self.config.use_amp)\n",
    "        \n",
    "    def setup_data(self):\n",
    "        with gzip.open(\"./data/enwik8.gz\") as file:\n",
    "            data = np.frombuffer(file.read(int(95e6)), dtype=np.uint8).copy()\n",
    "            np_train, np_valid = np.split(data, [int(90e6)])\n",
    "            self.data_train = torch.from_numpy(np_train)\n",
    "            self.data_val = torch.from_numpy(np_valid)\n",
    "\n",
    "        self.train_dataset = TextSamplerDataset(self.data_train, self.config.seq_len, self.device)\n",
    "        self.val_dataset = TextSamplerDataset(self.data_val, self.config.seq_len, self.device)\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.config.batch_size\n",
    "        )\n",
    "        \n",
    "    def setup_training(self):\n",
    "        self.optim = Adam(self.model.parameters(), lr=self.config.learning_rate)\n",
    "        \n",
    "        if not self.config.use_parametrize:\n",
    "            self.model.register_step_post_hook(self.optim)\n",
    "            \n",
    "    def save_checkpoint(self, step: int, loss: float):\n",
    "        checkpoint = {\n",
    "            'step': step,\n",
    "            'model_state': self.model.state_dict(),\n",
    "            'optimizer_state': self.optim.state_dict(),\n",
    "            'loss': loss,\n",
    "            'config': self.config.__dict__\n",
    "        }\n",
    "        path = os.path.join(self.config.checkpoint_dir, f'checkpoint_{step}.pt')\n",
    "        torch.save(checkpoint, path)\n",
    "        \n",
    "    def load_checkpoint(self, path: str):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        self.optim.load_state_dict(checkpoint['optimizer_state'])\n",
    "        return checkpoint['step'], checkpoint['loss']\n",
    "        \n",
    "    def validate(self) -> float:\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        batch = next(iter(self.val_loader))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss = self.model(batch, return_loss=True)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "    def log(self, t, eps = 1e-20):\n",
    "        return torch.log(t.clamp(min = eps))\n",
    "\n",
    "    def gumbel_noise(self, t):\n",
    "        noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "        return -self.log(-self.log(noise))\n",
    "\n",
    "    def gumbel_sample(self, t, temperature = 1., dim = -1, keepdim = True):\n",
    "        return ((t / max(temperature, 1e-10)) + self.gumbel_noise(t)).argmax(dim = dim, keepdim = keepdim)\n",
    "\n",
    "    def min_p_filter(self, logits, min_p = 0.1):\n",
    "        probs = logits.softmax(dim = -1)\n",
    "        max_probs = probs.amax(dim = -1, keepdim = True)\n",
    "        limit = min_p * max_probs\n",
    "        return torch.where(probs < limit, float('-inf'), logits)\n",
    "\n",
    "    def base_decoding(\n",
    "        self,\n",
    "        net,\n",
    "        prompt: Tensor,\n",
    "        seq_len: int,\n",
    "        temperature = 1.5,\n",
    "        min_p = 1e-1,\n",
    "        filter_thres = 0.9,\n",
    "    ):\n",
    "        prompt_seq_len, out = prompt.shape[-1], prompt.clone()\n",
    "        sample_num_times = max(0, seq_len - prompt_seq_len)\n",
    "\n",
    "        for _ in range(sample_num_times):\n",
    "            logits = net(out)\n",
    "            logits = logits[:, -1]\n",
    "\n",
    "            logits = self.min_p_filter(logits, min_p = min_p)\n",
    "            sample = self.gumbel_sample(logits, temperature = temperature, dim = -1)\n",
    "\n",
    "            out = torch.cat((out, sample), dim = -1)\n",
    "\n",
    "        return out[..., prompt_seq_len:]\n",
    "\n",
    "    def generate_sample(self) -> str:\n",
    "        self.model.eval()\n",
    "        \n",
    "        inp = random.choice(self.val_dataset)[:self.config.prime_length]\n",
    "        prime = decode_tokens(inp)\n",
    "        prompt = inp[None, ...]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sampled = self.base_decoding(\n",
    "                self.model,\n",
    "                prompt,\n",
    "                self.config.generate_length,\n",
    "                temperature=1.5,\n",
    "                min_p=0.1,\n",
    "                filter_thres=0.9\n",
    "            )\n",
    "                \n",
    "        generated = decode_tokens(sampled[0])\n",
    "        return f\"{prime}\\n\\n{'=' * 40}\\n\\n{generated}\"\n",
    "\n",
    "    def train(self):\n",
    "        start_time = time.time()\n",
    "        best_loss = float('inf')\n",
    "        running_loss = 0\n",
    "        \n",
    "        with Progress(\n",
    "            SpinnerColumn(),\n",
    "            *Progress.get_default_columns(),\n",
    "            TimeElapsedColumn(),\n",
    "            console=self.console\n",
    "        ) as progress:\n",
    "            \n",
    "            task = progress.add_task(\"[cyan]Training...\", total=self.config.num_batches)\n",
    "            \n",
    "            for i in range(self.config.num_batches):\n",
    "                self.model.train()\n",
    "                \n",
    "                # Gradient accumulation loop\n",
    "                for _ in range(self.config.grad_accum_every):\n",
    "                    batch = next(iter(self.train_loader))\n",
    "                    \n",
    "                    with torch.autocast(\n",
    "                        device_type='cuda',\n",
    "                        dtype=torch.float16,\n",
    "                        enabled=self.config.use_amp\n",
    "                    ):\n",
    "                        loss = self.model(batch, return_loss=True)\n",
    "                        \n",
    "                    self.scaler.scale(loss / self.config.grad_accum_every).backward()\n",
    "                    running_loss += loss.item() / self.config.grad_accum_every\n",
    "                \n",
    "                # Update weights\n",
    "                self.scaler.step(self.optim)\n",
    "                self.scaler.update()\n",
    "                self.optim.zero_grad()\n",
    "                \n",
    "                # Progress update\n",
    "                progress.update(\n",
    "                    task,\n",
    "                    advance=1,\n",
    "                    description=f\"[cyan]Training... Loss: {running_loss:.4f}\"\n",
    "                )\n",
    "                \n",
    "                # Validation\n",
    "                if i % self.config.validate_every == 0:\n",
    "                    val_loss = self.validate()\n",
    "                    self.console.print(f\"\\nValidation loss: {val_loss:.4f}\")\n",
    "                    \n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        self.save_checkpoint(i, val_loss)\n",
    "                        self.console.print(\"[green]Saved new best checkpoint!\")\n",
    "                \n",
    "                # Generate sample\n",
    "                if i % self.config.generate_every == 0:\n",
    "                    sample = self.generate_sample()\n",
    "                    self.console.print(f\"\\nGenerated sample:\\n{sample}\\n\")\n",
    "                \n",
    "                running_loss = 0\n",
    "                \n",
    "        training_time = time.time() - start_time\n",
    "        self.console.print(f\"\\nTraining completed in {format_time(training_time)}\")\n",
    "        self.console.print(f\"Best validation loss: {best_loss:.4f}\")\n",
    "\n",
    "def main():\n",
    "    console = Console()\n",
    "    \n",
    "    # Load or create config\n",
    "    config_file = \"training_config.json\"\n",
    "    config = TrainingConfig.load(config_file) if os.path.exists(config_file) else TrainingConfig()\n",
    "    \n",
    "    # Show configuration\n",
    "    table = Table(title=\"Training Configuration\")\n",
    "    table.add_column(\"Parameter\", style=\"cyan\")\n",
    "    table.add_column(\"Value\", style=\"green\")\n",
    "    \n",
    "    for key, value in config.__dict__.items():\n",
    "        table.add_row(key, str(value))\n",
    "    \n",
    "    console.print(table)\n",
    "    \n",
    "    if Confirm.ask(\"Start training with these parameters?\"):\n",
    "        # Save config\n",
    "        config.save(config_file)\n",
    "        \n",
    "        # Start training\n",
    "        trainer = TrainingManager(config)\n",
    "        trainer.train()\n",
    "    else:\n",
    "        console.print(\"Training cancelled\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
