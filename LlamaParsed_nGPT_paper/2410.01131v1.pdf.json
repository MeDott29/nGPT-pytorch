[
  {
    "page": 1,
    "text": "arXiv:2410.01131v1 [cs.LG] 1 Oct 2024Normalized Transformer\n                                 NGPT: NORMALIZED TRANSFORMER WITH REPRE-\n                                 SENTATION LEARNING ON THE HYPERSPHERE\n                                   Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun & Boris Ginsburg\n                                   NVIDIA\n                                   {iloshchilov,chsieh,simengs,bginsburg}@nvidia.com\n\n\n\n                                                                                     ABSTRACT\n\n\n\n                                           We propose a novel neural network architecture, the normalized Transformer\n                                           (nGPT) with representation learning on the hypersphere. In nGPT, all vectors\n                                           forming the embeddings, MLP, attention matrices and hidden states are unit norm\n                                           normalized. The input stream of tokens travels on the surface of a hypersphere,\n                                           with each layer contributing a displacement towards the target output predictions.\n                                           These displacements are defined by the MLP and attention blocks, whose vector\n                                           components also reside on the same hypersphere. Experiments show that nGPT\n                                           learns much faster, reducing the number of training steps required to achieve the\n                                           same accuracy by a factor of 4 to 20, depending on the sequence length.\n\n\n\n                                 1    INTRODUCTION\n\n\n\n                                 The Transformer architecture (Vaswani et al., 2017) is the foundation for most of modern language\n                                 models. An enormous number of modifications to this architecture have been proposed to improve\n                                 training stability, inference costs, context length, robustness, etc. It has been noted that the ap-\n                                 plication of various normalization techniques is beneficial (Salimans & Kingma, 2016), leading to\n                                 experiments with adding normalization layers such as LayerNorm and RMSNorm in nearly every\n                                 possible position within the network (Xiong et al., 2020). Another approach to the model normal-\n                                 ization is through controlling the norm of weights using weight decay (Loshchilov & Hutter, 2019).\n                                 Recent studies (Andriushchenko et al., 2023) suggest reevaluating the role of weight decay and tak-\n                                 ing a closer look at rotations rather than focusing solely on vector norms (Kodryan et al., 2022;\n                                 Kosson et al., 2023). Franke et al. (2023) suggested to enforce an upper bound on L2 norm of pa-\n                                 rameter groups. There is growing evidence that representation learning on the hypersphere is asso-\n                                 ciated with more stable training, greater embedding space separability, and better performance on\n                                 downstream tasks (Wang & Isola, 2020). Recent studies also suggest that transformers implicitly\n                                 perform gradient descent as meta-optimizers (Von Oswald et al., 2023; Dai et al., 2022).\n                                 We propose to unify various findings and observations made in the field under a new perspective of\n                                 the normalized Transformer. Our key contributions are as follows:\n\n\n\n                                  Optimization of network parameters on the hypersphere We propose to normalize all vectors\n                                           forming the embedding dimensions of network matrices to lie on a unit norm hypersphere.\n                                           This allows us to view matrix-vector multiplications as dot products representing cosine\n                                           similarities bounded in [-1,1]. The normalization renders weight decay unnecessary.\n                                  Normalized Transformer as a variable-metric optimizer on the hypersphere The normalized\n                                           Transformer itself performs a multi-step optimization (two steps per layer) on a hyper-\n                                           sphere, where each step of the attention and MLP updates is controlled by eigen learning\n                                           rates—the diagonal elements of a learnable variable-metric matrix. For each token ti in the\n                                           input sequence, the optimization path of the normalized Transformer begins at a point on\n                                           the hypersphere corresponding to its input embedding vector and moves to a point on the\n                                           hypersphere that best predicts the embedding vector of the next token ti+1.\n                                  Faster convergence We demonstrate that the normalized Transformer reduces the number of train-\n                                           ing steps required to achieve the same accuracy by a factor of 4 to 20.\n\n\n\n                                                                                             1",
    "md": "Normalized Transformer\n\n# NGPT: NORMALIZED TRANSFORMER WITH REPRESENTATION LEARNING ON THE HYPERSPHERE\n\nIlya Loshchilov, Cheng-Ping Hsieh, Simeng Sun & Boris Ginsburg\nNVIDIA\n{iloshchilov,chsieh,simengs,bginsburg}@nvidia.com\n\n## ABSTRACT\n\nWe propose a novel neural network architecture, the normalized Transformer\n(nGPT) with representation learning on the hypersphere. In nGPT, all vectors\nforming the embeddings, MLP, attention matrices and hidden states are unit norm\nnormalized. The input stream of tokens travels on the surface of a hypersphere,\nwith each layer contributing a displacement towards the target output predictions.\nThese displacements are defined by the MLP and attention blocks, whose vector\ncomponents also reside on the same hypersphere. Experiments show that nGPT\nlearns much faster, reducing the number of training steps required to achieve the\nsame accuracy by a factor of 4 to 20, depending on the sequence length.\n\n## 1 INTRODUCTION\n\nThe Transformer architecture (Vaswani et al., 2017) is the foundation for most of modern language\nmodels. An enormous number of modifications to this architecture have been proposed to improve\ntraining stability, inference costs, context length, robustness, etc. It has been noted that the ap-\nplication of various normalization techniques is beneficial (Salimans & Kingma, 2016), leading to\nexperiments with adding normalization layers such as LayerNorm and RMSNorm in nearly every\npossible position within the network (Xiong et al., 2020). Another approach to the model normal-\nization is through controlling the norm of weights using weight decay (Loshchilov & Hutter, 2019).\nRecent studies (Andriushchenko et al., 2023) suggest reevaluating the role of weight decay and tak-\ning a closer look at rotations rather than focusing solely on vector norms (Kodryan et al., 2022;\nKosson et al., 2023). Franke et al. (2023) suggested to enforce an upper bound on L2 norm of pa-\nrameter groups. There is growing evidence that representation learning on the hypersphere is asso-\nciated with more stable training, greater embedding space separability, and better performance on\ndownstream tasks (Wang & Isola, 2020). Recent studies also suggest that transformers implicitly\nperform gradient descent as meta-optimizers (Von Oswald et al., 2023; Dai et al., 2022).\n\nWe propose to unify various findings and observations made in the field under a new perspective of\nthe normalized Transformer. Our key contributions are as follows:\n\nOptimization of network parameters on the hypersphere We propose to normalize all vectors\nforming the embedding dimensions of network matrices to lie on a unit norm hypersphere.\nThis allows us to view matrix-vector multiplications as dot products representing cosine\nsimilarities bounded in [-1,1]. The normalization renders weight decay unnecessary.\n\nNormalized Transformer as a variable-metric optimizer on the hypersphere The normalized\nTransformer itself performs a multi-step optimization (two steps per layer) on a hyper-\nsphere, where each step of the attention and MLP updates is controlled by eigen learning\nrates—the diagonal elements of a learnable variable-metric matrix. For each token ti in the\ninput sequence, the optimization path of the normalized Transformer begins at a point on\nthe hypersphere corresponding to its input embedding vector and moves to a point on the\nhypersphere that best predicts the embedding vector of the next token ti+1.\n\nFaster convergence We demonstrate that the normalized Transformer reduces the number of train-\ning steps required to achieve the same accuracy by a factor of 4 to 20.\n\n1",
    "images": [
      {
        "name": "page_1.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "text",
        "value": "Normalized Transformer",
        "md": "Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "NGPT: NORMALIZED TRANSFORMER WITH REPRESENTATION LEARNING ON THE HYPERSPHERE",
        "md": "# NGPT: NORMALIZED TRANSFORMER WITH REPRESENTATION LEARNING ON THE HYPERSPHERE",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 337.57,
          "h": 17.22
        }
      },
      {
        "type": "text",
        "value": "Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun & Boris Ginsburg\nNVIDIA\n{iloshchilov,chsieh,simengs,bginsburg}@nvidia.com",
        "md": "Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun & Boris Ginsburg\nNVIDIA\n{iloshchilov,chsieh,simengs,bginsburg}@nvidia.com",
        "bBox": {
          "x": 114,
          "y": 145,
          "w": 290.74,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "ABSTRACT",
        "md": "## ABSTRACT",
        "bBox": {
          "x": 278,
          "y": 208,
          "w": 54.56,
          "h": 11.96
        }
      },
      {
        "type": "text",
        "value": "We propose a novel neural network architecture, the normalized Transformer\n(nGPT) with representation learning on the hypersphere. In nGPT, all vectors\nforming the embeddings, MLP, attention matrices and hidden states are unit norm\nnormalized. The input stream of tokens travels on the surface of a hypersphere,\nwith each layer contributing a displacement towards the target output predictions.\nThese displacements are defined by the MLP and attention blocks, whose vector\ncomponents also reside on the same hypersphere. Experiments show that nGPT\nlearns much faster, reducing the number of training steps required to achieve the\nsame accuracy by a factor of 4 to 20, depending on the sequence length.",
        "md": "We propose a novel neural network architecture, the normalized Transformer\n(nGPT) with representation learning on the hypersphere. In nGPT, all vectors\nforming the embeddings, MLP, attention matrices and hidden states are unit norm\nnormalized. The input stream of tokens travels on the surface of a hypersphere,\nwith each layer contributing a displacement towards the target output predictions.\nThese displacements are defined by the MLP and attention blocks, whose vector\ncomponents also reside on the same hypersphere. Experiments show that nGPT\nlearns much faster, reducing the number of training steps required to achieve the\nsame accuracy by a factor of 4 to 20, depending on the sequence length.",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 337.57,
          "h": 17.22
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "1 INTRODUCTION",
        "md": "## 1 INTRODUCTION",
        "bBox": {
          "x": 108,
          "y": 360,
          "w": 79.1,
          "h": 11.96
        }
      },
      {
        "type": "text",
        "value": "The Transformer architecture (Vaswani et al., 2017) is the foundation for most of modern language\nmodels. An enormous number of modifications to this architecture have been proposed to improve\ntraining stability, inference costs, context length, robustness, etc. It has been noted that the ap-\nplication of various normalization techniques is beneficial (Salimans & Kingma, 2016), leading to\nexperiments with adding normalization layers such as LayerNorm and RMSNorm in nearly every\npossible position within the network (Xiong et al., 2020). Another approach to the model normal-\nization is through controlling the norm of weights using weight decay (Loshchilov & Hutter, 2019).\nRecent studies (Andriushchenko et al., 2023) suggest reevaluating the role of weight decay and tak-\ning a closer look at rotations rather than focusing solely on vector norms (Kodryan et al., 2022;\nKosson et al., 2023). Franke et al. (2023) suggested to enforce an upper bound on L2 norm of pa-\nrameter groups. There is growing evidence that representation learning on the hypersphere is asso-\nciated with more stable training, greater embedding space separability, and better performance on\ndownstream tasks (Wang & Isola, 2020). Recent studies also suggest that transformers implicitly\nperform gradient descent as meta-optimizers (Von Oswald et al., 2023; Dai et al., 2022).\n\nWe propose to unify various findings and observations made in the field under a new perspective of\nthe normalized Transformer. Our key contributions are as follows:\n\nOptimization of network parameters on the hypersphere We propose to normalize all vectors\nforming the embedding dimensions of network matrices to lie on a unit norm hypersphere.\nThis allows us to view matrix-vector multiplications as dot products representing cosine\nsimilarities bounded in [-1,1]. The normalization renders weight decay unnecessary.\n\nNormalized Transformer as a variable-metric optimizer on the hypersphere The normalized\nTransformer itself performs a multi-step optimization (two steps per layer) on a hyper-\nsphere, where each step of the attention and MLP updates is controlled by eigen learning\nrates—the diagonal elements of a learnable variable-metric matrix. For each token ti in the\ninput sequence, the optimization path of the normalized Transformer begins at a point on\nthe hypersphere corresponding to its input embedding vector and moves to a point on the\nhypersphere that best predicts the embedding vector of the next token ti+1.\n\nFaster convergence We demonstrate that the normalized Transformer reduces the number of train-\ning steps required to achieve the same accuracy by a factor of 4 to 20.\n\n1",
        "md": "The Transformer architecture (Vaswani et al., 2017) is the foundation for most of modern language\nmodels. An enormous number of modifications to this architecture have been proposed to improve\ntraining stability, inference costs, context length, robustness, etc. It has been noted that the ap-\nplication of various normalization techniques is beneficial (Salimans & Kingma, 2016), leading to\nexperiments with adding normalization layers such as LayerNorm and RMSNorm in nearly every\npossible position within the network (Xiong et al., 2020). Another approach to the model normal-\nization is through controlling the norm of weights using weight decay (Loshchilov & Hutter, 2019).\nRecent studies (Andriushchenko et al., 2023) suggest reevaluating the role of weight decay and tak-\ning a closer look at rotations rather than focusing solely on vector norms (Kodryan et al., 2022;\nKosson et al., 2023). Franke et al. (2023) suggested to enforce an upper bound on L2 norm of pa-\nrameter groups. There is growing evidence that representation learning on the hypersphere is asso-\nciated with more stable training, greater embedding space separability, and better performance on\ndownstream tasks (Wang & Isola, 2020). Recent studies also suggest that transformers implicitly\nperform gradient descent as meta-optimizers (Von Oswald et al., 2023; Dai et al., 2022).\n\nWe propose to unify various findings and observations made in the field under a new perspective of\nthe normalized Transformer. Our key contributions are as follows:\n\nOptimization of network parameters on the hypersphere We propose to normalize all vectors\nforming the embedding dimensions of network matrices to lie on a unit norm hypersphere.\nThis allows us to view matrix-vector multiplications as dot products representing cosine\nsimilarities bounded in [-1,1]. The normalization renders weight decay unnecessary.\n\nNormalized Transformer as a variable-metric optimizer on the hypersphere The normalized\nTransformer itself performs a multi-step optimization (two steps per layer) on a hyper-\nsphere, where each step of the attention and MLP updates is controlled by eigen learning\nrates—the diagonal elements of a learnable variable-metric matrix. For each token ti in the\ninput sequence, the optimization path of the normalized Transformer begins at a point on\nthe hypersphere corresponding to its input embedding vector and moves to a point on the\nhypersphere that best predicts the embedding vector of the next token ti+1.\n\nFaster convergence We demonstrate that the normalized Transformer reduces the number of train-\ning steps required to achieve the same accuracy by a factor of 4 to 20.\n\n1",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 396.34,
          "h": 17.22
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "url": "http://arxiv.org/abs/2410.01131v1",
        "text": "arXiv:2410.01131v1 [cs.LG] 1 Oct 2024"
      },
      {
        "text": "models. An enormous number of modifications to this architecture have been proposed to improve"
      },
      {
        "text": "models. An enormous number of modifications to this architecture have been proposed to improve"
      },
      {
        "text": "experiments with adding normalization layers such as LayerNorm and RMSNorm in nearly every"
      },
      {
        "text": "experiments with adding normalization layers such as LayerNorm and RMSNorm in nearly every"
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": "Recent studies (Andriushchenko et al., 2023) suggest reevaluating the role of weight decay and tak-"
      },
      {
        "text": "Recent studies (Andriushchenko et al., 2023) suggest reevaluating the role of weight decay and tak-"
      },
      {
        "text": "ing a closer look at rotations rather than focusing solely on vector norms (Kodryan et al., 2022;"
      },
      {
        "text": "ing a closer look at rotations rather than focusing solely on vector norms (Kodryan et al., 2022;"
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": "Kosson et al., 2023). Franke et al. (2023) suggested to enforce an upper bound on"
      },
      {
        "text": "rameter groups. There is growing evidence that representation learning on the hypersphere is asso-"
      },
      {
        "text": "rameter groups. There is growing evidence that representation learning on the hypersphere is asso-"
      },
      {
        "text": "rameter groups. There is growing evidence that representation learning on the hypersphere is asso-"
      },
      {
        "text": "arXiv:2410.01131v1 [cs.LG] 1 Oct 2024 perform gradient descent as meta-optimizers (Von Oswald et al., 2023; Dai et al., 2022)."
      },
      {
        "text": "arXiv:2410.01131v1 [cs.LG] 1 Oct 2024 perform gradient descent as meta-optimizers (Von Oswald et al., 2023; Dai et al., 2022)."
      },
      {
        "text": "arXiv:2410.01131v1 [cs.LG] 1 Oct 2024"
      },
      {
        "text": "arXiv:2410.01131v1 [cs.LG] 1 Oct 2024"
      },
      {
        "text": ""
      },
      {
        "text": ""
      }
    ]
  },
  {
    "page": 2,
    "text": "Normalized Transformer\n\n\n\n2     EVOLUTION OF THE TRANSFORMER: FROM GPT TO NGPT\n\n\n\nThis section outlines the baseline Transformer and the modifications necessary to derive its nor-\nmalized version. We illustrate these changes for Transformer decoder with self-attention only. The\nextension to encoder-decoder and cross-attention is straightforward. A summary of these changes is\ngiven in Table 1, with details in Section 2.6.\n\n\n\n2.1     TOKEN EMBEDDINGS AND OUTPUT LOGITS\n\n\n\nThe decoder-only Transformer is trained to predict token ti using previous tokens input sequence\nx = (t1, t2, . . . , ti−1). For each input token ti, we retrieve its corresponding embedding in Rdmodel\nfrom a learnable embedding matrix Einput ∈ RV ×dmodel with vocabulary of size V . Similarly, the\ntarget output sequence y is represented using a learnable embedding matrix Eoutput ∈ RV ×dmodel .\nNotably, since both Einput and Eoutput are learnable (unless they are tied to be equivalent), any token\nti can have different embeddings in the input and output sequences. To measure token similarity\nduring model training, the dot product between the corresponding embedding vectors is used. How-\never, the norms of embedding vectors in the original Transformer are unconstrained, which can lead\nto inaccurate similarity estimation. To improve the accuracy of similarity estimation, we propose to\nnormalize the embedding vectors stored in Einput and Eoutput after each step of the training algorithm.\nThe next token prediction is enforced by causal masking (see Section 2.3) to ensure that no future\ntokens are considered. This allows the model to compute the prediction error for all T tokens in\nparallel during training, while preserving the autoregressive nature of the task. After the Transformer\nprocesses the sequence (x1, . . . , xi−1), it produces an output vector hi ∈ Rdmodel for each i-th\nposition in the predicted sequence. The logits zi ∈ RV , representing the unnormalized probabilities\nfor each token in the vocabulary, are computed using the output embedding matrix Eoutput:\n\n\n\n                                                       zi = Eoutputhi                                              (1)\n\n\n\nThe logits zi are passed through a softmax function to convert them into probabilities:\n\n\n\n                                       P (yi|x1, . . . , xi−1) =     ∑Vexp(zi,yi )                                 (2)\n                                                                        v=1 exp(zi,v)\nHere, zi,yi is the logit corresponding to the correct token yi, and the denominator normalizes the log-\nits into a probability distribution over the vocabulary. During inference, the prediction ˆi is obtainedy\nby selecting the token with the highest probability. Since all nGPT embeddings are normalized, the\nlogits z ∈ RV in equation 1 represent dot products bounded in the range [−1, 1]. This limits the\nconfidence (temperature) of the probability distribution generated by the softmax in equation 2. To\nadjust this during training, we introduce a trainable scaling parameter sz ∈ RV that scales the logits\nelement-wise:\n\n\n\n                                                          z ← zsz                                                  (3)\n\n\n\n                               Table 1: Transformer vs. Normalized Transformer.\n\n\n\n     Transformer                               Normalized Transformer\n     hA ← ATTN(RMSNorm(h))                     hA ← Norm(ATTN(h))\n     h ← h + hA                                h ← Norm(h + αA(hA − h))\n     hM ← MLP(RMSNorm(h))                      hM ← Norm(MLP(h))\n     h ← h + hM                                h ← Norm(h + αM(hM − h))\n     Final: h ← RMSNorm(h)\n     All parameters of matrices and            After each batch pass, all matrices and embeddings are normalized\n     embeddings are unconstrained.             along their embedding dimension. The hidden state updates are\n                                               controlled by learnable vectors of eigen learning rates αA and αM.\n\n\n\n                                                               2",
    "md": "# Normalized Transformer\n\n## 2 EVOLUTION OF THE TRANSFORMER: FROM GPT TO NGPT\n\nThis section outlines the baseline Transformer and the modifications necessary to derive its normalized version. We illustrate these changes for Transformer decoder with self-attention only. The extension to encoder-decoder and cross-attention is straightforward. A summary of these changes is given in Table 1, with details in Section 2.6.\n\n### 2.1 TOKEN EMBEDDINGS AND OUTPUT LOGITS\n\nThe decoder-only Transformer is trained to predict token ti using previous tokens input sequence x = (t1, t2, . . . , ti−1). For each input token ti, we retrieve its corresponding embedding in R^dmodel from a learnable embedding matrix Einput ∈ R^V×dmodel with vocabulary of size V . Similarly, the target output sequence y is represented using a learnable embedding matrix Eoutput ∈ R^V×dmodel. Notably, since both Einput and Eoutput are learnable (unless they are tied to be equivalent), any token ti can have different embeddings in the input and output sequences. To measure token similarity during model training, the dot product between the corresponding embedding vectors is used. However, the norms of embedding vectors in the original Transformer are unconstrained, which can lead to inaccurate similarity estimation. To improve the accuracy of similarity estimation, we propose to normalize the embedding vectors stored in Einput and Eoutput after each step of the training algorithm.\n\nThe next token prediction is enforced by causal masking (see Section 2.3) to ensure that no future tokens are considered. This allows the model to compute the prediction error for all T tokens in parallel during training, while preserving the autoregressive nature of the task. After the Transformer processes the sequence (x1, . . . , xi−1), it produces an output vector hi ∈ R^dmodel for each i-th position in the predicted sequence. The logits zi ∈ R^V, representing the unnormalized probabilities for each token in the vocabulary, are computed using the output embedding matrix Eoutput:\n\n$$z_i = E_{output}h_i$$ (1)\n\nThe logits zi are passed through a softmax function to convert them into probabilities:\n\n$$P(y_i|x_1, \\ldots, x_{i-1}) = \\frac{\\exp(z_{i,y_i})}{\\sum_{v=1}^V \\exp(z_{i,v})}$$ (2)\n\nHere, zi,yi is the logit corresponding to the correct token yi, and the denominator normalizes the logits into a probability distribution over the vocabulary. During inference, the prediction ŷi is obtained by selecting the token with the highest probability. Since all nGPT embeddings are normalized, the logits z ∈ R^V in equation 1 represent dot products bounded in the range [−1, 1]. This limits the confidence (temperature) of the probability distribution generated by the softmax in equation 2. To adjust this during training, we introduce a trainable scaling parameter sz ∈ R^V that scales the logits element-wise:\n\n$$z ← zsz$$ (3)\n\nTable 1: Transformer vs. Normalized Transformer.\n\n| Transformer | Normalized Transformer |\n|-------------|------------------------|\n| hA ← ATTN(RMSNorm(h)) | hA ← Norm(ATTN(h)) |\n| h ← h + hA | h ← Norm(h + αA(hA − h)) |\n| hM ← MLP(RMSNorm(h)) | hM ← Norm(MLP(h)) |\n| h ← h + hM | h ← Norm(h + αM(hM − h)) |\n| Final: h ← RMSNorm(h) | |\n| All parameters of matrices and embeddings are unconstrained. | After each batch pass, all matrices and embeddings are normalized along their embedding dimension. The hidden state updates are controlled by learnable vectors of eigen learning rates αA and αM. |",
    "images": [
      {
        "name": "page_2.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "2 EVOLUTION OF THE TRANSFORMER: FROM GPT TO NGPT",
        "md": "## 2 EVOLUTION OF THE TRANSFORMER: FROM GPT TO NGPT",
        "bBox": {
          "x": 108,
          "y": 92,
          "w": 299.5,
          "h": 11.96
        }
      },
      {
        "type": "text",
        "value": "This section outlines the baseline Transformer and the modifications necessary to derive its normalized version. We illustrate these changes for Transformer decoder with self-attention only. The extension to encoder-decoder and cross-attention is straightforward. A summary of these changes is given in Table 1, with details in Section 2.6.",
        "md": "This section outlines the baseline Transformer and the modifications necessary to derive its normalized version. We illustrate these changes for Transformer decoder with self-attention only. The extension to encoder-decoder and cross-attention is straightforward. A summary of these changes is given in Table 1, with details in Section 2.6.",
        "bBox": {
          "x": 108,
          "y": 92,
          "w": 396.1,
          "h": 11.96
        }
      },
      {
        "type": "heading",
        "lvl": 3,
        "value": "2.1 TOKEN EMBEDDINGS AND OUTPUT LOGITS",
        "md": "### 2.1 TOKEN EMBEDDINGS AND OUTPUT LOGITS",
        "bBox": {
          "x": 108,
          "y": 92,
          "w": 185.43,
          "h": 11.96
        }
      },
      {
        "type": "text",
        "value": "The decoder-only Transformer is trained to predict token ti using previous tokens input sequence x = (t1, t2, . . . , ti−1). For each input token ti, we retrieve its corresponding embedding in R^dmodel from a learnable embedding matrix Einput ∈ R^V×dmodel with vocabulary of size V . Similarly, the target output sequence y is represented using a learnable embedding matrix Eoutput ∈ R^V×dmodel. Notably, since both Einput and Eoutput are learnable (unless they are tied to be equivalent), any token ti can have different embeddings in the input and output sequences. To measure token similarity during model training, the dot product between the corresponding embedding vectors is used. However, the norms of embedding vectors in the original Transformer are unconstrained, which can lead to inaccurate similarity estimation. To improve the accuracy of similarity estimation, we propose to normalize the embedding vectors stored in Einput and Eoutput after each step of the training algorithm.\n\nThe next token prediction is enforced by causal masking (see Section 2.3) to ensure that no future tokens are considered. This allows the model to compute the prediction error for all T tokens in parallel during training, while preserving the autoregressive nature of the task. After the Transformer processes the sequence (x1, . . . , xi−1), it produces an output vector hi ∈ R^dmodel for each i-th position in the predicted sequence. The logits zi ∈ R^V, representing the unnormalized probabilities for each token in the vocabulary, are computed using the output embedding matrix Eoutput:\n\n$$z_i = E_{output}h_i$$ (1)\n\nThe logits zi are passed through a softmax function to convert them into probabilities:\n\n$$P(y_i|x_1, \\ldots, x_{i-1}) = \\frac{\\exp(z_{i,y_i})}{\\sum_{v=1}^V \\exp(z_{i,v})}$$ (2)\n\nHere, zi,yi is the logit corresponding to the correct token yi, and the denominator normalizes the logits into a probability distribution over the vocabulary. During inference, the prediction ŷi is obtained by selecting the token with the highest probability. Since all nGPT embeddings are normalized, the logits z ∈ R^V in equation 1 represent dot products bounded in the range [−1, 1]. This limits the confidence (temperature) of the probability distribution generated by the softmax in equation 2. To adjust this during training, we introduce a trainable scaling parameter sz ∈ R^V that scales the logits element-wise:\n\n$$z ← zsz$$ (3)\n\nTable 1: Transformer vs. Normalized Transformer.",
        "md": "The decoder-only Transformer is trained to predict token ti using previous tokens input sequence x = (t1, t2, . . . , ti−1). For each input token ti, we retrieve its corresponding embedding in R^dmodel from a learnable embedding matrix Einput ∈ R^V×dmodel with vocabulary of size V . Similarly, the target output sequence y is represented using a learnable embedding matrix Eoutput ∈ R^V×dmodel. Notably, since both Einput and Eoutput are learnable (unless they are tied to be equivalent), any token ti can have different embeddings in the input and output sequences. To measure token similarity during model training, the dot product between the corresponding embedding vectors is used. However, the norms of embedding vectors in the original Transformer are unconstrained, which can lead to inaccurate similarity estimation. To improve the accuracy of similarity estimation, we propose to normalize the embedding vectors stored in Einput and Eoutput after each step of the training algorithm.\n\nThe next token prediction is enforced by causal masking (see Section 2.3) to ensure that no future tokens are considered. This allows the model to compute the prediction error for all T tokens in parallel during training, while preserving the autoregressive nature of the task. After the Transformer processes the sequence (x1, . . . , xi−1), it produces an output vector hi ∈ R^dmodel for each i-th position in the predicted sequence. The logits zi ∈ R^V, representing the unnormalized probabilities for each token in the vocabulary, are computed using the output embedding matrix Eoutput:\n\n$$z_i = E_{output}h_i$$ (1)\n\nThe logits zi are passed through a softmax function to convert them into probabilities:\n\n$$P(y_i|x_1, \\ldots, x_{i-1}) = \\frac{\\exp(z_{i,y_i})}{\\sum_{v=1}^V \\exp(z_{i,v})}$$ (2)\n\nHere, zi,yi is the logit corresponding to the correct token yi, and the denominator normalizes the logits into a probability distribution over the vocabulary. During inference, the prediction ŷi is obtained by selecting the token with the highest probability. Since all nGPT embeddings are normalized, the logits z ∈ R^V in equation 1 represent dot products bounded in the range [−1, 1]. This limits the confidence (temperature) of the probability distribution generated by the softmax in equation 2. To adjust this during training, we introduce a trainable scaling parameter sz ∈ R^V that scales the logits element-wise:\n\n$$z ← zsz$$ (3)\n\nTable 1: Transformer vs. Normalized Transformer.",
        "bBox": {
          "x": 107,
          "y": 36,
          "w": 397.22,
          "h": 11.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Transformer",
            "Normalized Transformer"
          ],
          [
            "hA ← ATTN(RMSNorm(h))",
            "hA ← Norm(ATTN(h))"
          ],
          [
            "h ← h + hA",
            "h ← Norm(h + αA(hA − h))"
          ],
          [
            "hM ← MLP(RMSNorm(h))",
            "hM ← Norm(MLP(h))"
          ],
          [
            "h ← h + hM",
            "h ← Norm(h + αM(hM − h))"
          ],
          [
            "Final: h ← RMSNorm(h)",
            ""
          ],
          [
            "All parameters of matrices and embeddings are unconstrained.",
            "After each batch pass, all matrices and embeddings are normalized along their embedding dimension. The hidden state updates are controlled by learnable vectors of eigen learning rates αA and αM."
          ]
        ],
        "md": "| Transformer | Normalized Transformer |\n|-------------|------------------------|\n| hA ← ATTN(RMSNorm(h)) | hA ← Norm(ATTN(h)) |\n| h ← h + hA | h ← Norm(h + αA(hA − h)) |\n| hM ← MLP(RMSNorm(h)) | hM ← Norm(MLP(h)) |\n| h ← h + hM | h ← Norm(h + αM(hM − h)) |\n| Final: h ← RMSNorm(h) | |\n| All parameters of matrices and embeddings are unconstrained. | After each batch pass, all matrices and embeddings are normalized along their embedding dimension. The hidden state updates are controlled by learnable vectors of eigen learning rates αA and αM. |",
        "isPerfectTable": true,
        "csv": "\"Transformer\",\"Normalized Transformer\"\n\"hA ← ATTN(RMSNorm(h))\",\"hA ← Norm(ATTN(h))\"\n\"h ← h + hA\",\"h ← Norm(h + αA(hA − h))\"\n\"hM ← MLP(RMSNorm(h))\",\"hM ← Norm(MLP(h))\"\n\"h ← h + hM\",\"h ← Norm(h + αM(hM − h))\"\n\"Final: h ← RMSNorm(h)\",\"\"\n\"All parameters of matrices and embeddings are unconstrained.\",\"After each batch pass, all matrices and embeddings are normalized along their embedding dimension. The hidden state updates are controlled by learnable vectors of eigen learning rates αA and αM.\"",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 238.22,
          "h": 589.97
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": "tokens are considered. This allows the model to compute the prediction error for all"
      },
      {
        "text": "confidence (temperature) of the probability distribution generated by the softmax in equation 2. To"
      },
      {
        "text": "that scales the logits"
      }
    ]
  },
  {
    "page": 3,
    "text": "Normalized Transformer\n\n\n\n2.2    LAYERS AND BLOCKS\n\n\n\n2.2.1     BASELINE TRANSFORMER\nL layers of transformations are applied to the hidden state h, consisting of alternating the self-\nattention (ATTN) and multi-layer perceptron (MLP) blocks:\n                                         h ← h + ATTN(RMSNorm(h))                                                  (4)\n                                         h ← h + MLP(RMSNorm(h)),                                                  (5)\nwhere RMSNorm(h) is one of several possible normalizations. It is used first to normalize each\nembedding to a norm of√dmodel, then scales each dimension by a learnable vector of dmodel factors,\ntypically initialized to 1. Since the transformation block outputs are added to h, the token embedding\nnorms can vary significantly. To address this, normalization is also applied after the final layer.\n\n\n\n2.2.2     NORMALIZED TRANSFORMER\nFor any points a and b on the surface of a hypersphere in Rdmodel , SLERP (Spherical Linear Interpo-\nlation) by Shoemake (1985) computes an interpolation along the geodesic (shortest path):\n                               SLERP(a, b; α) = sin((1 − α)θ)a + sin(θ) sin(θ)  sin(αθ)b                           (6)\nwhere θ = arccos(a · b) is the angle between the points a and b, and α ∈ [0, 1] is the interpolation\nparameter, with α = 0 returning a and α = 1 returning b. Our experiments suggest that SLERP can\nbe approximated by simple linear interpolation (LERP):\n                                        LERP(a, b; w) = (1 − α)a + αb                                              (7)\nLet us rewrite this equation as an update equation in nGPT:\n                                                 a ← a + α(b − a)                                                  (8)\nwhere a is h, and, b is the point suggested by the attention or MLP block. Then, for the gradient\ng = a − b, a more general form involving a variable matrix B ∈ Rdmodel×dmodel becomes:\n                                                   a ← a − αBg                                                     (9)\nIn quasi-Newton methods, B approximates the inverse Hessian matrix H−1.When B is diagonal\nwith non-negative elements, αB becomes a vector α ∈ R≥0dmodel                   whose elements correspond to the\ndiagonal of B times the learning rate α. We denote α as eigen learning rates (from the German\nword eigen, meaning ”own,” referring to the internal structure of the Transformer). We provide\nsome notes in Appendix A.2. Following equation 8, the update equations for the attention and MLP\nblocks are as follows:\n                                         h ← Norm( h + αA(hA − h) )                                               (10)\n                                         h ← Norm( h + αM(hM − h) ),                                              (11)\nwhere αA ∈ R≥0dmodel      and αM ∈ R≥0dmodel     are learnable parameters applied to the normalized outputs\nof the attention and MLP blocks hA = Norm(ATTN(h)) and hM = Norm(MLP(h)), respectively.\nThe function Norm(x) normalizes any vector x to have unit norm, and, unlike RMSNorm or Lay-\nerNorm, does not introduce any element-wise scaling factors. The normalization can be viewed as\nthe retraction step in Riemannian optimization, mapping the updated solution back to the manifold.\nAppendix A.3 discusses the extension of our update equations in the context Riemannian optimiza-\ntion. In contrast to the baseline Transformer, no additional normalization is required after the final\nlayer, as the embeddings are already normalized by the proposed scheme.\n\n\n\n2.3    SELF-ATTENTION BLOCK\n2.3.1     BASELINE TRANSFORMER\nThe attention mechanism is a key component of the Transformer. It allows each token to attend\nto every other token in the sequence, enabling the model to capture long-range dependencies. The\n\n\n\n                                                            3",
    "md": "# Normalized Transformer\n\n## 2.2 LAYERS AND BLOCKS\n\n### 2.2.1 BASELINE TRANSFORMER\n\nL layers of transformations are applied to the hidden state h, consisting of alternating the self-attention (ATTN) and multi-layer perceptron (MLP) blocks:\n\nh ← h + ATTN(RMSNorm(h))                (4)\nh ← h + MLP(RMSNorm(h)),                (5)\n\nwhere RMSNorm(h) is one of several possible normalizations. It is used first to normalize each embedding to a norm of √d_model, then scales each dimension by a learnable vector of d_model factors, typically initialized to 1. Since the transformation block outputs are added to h, the token embedding norms can vary significantly. To address this, normalization is also applied after the final layer.\n\n### 2.2.2 NORMALIZED TRANSFORMER\n\nFor any points a and b on the surface of a hypersphere in R^d_model, SLERP (Spherical Linear Interpolation) by Shoemake (1985) computes an interpolation along the geodesic (shortest path):\n\nSLERP(a, b; α) = sin((1 - α)θ)/sin(θ) * a + sin(αθ)/sin(θ) * b        (6)\n\nwhere θ = arccos(a · b) is the angle between the points a and b, and α ∈ [0, 1] is the interpolation parameter, with α = 0 returning a and α = 1 returning b. Our experiments suggest that SLERP can be approximated by simple linear interpolation (LERP):\n\nLERP(a, b; w) = (1 - α)a + αb                (7)\n\nLet us rewrite this equation as an update equation in nGPT:\n\na ← a + α(b - a)                              (8)\n\nwhere a is h, and, b is the point suggested by the attention or MLP block. Then, for the gradient g = a - b, a more general form involving a variable matrix B ∈ R^d_model×d_model becomes:\n\na ← a - αBg                                   (9)\n\nIn quasi-Newton methods, B approximates the inverse Hessian matrix H^-1. When B is diagonal with non-negative elements, αB becomes a vector α ∈ R^d_model_≥0 whose elements correspond to the diagonal of B times the learning rate α. We denote α as eigen learning rates (from the German word eigen, meaning \"own,\" referring to the internal structure of the Transformer). We provide some notes in Appendix A.2. Following equation 8, the update equations for the attention and MLP blocks are as follows:\n\nh ← Norm( h + α_A(h_A - h) )                 (10)\nh ← Norm( h + α_M(h_M - h) ),                (11)\n\nwhere α_A ∈ R^d_model_≥0 and α_M ∈ R^d_model_≥0 are learnable parameters applied to the normalized outputs of the attention and MLP blocks h_A = Norm(ATTN(h)) and h_M = Norm(MLP(h)), respectively. The function Norm(x) normalizes any vector x to have unit norm, and, unlike RMSNorm or LayerNorm, does not introduce any element-wise scaling factors. The normalization can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold. Appendix A.3 discusses the extension of our update equations in the context Riemannian optimization. In contrast to the baseline Transformer, no additional normalization is required after the final layer, as the embeddings are already normalized by the proposed scheme.\n\n## 2.3 SELF-ATTENTION BLOCK\n\n### 2.3.1 BASELINE TRANSFORMER\n\nThe attention mechanism is a key component of the Transformer. It allows each token to attend to every other token in the sequence, enabling the model to capture long-range dependencies. The",
    "images": [
      {
        "name": "page_3.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 126.36,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "2.2 LAYERS AND BLOCKS",
        "md": "## 2.2 LAYERS AND BLOCKS",
        "bBox": {
          "x": 108,
          "y": 92,
          "w": 93.07,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 3,
        "value": "2.2.1 BASELINE TRANSFORMER",
        "md": "### 2.2.1 BASELINE TRANSFORMER",
        "bBox": {
          "x": 108,
          "y": 92,
          "w": 111.36,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "L layers of transformations are applied to the hidden state h, consisting of alternating the self-attention (ATTN) and multi-layer perceptron (MLP) blocks:\n\nh ← h + ATTN(RMSNorm(h))                (4)\nh ← h + MLP(RMSNorm(h)),                (5)\n\nwhere RMSNorm(h) is one of several possible normalizations. It is used first to normalize each embedding to a norm of √d_model, then scales each dimension by a learnable vector of d_model factors, typically initialized to 1. Since the transformation block outputs are added to h, the token embedding norms can vary significantly. To address this, normalization is also applied after the final layer.",
        "md": "L layers of transformations are applied to the hidden state h, consisting of alternating the self-attention (ATTN) and multi-layer perceptron (MLP) blocks:\n\nh ← h + ATTN(RMSNorm(h))                (4)\nh ← h + MLP(RMSNorm(h)),                (5)\n\nwhere RMSNorm(h) is one of several possible normalizations. It is used first to normalize each embedding to a norm of √d_model, then scales each dimension by a learnable vector of d_model factors, typically initialized to 1. Since the transformation block outputs are added to h, the token embedding norms can vary significantly. To address this, normalization is also applied after the final layer.",
        "bBox": {
          "x": 107,
          "y": 132,
          "w": 396.98,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 3,
        "value": "2.2.2 NORMALIZED TRANSFORMER",
        "md": "### 2.2.2 NORMALIZED TRANSFORMER",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 126.36,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "For any points a and b on the surface of a hypersphere in R^d_model, SLERP (Spherical Linear Interpolation) by Shoemake (1985) computes an interpolation along the geodesic (shortest path):\n\nSLERP(a, b; α) = sin((1 - α)θ)/sin(θ) * a + sin(αθ)/sin(θ) * b        (6)\n\nwhere θ = arccos(a · b) is the angle between the points a and b, and α ∈ [0, 1] is the interpolation parameter, with α = 0 returning a and α = 1 returning b. Our experiments suggest that SLERP can be approximated by simple linear interpolation (LERP):\n\nLERP(a, b; w) = (1 - α)a + αb                (7)\n\nLet us rewrite this equation as an update equation in nGPT:\n\na ← a + α(b - a)                              (8)\n\nwhere a is h, and, b is the point suggested by the attention or MLP block. Then, for the gradient g = a - b, a more general form involving a variable matrix B ∈ R^d_model×d_model becomes:\n\na ← a - αBg                                   (9)\n\nIn quasi-Newton methods, B approximates the inverse Hessian matrix H^-1. When B is diagonal with non-negative elements, αB becomes a vector α ∈ R^d_model_≥0 whose elements correspond to the diagonal of B times the learning rate α. We denote α as eigen learning rates (from the German word eigen, meaning \"own,\" referring to the internal structure of the Transformer). We provide some notes in Appendix A.2. Following equation 8, the update equations for the attention and MLP blocks are as follows:\n\nh ← Norm( h + α_A(h_A - h) )                 (10)\nh ← Norm( h + α_M(h_M - h) ),                (11)\n\nwhere α_A ∈ R^d_model_≥0 and α_M ∈ R^d_model_≥0 are learnable parameters applied to the normalized outputs of the attention and MLP blocks h_A = Norm(ATTN(h)) and h_M = Norm(MLP(h)), respectively. The function Norm(x) normalizes any vector x to have unit norm, and, unlike RMSNorm or LayerNorm, does not introduce any element-wise scaling factors. The normalization can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold. Appendix A.3 discusses the extension of our update equations in the context Riemannian optimization. In contrast to the baseline Transformer, no additional normalization is required after the final layer, as the embeddings are already normalized by the proposed scheme.",
        "md": "For any points a and b on the surface of a hypersphere in R^d_model, SLERP (Spherical Linear Interpolation) by Shoemake (1985) computes an interpolation along the geodesic (shortest path):\n\nSLERP(a, b; α) = sin((1 - α)θ)/sin(θ) * a + sin(αθ)/sin(θ) * b        (6)\n\nwhere θ = arccos(a · b) is the angle between the points a and b, and α ∈ [0, 1] is the interpolation parameter, with α = 0 returning a and α = 1 returning b. Our experiments suggest that SLERP can be approximated by simple linear interpolation (LERP):\n\nLERP(a, b; w) = (1 - α)a + αb                (7)\n\nLet us rewrite this equation as an update equation in nGPT:\n\na ← a + α(b - a)                              (8)\n\nwhere a is h, and, b is the point suggested by the attention or MLP block. Then, for the gradient g = a - b, a more general form involving a variable matrix B ∈ R^d_model×d_model becomes:\n\na ← a - αBg                                   (9)\n\nIn quasi-Newton methods, B approximates the inverse Hessian matrix H^-1. When B is diagonal with non-negative elements, αB becomes a vector α ∈ R^d_model_≥0 whose elements correspond to the diagonal of B times the learning rate α. We denote α as eigen learning rates (from the German word eigen, meaning \"own,\" referring to the internal structure of the Transformer). We provide some notes in Appendix A.2. Following equation 8, the update equations for the attention and MLP blocks are as follows:\n\nh ← Norm( h + α_A(h_A - h) )                 (10)\nh ← Norm( h + α_M(h_M - h) ),                (11)\n\nwhere α_A ∈ R^d_model_≥0 and α_M ∈ R^d_model_≥0 are learnable parameters applied to the normalized outputs of the attention and MLP blocks h_A = Norm(ATTN(h)) and h_M = Norm(MLP(h)), respectively. The function Norm(x) normalizes any vector x to have unit norm, and, unlike RMSNorm or LayerNorm, does not introduce any element-wise scaling factors. The normalization can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold. Appendix A.3 discusses the extension of our update equations in the context Riemannian optimization. In contrast to the baseline Transformer, no additional normalization is required after the final layer, as the embeddings are already normalized by the proposed scheme.",
        "bBox": {
          "x": 107,
          "y": 113,
          "w": 397.18,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "2.3 SELF-ATTENTION BLOCK",
        "md": "## 2.3 SELF-ATTENTION BLOCK",
        "bBox": {
          "x": 108,
          "y": 680,
          "w": 108.15,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 3,
        "value": "2.3.1 BASELINE TRANSFORMER",
        "md": "### 2.3.1 BASELINE TRANSFORMER",
        "bBox": {
          "x": 108,
          "y": 113,
          "w": 143.36,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "The attention mechanism is a key component of the Transformer. It allows each token to attend to every other token in the sequence, enabling the model to capture long-range dependencies. The",
        "md": "The attention mechanism is a key component of the Transformer. It allows each token to attend to every other token in the sequence, enabling the model to capture long-range dependencies. The",
        "bBox": {
          "x": 107,
          "y": 719,
          "w": 396.1,
          "h": 9.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": "tion. In contrast to the baseline Transformer, no additional normalization is required after the final"
      }
    ]
  },
  {
    "page": 4,
    "text": "Normalized Transformer\n\n\n\nblock typically starts with a normalization of the input hidden state h using RMSNorm to deal with\nfluctuating norms of embeddings. Then, the normalized h is projected into three separate vectors -\nthe query q, the key k, and the value v:\n                                         q ← hWq , k ← hWk , v ← hWv                                                      (12)\nwhere Wq, Wk, Wv ∈ Rdmodel ×dk are learned projection matrices, and dk is the dimensionality of\nthe query/key vectors. To incorporate positional information, we apply Rotary Position Embeddings\n(RoPE) by Su et al. (2024) to both the query and key vectors. The attention scores are computed by√1dk, then applying a softmax\ntaking the dot product of the query and key vectors, scaling them by\nfunction to obtain attention weights, and finally computing a weighted sum of the value vectors v:\n                                 Attention(q, k, v) ← softmax( qk⊤                       )\n                                                                          √dk    + M        v,                            (13)\nwhere M is a matrix that prevents attending to future tokens by setting the corresponding entries to\n−∞. Specifically, Mi,j = 0 if j ≤ i and Mi,j = −∞ if j > i.\nIn practice, nheads attention heads are used where for each i-th head, separate linear projections\n    i, W ki, W v are applied, and the attention mechanism is computed independently for each head:i\nW q\n                                      hA ← Concat(head1, . . . , headnheads )Wo                                           (14)\nwhere headi = Attention(qi, ki, vi) and WO ∈ Rnheads×dk ×dmodel is a learned projection matrix,\nwhere dk is typically set to dmodel/nheads.\n\n\n\n2.3.2     NORMALIZED TRANSFORMER\nThe matrix-vector multiplication of Wq ∈ Rdmodel×dk of the i-th head1 and h ∈ Rdmodel can be viewed\nas a dot product between the columns of Wq and h. In the baseline Transformer, all matrices,\nincluding Wq are unconstrained, leading to unbounded values in q. We propose to normalize Wq,\nWk, Wv and Wo along their embedding dimension so that the computed dot products with h can\nbe interpreted as cosine similarity between unit norm vectors bounded in [−1, 1]. Thus, all attention\nmatrices can be viewed as collections of normalized embedding vectors to be compared with.\nWhile each element of q and k is now bounded, the norms of these two vectors can still vary.\nMoreover, injection of positional information by RoPE further distorts q and k. We propose to\nadditionally normalize q and k, ensuring that the dot product of every query and key is under control:\n                                                    q ← Norm(q)sqk                                                        (15)\n                                                    k ← Norm(k)sqk ,                                                      (16)\nwhere sqk ∈ Rdk is a vector2 of trainable scaling factors for the i-th head.\nIn the original Transformer, the softmax scaling factor 1/√dk in equation 13 is introduced to account\nfor the expected variance of dk in the dot product of non-normalized query and key vectors. In the\nnormalized Transformer, the expected variance of the dot product between normalized query and\nthe softmax scaling factor is set to 1, this is equivalent to initializing the scaling factors sqk √dk1/4.\nkey vectors is 1/dk. To restore a variance of 1, the softmax scaling factor should instead be                              . If\n                                                                                                                     at dk\n\n\n\n2.4     MLP BLOCK\n\n\n\n2.4.1     BASELINE TRANSFORMER\nThe input hidden state h of the MLP block is first normalized using RMSNorm and then passed\nthrough two separate linear projections, producing two intermediate vectors (we omit bias terms):\n                                               u ← hWu,           ν ← hWν                                                 (17)\n    1All the following equations are defined per head but we omit i-th head index for the sake of readability.\n    2There is no need for separate scaling factors for q and k as the scaling would simply be applied element-\nwise when computing the dot product between q and k.\n\n\n\n                                                               4",
    "md": "# Normalized Transformer\n\nblock typically starts with a normalization of the input hidden state h using RMSNorm to deal with\nfluctuating norms of embeddings. Then, the normalized h is projected into three separate vectors -\nthe query q, the key k, and the value v:\n\n$$q ← hW_q, k ← hW_k, v ← hW_v$$                                                      (12)\n\nwhere $W_q, W_k, W_v ∈ R^{d_{model}×d_k}$ are learned projection matrices, and $d_k$ is the dimensionality of\nthe query/key vectors. To incorporate positional information, we apply Rotary Position Embeddings\n(RoPE) by Su et al. (2024) to both the query and key vectors. The attention scores are computed by\ntaking the dot product of the query and key vectors, scaling them by $\\frac{1}{\\sqrt{d_k}}$, then applying a softmax\nfunction to obtain attention weights, and finally computing a weighted sum of the value vectors v:\n\n$$\\text{Attention}(q, k, v) ← \\text{softmax} \\left(\\frac{qk^⊤}{\\sqrt{d_k}} + M\\right) v,$$                            (13)\n\nwhere M is a matrix that prevents attending to future tokens by setting the corresponding entries to\n−∞. Specifically, $M_{i,j} = 0$ if $j ≤ i$ and $M_{i,j} = −∞$ if $j > i$.\n\nIn practice, $n_{heads}$ attention heads are used where for each i-th head, separate linear projections\n$W^i_q, W^i_k, W^i_v$ are applied, and the attention mechanism is computed independently for each head:\n\n$$h_A ← \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_{n_{heads}})W_O$$                                           (14)\n\nwhere $\\text{head}_i = \\text{Attention}(q^i, k^i, v^i)$ and $W_O ∈ R^{n_{heads}×d_k×d_{model}}$ is a learned projection matrix,\nwhere $d_k$ is typically set to $d_{model}/n_{heads}$.\n\n## 2.3.2 NORMALIZED TRANSFORMER\n\nThe matrix-vector multiplication of $W_q ∈ R^{d_{model}×d_k}$ of the i-th head¹ and $h ∈ R^{d_{model}}$ can be viewed\nas a dot product between the columns of $W_q$ and h. In the baseline Transformer, all matrices,\nincluding $W_q$ are unconstrained, leading to unbounded values in q. We propose to normalize $W_q$,\n$W_k, W_v$ and $W_o$ along their embedding dimension so that the computed dot products with h can\nbe interpreted as cosine similarity between unit norm vectors bounded in [−1, 1]. Thus, all attention\nmatrices can be viewed as collections of normalized embedding vectors to be compared with.\n\nWhile each element of q and k is now bounded, the norms of these two vectors can still vary.\nMoreover, injection of positional information by RoPE further distorts q and k. We propose to\nadditionally normalize q and k, ensuring that the dot product of every query and key is under control:\n\n$$q ← \\text{Norm}(q)s_{qk}$$                                                        (15)\n$$k ← \\text{Norm}(k)s_{qk},$$                                                      (16)\n\nwhere $s_{qk} ∈ R^{d_k}$ is a vector² of trainable scaling factors for the i-th head.\n\nIn the original Transformer, the softmax scaling factor $1/\\sqrt{d_k}$ in equation 13 is introduced to account\nfor the expected variance of $d_k$ in the dot product of non-normalized query and key vectors. In the\nnormalized Transformer, the expected variance of the dot product between normalized query and\nkey vectors is $1/d_k$. To restore a variance of 1, the softmax scaling factor should instead be $\\sqrt{d_k}$. If\nthe softmax scaling factor is set to 1, this is equivalent to initializing the scaling factors $s_{qk}$ at $d_k^{1/4}$.\n\n## 2.4 MLP BLOCK\n\n### 2.4.1 BASELINE TRANSFORMER\n\nThe input hidden state h of the MLP block is first normalized using RMSNorm and then passed\nthrough two separate linear projections, producing two intermediate vectors (we omit bias terms):\n\n$$u ← hW_u,   v ← hW_v$$                                                 (17)\n\n¹All the following equations are defined per head but we omit i-th head index for the sake of readability.\n²There is no need for separate scaling factors for q and k as the scaling would simply be applied element-\nwise when computing the dot product between q and k.",
    "images": [
      {
        "name": "page_4.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 126.36,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "block typically starts with a normalization of the input hidden state h using RMSNorm to deal with\nfluctuating norms of embeddings. Then, the normalized h is projected into three separate vectors -\nthe query q, the key k, and the value v:\n\n$$q ← hW_q, k ← hW_k, v ← hW_v$$                                                      (12)\n\nwhere $W_q, W_k, W_v ∈ R^{d_{model}×d_k}$ are learned projection matrices, and $d_k$ is the dimensionality of\nthe query/key vectors. To incorporate positional information, we apply Rotary Position Embeddings\n(RoPE) by Su et al. (2024) to both the query and key vectors. The attention scores are computed by\ntaking the dot product of the query and key vectors, scaling them by $\\frac{1}{\\sqrt{d_k}}$, then applying a softmax\nfunction to obtain attention weights, and finally computing a weighted sum of the value vectors v:\n\n$$\\text{Attention}(q, k, v) ← \\text{softmax} \\left(\\frac{qk^⊤}{\\sqrt{d_k}} + M\\right) v,$$                            (13)\n\nwhere M is a matrix that prevents attending to future tokens by setting the corresponding entries to\n−∞. Specifically, $M_{i,j} = 0$ if $j ≤ i$ and $M_{i,j} = −∞$ if $j > i$.\n\nIn practice, $n_{heads}$ attention heads are used where for each i-th head, separate linear projections\n$W^i_q, W^i_k, W^i_v$ are applied, and the attention mechanism is computed independently for each head:\n\n$$h_A ← \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_{n_{heads}})W_O$$                                           (14)\n\nwhere $\\text{head}_i = \\text{Attention}(q^i, k^i, v^i)$ and $W_O ∈ R^{n_{heads}×d_k×d_{model}}$ is a learned projection matrix,\nwhere $d_k$ is typically set to $d_{model}/n_{heads}$.",
        "md": "block typically starts with a normalization of the input hidden state h using RMSNorm to deal with\nfluctuating norms of embeddings. Then, the normalized h is projected into three separate vectors -\nthe query q, the key k, and the value v:\n\n$$q ← hW_q, k ← hW_k, v ← hW_v$$                                                      (12)\n\nwhere $W_q, W_k, W_v ∈ R^{d_{model}×d_k}$ are learned projection matrices, and $d_k$ is the dimensionality of\nthe query/key vectors. To incorporate positional information, we apply Rotary Position Embeddings\n(RoPE) by Su et al. (2024) to both the query and key vectors. The attention scores are computed by\ntaking the dot product of the query and key vectors, scaling them by $\\frac{1}{\\sqrt{d_k}}$, then applying a softmax\nfunction to obtain attention weights, and finally computing a weighted sum of the value vectors v:\n\n$$\\text{Attention}(q, k, v) ← \\text{softmax} \\left(\\frac{qk^⊤}{\\sqrt{d_k}} + M\\right) v,$$                            (13)\n\nwhere M is a matrix that prevents attending to future tokens by setting the corresponding entries to\n−∞. Specifically, $M_{i,j} = 0$ if $j ≤ i$ and $M_{i,j} = −∞$ if $j > i$.\n\nIn practice, $n_{heads}$ attention heads are used where for each i-th head, separate linear projections\n$W^i_q, W^i_k, W^i_v$ are applied, and the attention mechanism is computed independently for each head:\n\n$$h_A ← \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_{n_{heads}})W_O$$                                           (14)\n\nwhere $\\text{head}_i = \\text{Attention}(q^i, k^i, v^i)$ and $W_O ∈ R^{n_{heads}×d_k×d_{model}}$ is a learned projection matrix,\nwhere $d_k$ is typically set to $d_{model}/n_{heads}$.",
        "bBox": {
          "x": 107,
          "y": 92,
          "w": 397.4,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "2.3.2 NORMALIZED TRANSFORMER",
        "md": "## 2.3.2 NORMALIZED TRANSFORMER",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 126.36,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "The matrix-vector multiplication of $W_q ∈ R^{d_{model}×d_k}$ of the i-th head¹ and $h ∈ R^{d_{model}}$ can be viewed\nas a dot product between the columns of $W_q$ and h. In the baseline Transformer, all matrices,\nincluding $W_q$ are unconstrained, leading to unbounded values in q. We propose to normalize $W_q$,\n$W_k, W_v$ and $W_o$ along their embedding dimension so that the computed dot products with h can\nbe interpreted as cosine similarity between unit norm vectors bounded in [−1, 1]. Thus, all attention\nmatrices can be viewed as collections of normalized embedding vectors to be compared with.\n\nWhile each element of q and k is now bounded, the norms of these two vectors can still vary.\nMoreover, injection of positional information by RoPE further distorts q and k. We propose to\nadditionally normalize q and k, ensuring that the dot product of every query and key is under control:\n\n$$q ← \\text{Norm}(q)s_{qk}$$                                                        (15)\n$$k ← \\text{Norm}(k)s_{qk},$$                                                      (16)\n\nwhere $s_{qk} ∈ R^{d_k}$ is a vector² of trainable scaling factors for the i-th head.\n\nIn the original Transformer, the softmax scaling factor $1/\\sqrt{d_k}$ in equation 13 is introduced to account\nfor the expected variance of $d_k$ in the dot product of non-normalized query and key vectors. In the\nnormalized Transformer, the expected variance of the dot product between normalized query and\nkey vectors is $1/d_k$. To restore a variance of 1, the softmax scaling factor should instead be $\\sqrt{d_k}$. If\nthe softmax scaling factor is set to 1, this is equivalent to initializing the scaling factors $s_{qk}$ at $d_k^{1/4}$.",
        "md": "The matrix-vector multiplication of $W_q ∈ R^{d_{model}×d_k}$ of the i-th head¹ and $h ∈ R^{d_{model}}$ can be viewed\nas a dot product between the columns of $W_q$ and h. In the baseline Transformer, all matrices,\nincluding $W_q$ are unconstrained, leading to unbounded values in q. We propose to normalize $W_q$,\n$W_k, W_v$ and $W_o$ along their embedding dimension so that the computed dot products with h can\nbe interpreted as cosine similarity between unit norm vectors bounded in [−1, 1]. Thus, all attention\nmatrices can be viewed as collections of normalized embedding vectors to be compared with.\n\nWhile each element of q and k is now bounded, the norms of these two vectors can still vary.\nMoreover, injection of positional information by RoPE further distorts q and k. We propose to\nadditionally normalize q and k, ensuring that the dot product of every query and key is under control:\n\n$$q ← \\text{Norm}(q)s_{qk}$$                                                        (15)\n$$k ← \\text{Norm}(k)s_{qk},$$                                                      (16)\n\nwhere $s_{qk} ∈ R^{d_k}$ is a vector² of trainable scaling factors for the i-th head.\n\nIn the original Transformer, the softmax scaling factor $1/\\sqrt{d_k}$ in equation 13 is introduced to account\nfor the expected variance of $d_k$ in the dot product of non-normalized query and key vectors. In the\nnormalized Transformer, the expected variance of the dot product between normalized query and\nkey vectors is $1/d_k$. To restore a variance of 1, the softmax scaling factor should instead be $\\sqrt{d_k}$. If\nthe softmax scaling factor is set to 1, this is equivalent to initializing the scaling factors $s_{qk}$ at $d_k^{1/4}$.",
        "bBox": {
          "x": 107,
          "y": 36,
          "w": 396.58,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "2.4 MLP BLOCK",
        "md": "## 2.4 MLP BLOCK",
        "bBox": {
          "x": 108,
          "y": 619,
          "w": 54.15,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 3,
        "value": "2.4.1 BASELINE TRANSFORMER",
        "md": "### 2.4.1 BASELINE TRANSFORMER",
        "bBox": {
          "x": 108,
          "y": 284,
          "w": 111.36,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "The input hidden state h of the MLP block is first normalized using RMSNorm and then passed\nthrough two separate linear projections, producing two intermediate vectors (we omit bias terms):\n\n$$u ← hW_u,   v ← hW_v$$                                                 (17)\n\n¹All the following equations are defined per head but we omit i-th head index for the sake of readability.\n²There is no need for separate scaling factors for q and k as the scaling would simply be applied element-\nwise when computing the dot product between q and k.",
        "md": "The input hidden state h of the MLP block is first normalized using RMSNorm and then passed\nthrough two separate linear projections, producing two intermediate vectors (we omit bias terms):\n\n$$u ← hW_u,   v ← hW_v$$                                                 (17)\n\n¹All the following equations are defined per head but we omit i-th head index for the sake of readability.\n²There is no need for separate scaling factors for q and k as the scaling would simply be applied element-\nwise when computing the dot product between q and k.",
        "bBox": {
          "x": 108,
          "y": 210,
          "w": 395.18,
          "h": 9.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": "taking the dot product of the query and key vectors, scaling them by"
      },
      {
        "text": "taking the dot product of the query and key vectors, scaling them by"
      },
      {
        "text": "1"
      },
      {
        "text": "2"
      },
      {
        "text": "in the dot product of non-normalized query and key vectors. In the"
      }
    ]
  },
  {
    "page": 5,
    "text": " Normalized Transformer\n\n\n\n where Wu, Wν ∈ Rdmodel ×dMLP are the learned weight matrices. The intermediate vectors u and ν\n are combined using a gated activation function called SwiGLU defined by Shazeer (2020) as:\n                                           SwiGLU(u, ν) ← u · SiLU(ν)                                                    (18)\n where SiLU(ν) = ν · σ(ν), and σ(ν) is the sigmoid function. The result of the gated activation is\n then passed through a final linear transformation WoMLP ∈ RdMLP×dmodel :\n                                             hM ← SwiGLU(u, ν)WoMLP                                                      (19)\n\n\n\n 2.4.2     NORMALIZED TRANSFORMER\n We propose to normalize matrices Wu and Wν along the embedding dimension so that the u and\n ν vectors represent the cosine similarity between h and vectors stored in Wu MLP :\n                                                                                   To control their impact, we introduce scaling factors su ∈ RdMLP and sν ∈ Rdand Wν , respectively.\n                                                   u ← usu√dmodel,,                                                      (20)\n                                                   ν ← νsν                                                               (21)\nwhere the rescaling of ν by √dmodel is needed to benefit from the non-linearity of SiLU (see the\nAppendix A.1). The output of the MLP block is invariant to rescaling of u by a scalar.\n\n\n\n 2.5    EFFECTIVE LEARNING RATES IN ADAM\n The core of the Adam algorithm by Kingma (2014) is as follows:\n                                               m ← β1m + (1 − β1)g\n                                                v ← β2v + (1 − β2)g2\n                                                θ ← θ − αm/(√v + ǫ),                                                     (22)\n where θ is the parameter vector, g is the batch gradient, m is the momentum, v is the estimate\n of the per-element gradient amplitudes, α is the scheduled learning rate, ǫ is a small constant, and\n β1 < β2 are momentum factors close to 1. We cite the text of the original Adam paper using our∣\n                                                                                                ∣E[g]/√E[g2∣\n notation: In more common scenarios, we will have that m√v ≈ ±1 since∣                                         ]∣ ≤ 1. The\n                                                                                                                ∣\neffective magnitude of the steps taken in parameter space at each timestep is approximately bounded\nby the stepsize setting α. Thus, α controls the effective step-size in the search space, while the ratio\n mv can temporarily increase (respectively, decrease) the step-size if the current amplitude of per-\n √\nparameter momentum is greater (respectively, smaller) than its estimated value over longer time\nhorizon. Consider an example where θi = 0.01 and the global learning rate is 0.001. If the gradient\n amplitude remains stable (i.e.,√vi ≈ 1), it would take 0.02−0.01mi\n                                                                                   = 10 steps to double θi. However,\n                                                                         0.001\nif θi = 1.0, it would take 2.0−1.0\nin the second case, the number of steps would only decrease if mi > √v.0.001 = 1000 steps to double. Even if the gradient’s amplitude is larger\n In nGPT, for any trainable vector of scaling parameters such as sa, we use two scalars sa,init and\nsa,scale. When initializing sa as a trainable parameter, its initial value is set to sa,scale. However,\nduring the forward pass we restore its actual value by multiplying sa,init/sa,scale. This allows us to\ncontrol the effective learning rate for sa by adjusting sa,scale, while keeping the global learning rate\nunchanged. For example, setting sa,init = 1 and sa,scale = 1/√dmodel ensures that this parameter\n is updated with the same effective learning rate as other normalized parameters in the network.\n\n\n\n 2.6    SUMMARY OF MODIFICATIONS\n The recipe to convert the baseline Transformer into the normalized Transformer is as follows:\n\n\n\n        1. Remove all normalization layers such as RMSNorm or LayerNorm.\n        2. After each training step, normalize all matrices (Einput, Eoutput, Wq, Wk, Wv, Wo, Wu,\n            Wν and WoMLP) along their embedding dimension.\n        3. Replace the update equations 4 and 5 by equations 10 and 11, where αA (and also αM) is\n            treated with αA,init = 0.05 (in order of 1/nlayers) and αA,scale = 1/√dmodel.\n\n\n\n                                                               5",
    "md": "# Normalized Transformer\n\nwhere Wu, Wν ∈ ℝ^(dmodel×dMLP) are the learned weight matrices. The intermediate vectors u and ν are combined using a gated activation function called SwiGLU defined by Shazeer (2020) as:\n\nSwiGLU(u, ν) ← u · SiLU(ν)                                                    (18)\n\nwhere SiLU(ν) = ν · σ(ν), and σ(ν) is the sigmoid function. The result of the gated activation is then passed through a final linear transformation WoMLP ∈ ℝ^(dMLP×dmodel):\n\nhM ← SwiGLU(u, ν)WoMLP                                                      (19)\n\n## 2.4.2 NORMALIZED TRANSFORMER\n\nWe propose to normalize matrices Wu and Wν along the embedding dimension so that the u and ν vectors represent the cosine similarity between h and vectors stored in Wu and Wν, respectively. To control their impact, we introduce scaling factors su ∈ ℝ^dMLP and sν ∈ ℝ^dMLP:\n\nu ← usu,                                                               (20)\n\nν ← νsν√dmodel,                                                         (21)\n\nwhere the rescaling of ν by √dmodel is needed to benefit from the non-linearity of SiLU (see the Appendix A.1). The output of the MLP block is invariant to rescaling of u by a scalar.\n\n## 2.5 EFFECTIVE LEARNING RATES IN ADAM\n\nThe core of the Adam algorithm by Kingma (2014) is as follows:\n\nm ← β1m + (1 − β1)g\nv ← β2v + (1 − β2)g^2\nθ ← θ − αm/(√v + ǫ),                                                     (22)\n\nwhere θ is the parameter vector, g is the batch gradient, m is the momentum, v is the estimate of the per-element gradient amplitudes, α is the scheduled learning rate, ǫ is a small constant, and β1 < β2 are momentum factors close to 1. We cite the text of the original Adam paper using our notation: In more common scenarios, we will have that m/√v ≈ ±1 since |E[g]/√E[g^2]| ≤ 1. The effective magnitude of the steps taken in parameter space at each timestep is approximately bounded by the stepsize setting α. Thus, α controls the effective step-size in the search space, while the ratio m/√v can temporarily increase (respectively, decrease) the step-size if the current amplitude of per-parameter momentum is greater (respectively, smaller) than its estimated value over longer time horizon. Consider an example where θi = 0.01 and the global learning rate is 0.001. If the gradient amplitude remains stable (i.e., √vi ≈ 1), it would take (0.02−0.01)/(0.001mi) = 10 steps to double θi. However, if θi = 1.0, it would take (2.0−1.0)/(0.001) = 1000 steps to double. Even if the gradient's amplitude is larger in the second case, the number of steps would only decrease if mi > √v.\n\nIn nGPT, for any trainable vector of scaling parameters such as sa, we use two scalars sa,init and sa,scale. When initializing sa as a trainable parameter, its initial value is set to sa,scale. However, during the forward pass we restore its actual value by multiplying sa,init/sa,scale. This allows us to control the effective learning rate for sa by adjusting sa,scale, while keeping the global learning rate unchanged. For example, setting sa,init = 1 and sa,scale = 1/√dmodel ensures that this parameter is updated with the same effective learning rate as other normalized parameters in the network.\n\n## 2.6 SUMMARY OF MODIFICATIONS\n\nThe recipe to convert the baseline Transformer into the normalized Transformer is as follows:\n\n1. Remove all normalization layers such as RMSNorm or LayerNorm.\n2. After each training step, normalize all matrices (Einput, Eoutput, Wq, Wk, Wv, Wo, Wu, Wν and WoMLP) along their embedding dimension.\n3. Replace the update equations 4 and 5 by equations 10 and 11, where αA (and also αM) is treated with αA,init = 0.05 (in order of 1/nlayers) and αA,scale = 1/√dmodel.\n\n5",
    "images": [
      {
        "name": "page_5.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 126.36,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "where Wu, Wν ∈ ℝ^(dmodel×dMLP) are the learned weight matrices. The intermediate vectors u and ν are combined using a gated activation function called SwiGLU defined by Shazeer (2020) as:\n\nSwiGLU(u, ν) ← u · SiLU(ν)                                                    (18)\n\nwhere SiLU(ν) = ν · σ(ν), and σ(ν) is the sigmoid function. The result of the gated activation is then passed through a final linear transformation WoMLP ∈ ℝ^(dMLP×dmodel):\n\nhM ← SwiGLU(u, ν)WoMLP                                                      (19)",
        "md": "where Wu, Wν ∈ ℝ^(dmodel×dMLP) are the learned weight matrices. The intermediate vectors u and ν are combined using a gated activation function called SwiGLU defined by Shazeer (2020) as:\n\nSwiGLU(u, ν) ← u · SiLU(ν)                                                    (18)\n\nwhere SiLU(ν) = ν · σ(ν), and σ(ν) is the sigmoid function. The result of the gated activation is then passed through a final linear transformation WoMLP ∈ ℝ^(dMLP×dmodel):\n\nhM ← SwiGLU(u, ν)WoMLP                                                      (19)",
        "bBox": {
          "x": 108,
          "y": 103,
          "w": 395.8,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "2.4.2 NORMALIZED TRANSFORMER",
        "md": "## 2.4.2 NORMALIZED TRANSFORMER",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 126.36,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "We propose to normalize matrices Wu and Wν along the embedding dimension so that the u and ν vectors represent the cosine similarity between h and vectors stored in Wu and Wν, respectively. To control their impact, we introduce scaling factors su ∈ ℝ^dMLP and sν ∈ ℝ^dMLP:\n\nu ← usu,                                                               (20)\n\nν ← νsν√dmodel,                                                         (21)\n\nwhere the rescaling of ν by √dmodel is needed to benefit from the non-linearity of SiLU (see the Appendix A.1). The output of the MLP block is invariant to rescaling of u by a scalar.",
        "md": "We propose to normalize matrices Wu and Wν along the embedding dimension so that the u and ν vectors represent the cosine similarity between h and vectors stored in Wu and Wν, respectively. To control their impact, we introduce scaling factors su ∈ ℝ^dMLP and sν ∈ ℝ^dMLP:\n\nu ← usu,                                                               (20)\n\nν ← νsν√dmodel,                                                         (21)\n\nwhere the rescaling of ν by √dmodel is needed to benefit from the non-linearity of SiLU (see the Appendix A.1). The output of the MLP block is invariant to rescaling of u by a scalar.",
        "bBox": {
          "x": 107,
          "y": 207,
          "w": 396.78,
          "h": 17.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "2.5 EFFECTIVE LEARNING RATES IN ADAM",
        "md": "## 2.5 EFFECTIVE LEARNING RATES IN ADAM",
        "bBox": {
          "x": 108,
          "y": 318,
          "w": 164.21,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "The core of the Adam algorithm by Kingma (2014) is as follows:\n\nm ← β1m + (1 − β1)g\nv ← β2v + (1 − β2)g^2\nθ ← θ − αm/(√v + ǫ),                                                     (22)\n\nwhere θ is the parameter vector, g is the batch gradient, m is the momentum, v is the estimate of the per-element gradient amplitudes, α is the scheduled learning rate, ǫ is a small constant, and β1 < β2 are momentum factors close to 1. We cite the text of the original Adam paper using our notation: In more common scenarios, we will have that m/√v ≈ ±1 since |E[g]/√E[g^2]| ≤ 1. The effective magnitude of the steps taken in parameter space at each timestep is approximately bounded by the stepsize setting α. Thus, α controls the effective step-size in the search space, while the ratio m/√v can temporarily increase (respectively, decrease) the step-size if the current amplitude of per-parameter momentum is greater (respectively, smaller) than its estimated value over longer time horizon. Consider an example where θi = 0.01 and the global learning rate is 0.001. If the gradient amplitude remains stable (i.e., √vi ≈ 1), it would take (0.02−0.01)/(0.001mi) = 10 steps to double θi. However, if θi = 1.0, it would take (2.0−1.0)/(0.001) = 1000 steps to double. Even if the gradient's amplitude is larger in the second case, the number of steps would only decrease if mi > √v.\n\nIn nGPT, for any trainable vector of scaling parameters such as sa, we use two scalars sa,init and sa,scale. When initializing sa as a trainable parameter, its initial value is set to sa,scale. However, during the forward pass we restore its actual value by multiplying sa,init/sa,scale. This allows us to control the effective learning rate for sa by adjusting sa,scale, while keeping the global learning rate unchanged. For example, setting sa,init = 1 and sa,scale = 1/√dmodel ensures that this parameter is updated with the same effective learning rate as other normalized parameters in the network.",
        "md": "The core of the Adam algorithm by Kingma (2014) is as follows:\n\nm ← β1m + (1 − β1)g\nv ← β2v + (1 − β2)g^2\nθ ← θ − αm/(√v + ǫ),                                                     (22)\n\nwhere θ is the parameter vector, g is the batch gradient, m is the momentum, v is the estimate of the per-element gradient amplitudes, α is the scheduled learning rate, ǫ is a small constant, and β1 < β2 are momentum factors close to 1. We cite the text of the original Adam paper using our notation: In more common scenarios, we will have that m/√v ≈ ±1 since |E[g]/√E[g^2]| ≤ 1. The effective magnitude of the steps taken in parameter space at each timestep is approximately bounded by the stepsize setting α. Thus, α controls the effective step-size in the search space, while the ratio m/√v can temporarily increase (respectively, decrease) the step-size if the current amplitude of per-parameter momentum is greater (respectively, smaller) than its estimated value over longer time horizon. Consider an example where θi = 0.01 and the global learning rate is 0.001. If the gradient amplitude remains stable (i.e., √vi ≈ 1), it would take (0.02−0.01)/(0.001mi) = 10 steps to double θi. However, if θi = 1.0, it would take (2.0−1.0)/(0.001) = 1000 steps to double. Even if the gradient's amplitude is larger in the second case, the number of steps would only decrease if mi > √v.\n\nIn nGPT, for any trainable vector of scaling parameters such as sa, we use two scalars sa,init and sa,scale. When initializing sa as a trainable parameter, its initial value is set to sa,scale. However, during the forward pass we restore its actual value by multiplying sa,init/sa,scale. This allows us to control the effective learning rate for sa by adjusting sa,scale, while keeping the global learning rate unchanged. For example, setting sa,init = 1 and sa,scale = 1/√dmodel ensures that this parameter is updated with the same effective learning rate as other normalized parameters in the network.",
        "bBox": {
          "x": 107,
          "y": 246,
          "w": 397.09,
          "h": 17.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "2.6 SUMMARY OF MODIFICATIONS",
        "md": "## 2.6 SUMMARY OF MODIFICATIONS",
        "bBox": {
          "x": 108,
          "y": 637,
          "w": 128.75,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "The recipe to convert the baseline Transformer into the normalized Transformer is as follows:\n\n1. Remove all normalization layers such as RMSNorm or LayerNorm.\n2. After each training step, normalize all matrices (Einput, Eoutput, Wq, Wk, Wv, Wo, Wu, Wν and WoMLP) along their embedding dimension.\n3. Replace the update equations 4 and 5 by equations 10 and 11, where αA (and also αM) is treated with αA,init = 0.05 (in order of 1/nlayers) and αA,scale = 1/√dmodel.\n\n5",
        "md": "The recipe to convert the baseline Transformer into the normalized Transformer is as follows:\n\n1. Remove all normalization layers such as RMSNorm or LayerNorm.\n2. After each training step, normalize all matrices (Einput, Eoutput, Wq, Wk, Wv, Wo, Wu, Wν and WoMLP) along their embedding dimension.\n3. Replace the update equations 4 and 5 by equations 10 and 11, where αA (and also αM) is treated with αA,init = 0.05 (in order of 1/nlayers) and αA,scale = 1/√dmodel.\n\n5",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 372.97,
          "h": 16.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": "(in order of"
      },
      {
        "text": "(in order of"
      },
      {
        "text": ") and"
      },
      {
        "text": ""
      }
    ]
  },
  {
    "page": 6,
    "text": "Normalized Transformer\n\n\n\n         4. Change the softmax scaling factor in attention from 1/√dk to √dk. Implement the rescal-\n             ing and normalization (normalization here is optional) of q and k as in equations 15 and\n             16, where sqk is treated with sqk,init = 1 and sqk,scale = 1/√dmodel.Validation loss\n         5. Implement the rescaling of the intermediate state of the MLP block using equations 20 and\n             21, where su (and also sν ) is treated with su,init = 1 and su,scale = 1\n         6. Implement the rescaling of logits using equation 3, where sz is treated with sz,init = 1 and\n             sz,scale = 1/√dmodel.\n         7. Remove weight decay and learning rate warmup.\n\n\n\n3      EXPERIMENTS\nWe train both the baseline Transformer (GPT) and the normalized Transformer (nGPT) on the Open-Validation lossValidation lossWebText dataset (Gokaslan & Cohen, 2019) and evaluate them on a set of standard downstream\ntasks. We experiment with models containing 0.5B and 1B parameters, including the embeddings.\nFor both GPT and nGPT, we report results using the best initial learning rate settings (see Appendix\nA.6). A detailed description of the setup and hyperparameters is in Appendix A.5.\n\n\n\n3.1      ACCELERATION OF TRAINING\n                                                   2.8           4k context: OpenWebText\n\n\n\n                                                                               GPT with 200k budget\n                                                   2.7                         nGPT with 20k budget\n                                                                               nGPT with 200k budget\n                                                   2.6\n\n\n\n                                                   2.5\n                                                   2.4Validation loss\n\n\n\n                                                   2.3\n\n\n\n                                                   2.2\n\n\n\n                                                   2.1\n\n\n\n                                                   2\n                                                    0   20   40  60   80  100 120 140 160 180 200\n                                                                   Iterations in thousands\n\n\n\n          Figure 1: Validation loss during training of 1B GPT and nGPT with 4k context length.\n\n\n\nFigure 1 presents the validation loss during the training of GPT and nGPT models with 1 billion pa-\nrameters and a sample length of 4k tokens. After 20k iterations, nGPT achieves the same validation\nloss that GPT reaches only after 200k iterations (approximately 400 billion tokens), demonstrating\na 10x speedup in terms of iterations and tokens used.3\n    2.4        1K context: OpenWebText              2.4        4K context: OpenWebText               2.4       8K context: OpenWebText\n\n\n\n                                     GPT 0.5B                                        GPT 0.5B                                         GPT 0.5B\n   2.35                              GPT 1B        2.35                              GPT 1B         2.35                              GPT 1B\n                                     nGPT 0.5B                                       nGPT 0.5B                                        nGPT 0.5B\n    2.3                              nGPT 1B        2.3                              nGPT 1B         2.3                              nGPT 1B\n\n\n\n   2.25                                            2.25                                             2.25\n\n\n\n    2.2                                             2.2                                              2.2\n\n\n\n   2.15                                            2.15                                             2.15\n\n\n\n    2.1                                             2.1                                              2.1\n\n\n\n   2.05                                            2.05                                             2.05\n\n\n\n     20   50 100 150 200 250 300 350 400 450 500     20   50 100 150 200 250 300 350 400 450 500     20 100 200 300 400 500 600 700 800 900 1000\n\n\n\n                Training tokens in billions                     Training tokens in billions                     Training tokens in billions\n\n\n\nFigure 2: Final validation loss (y-axis) for training runs with different computation budgets in tokens\n(x-axis). The training of 0.5B and 1B nGPT models is about 4x, 10x and 20x faster (in terms of\ntokens) on 1k, 4k and 8k context lengths, respectively.\n\n\n\n     3While the time per step of nGPT is higher (80% - for 4k, and 60% - for 8k context respectively), it can be\nreduced after code optimization. Also the overhead is less significant for larger networks (see Appendix A.4).\n\n\n\n                                                                         6",
    "md": "# Normalized Transformer\n\n4. Change the softmax scaling factor in attention from 1/√dk to √dk. Implement the rescaling and normalization (normalization here is optional) of q and k as in equations 15 and 16, where sqk is treated with sqk,init = 1 and sqk,scale = 1/√dmodel.\n\n5. Implement the rescaling of the intermediate state of the MLP block using equations 20 and 21, where su (and also sν) is treated with su,init = 1 and su,scale = 1\n\n6. Implement the rescaling of logits using equation 3, where sz is treated with sz,init = 1 and sz,scale = 1/√dmodel.\n\n7. Remove weight decay and learning rate warmup.\n\n## 3 EXPERIMENTS\n\nWe train both the baseline Transformer (GPT) and the normalized Transformer (nGPT) on the OpenWebText dataset (Gokaslan & Cohen, 2019) and evaluate them on a set of standard downstream tasks. We experiment with models containing 0.5B and 1B parameters, including the embeddings. For both GPT and nGPT, we report results using the best initial learning rate settings (see Appendix A.6). A detailed description of the setup and hyperparameters is in Appendix A.5.\n\n### 3.1 ACCELERATION OF TRAINING\n\nFigure 1: Validation loss during training of 1B GPT and nGPT with 4k context length.\n\n| 4k context: OpenWebText |\n|--------------------------|\n| GPT with 200k budget     |\n| nGPT with 20k budget     |\n| nGPT with 200k budget    |\n\nFigure 1 presents the validation loss during the training of GPT and nGPT models with 1 billion parameters and a sample length of 4k tokens. After 20k iterations, nGPT achieves the same validation loss that GPT reaches only after 200k iterations (approximately 400 billion tokens), demonstrating a 10x speedup in terms of iterations and tokens used.³\n\nFigure 2: Final validation loss (y-axis) for training runs with different computation budgets in tokens (x-axis). The training of 0.5B and 1B nGPT models is about 4x, 10x and 20x faster (in terms of tokens) on 1k, 4k and 8k context lengths, respectively.\n\n| 1K context: OpenWebText | 4K context: OpenWebText | 8K context: OpenWebText |\n|-------------------------|-------------------------|-------------------------|\n| GPT 0.5B                | GPT 0.5B                | GPT 0.5B                |\n| GPT 1B                  | GPT 1B                  | GPT 1B                  |\n| nGPT 0.5B               | nGPT 0.5B               | nGPT 0.5B               |\n| nGPT 1B                 | nGPT 1B                 | nGPT 1B                 |\n\n³While the time per step of nGPT is higher (80% - for 4k, and 60% - for 8k context respectively), it can be reduced after code optimization. Also the overhead is less significant for larger networks (see Appendix A.4).",
    "images": [
      {
        "name": "page_6.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "4. Change the softmax scaling factor in attention from 1/√dk to √dk. Implement the rescaling and normalization (normalization here is optional) of q and k as in equations 15 and 16, where sqk is treated with sqk,init = 1 and sqk,scale = 1/√dmodel.\n\n5. Implement the rescaling of the intermediate state of the MLP block using equations 20 and 21, where su (and also sν) is treated with su,init = 1 and su,scale = 1\n\n6. Implement the rescaling of logits using equation 3, where sz is treated with sz,init = 1 and sz,scale = 1/√dmodel.\n\n7. Remove weight decay and learning rate warmup.",
        "md": "4. Change the softmax scaling factor in attention from 1/√dk to √dk. Implement the rescaling and normalization (normalization here is optional) of q and k as in equations 15 and 16, where sqk is treated with sqk,init = 1 and sqk,scale = 1/√dmodel.\n\n5. Implement the rescaling of the intermediate state of the MLP block using equations 20 and 21, where su (and also sν) is treated with su,init = 1 and su,scale = 1\n\n6. Implement the rescaling of logits using equation 3, where sz is treated with sz,init = 1 and sz,scale = 1/√dmodel.\n\n7. Remove weight decay and learning rate warmup.",
        "bBox": {
          "x": 108,
          "y": 103,
          "w": 395.9,
          "h": 17.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "3 EXPERIMENTS",
        "md": "## 3 EXPERIMENTS",
        "bBox": {
          "x": 108,
          "y": 213,
          "w": 72.84,
          "h": 11.96
        }
      },
      {
        "type": "text",
        "value": "We train both the baseline Transformer (GPT) and the normalized Transformer (nGPT) on the OpenWebText dataset (Gokaslan & Cohen, 2019) and evaluate them on a set of standard downstream tasks. We experiment with models containing 0.5B and 1B parameters, including the embeddings. For both GPT and nGPT, we report results using the best initial learning rate settings (see Appendix A.6). A detailed description of the setup and hyperparameters is in Appendix A.5.",
        "md": "We train both the baseline Transformer (GPT) and the normalized Transformer (nGPT) on the OpenWebText dataset (Gokaslan & Cohen, 2019) and evaluate them on a set of standard downstream tasks. We experiment with models containing 0.5B and 1B parameters, including the embeddings. For both GPT and nGPT, we report results using the best initial learning rate settings (see Appendix A.6). A detailed description of the setup and hyperparameters is in Appendix A.5.",
        "bBox": {
          "x": 107,
          "y": 36,
          "w": 396.33,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 3,
        "value": "3.1 ACCELERATION OF TRAINING",
        "md": "### 3.1 ACCELERATION OF TRAINING",
        "bBox": {
          "x": 108,
          "y": 213,
          "w": 124.95,
          "h": 11.96
        }
      },
      {
        "type": "text",
        "value": "Figure 1: Validation loss during training of 1B GPT and nGPT with 4k context length.",
        "md": "Figure 1: Validation loss during training of 1B GPT and nGPT with 4k context length.",
        "bBox": {
          "x": null,
          "y": 113,
          "w": 344.13,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "4k context: OpenWebText"
          ],
          [
            "GPT with 200k budget"
          ],
          [
            "nGPT with 20k budget"
          ],
          [
            "nGPT with 200k budget"
          ]
        ],
        "md": "| 4k context: OpenWebText |\n|--------------------------|\n| GPT with 200k budget     |\n| nGPT with 20k budget     |\n| nGPT with 200k budget    |",
        "isPerfectTable": true,
        "csv": "\"4k context: OpenWebText\"\n\"GPT with 200k budget\"\n\"nGPT with 20k budget\"\n\"nGPT with 200k budget\"",
        "bBox": {
          "x": 120,
          "y": 327,
          "w": 222.71,
          "h": 6.3
        }
      },
      {
        "type": "text",
        "value": "Figure 1 presents the validation loss during the training of GPT and nGPT models with 1 billion parameters and a sample length of 4k tokens. After 20k iterations, nGPT achieves the same validation loss that GPT reaches only after 200k iterations (approximately 400 billion tokens), demonstrating a 10x speedup in terms of iterations and tokens used.³\n\nFigure 2: Final validation loss (y-axis) for training runs with different computation budgets in tokens (x-axis). The training of 0.5B and 1B nGPT models is about 4x, 10x and 20x faster (in terms of tokens) on 1k, 4k and 8k context lengths, respectively.",
        "md": "Figure 1 presents the validation loss during the training of GPT and nGPT models with 1 billion parameters and a sample length of 4k tokens. After 20k iterations, nGPT achieves the same validation loss that GPT reaches only after 200k iterations (approximately 400 billion tokens), demonstrating a 10x speedup in terms of iterations and tokens used.³\n\nFigure 2: Final validation loss (y-axis) for training runs with different computation budgets in tokens (x-axis). The training of 0.5B and 1B nGPT models is about 4x, 10x and 20x faster (in terms of tokens) on 1k, 4k and 8k context lengths, respectively.",
        "bBox": {
          "x": null,
          "y": 113,
          "w": 396.3,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "1K context: OpenWebText",
            "4K context: OpenWebText",
            "8K context: OpenWebText"
          ],
          [
            "GPT 0.5B",
            "GPT 0.5B",
            "GPT 0.5B"
          ],
          [
            "GPT 1B",
            "GPT 1B",
            "GPT 1B"
          ],
          [
            "nGPT 0.5B",
            "nGPT 0.5B",
            "nGPT 0.5B"
          ],
          [
            "nGPT 1B",
            "nGPT 1B",
            "nGPT 1B"
          ]
        ],
        "md": "| 1K context: OpenWebText | 4K context: OpenWebText | 8K context: OpenWebText |\n|-------------------------|-------------------------|-------------------------|\n| GPT 0.5B                | GPT 0.5B                | GPT 0.5B                |\n| GPT 1B                  | GPT 1B                  | GPT 1B                  |\n| nGPT 0.5B               | nGPT 0.5B               | nGPT 0.5B               |\n| nGPT 1B                 | nGPT 1B                 | nGPT 1B                 |",
        "isPerfectTable": true,
        "csv": "\"1K context: OpenWebText\",\"4K context: OpenWebText\",\"8K context: OpenWebText\"\n\"GPT 0.5B\",\"GPT 0.5B\",\"GPT 0.5B\"\n\"GPT 1B\",\"GPT 1B\",\"GPT 1B\"\n\"nGPT 0.5B\",\"nGPT 0.5B\",\"nGPT 0.5B\"\n\"nGPT 1B\",\"nGPT 1B\",\"nGPT 1B\"",
        "bBox": {
          "x": 147,
          "y": 327,
          "w": 65.71,
          "h": 205.28
        }
      },
      {
        "type": "text",
        "value": "³While the time per step of nGPT is higher (80% - for 4k, and 60% - for 8k context respectively), it can be reduced after code optimization. Also the overhead is less significant for larger networks (see Appendix A.4).",
        "md": "³While the time per step of nGPT is higher (80% - for 4k, and 60% - for 8k context respectively), it can be reduced after code optimization. Also the overhead is less significant for larger networks (see Appendix A.4).",
        "bBox": {
          "x": 107,
          "y": 424,
          "w": 392.96,
          "h": 9.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": ""
      },
      {
        "text": "16, where"
      },
      {
        "text": ""
      },
      {
        "text": "21, where"
      },
      {
        "text": ""
      },
      {
        "text": "tasks. We experiment with models containing 0.5B and 1B parameters, including the embeddings."
      },
      {
        "text": "tasks. We experiment with models containing 0.5B and 1B parameters, including the embeddings."
      },
      {
        "text": "A.6). A detailed description of the setup and hyperparameters is in Appendix A.5."
      },
      {
        "text": ""
      },
      {
        "text": "rameters and a sample length of 4k tokens. After 20k iterations, nGPT achieves the same validation"
      },
      {
        "text": "3"
      },
      {
        "text": ""
      }
    ]
  },
  {
    "page": 7,
    "text": "                   Normalized Transformer\n\n\n\n                      0.6         4K context: arc easy               0.6        4K context: hellaswag             0.62     4K context: winogrande\nValue0.6Value0.55\n\n\n\n                     0.55                                                                                         0.58\n\n\n\n                                                                     0.5\n\n\n\n                                                                                                                  0.56\n\n\n\n                                                                    0.45\n\n\n\n                      0.5                                                                                         0.54\n\n\n\n                                                     GPT 0.5B        0.4                           GPT 0.5B                                     GPT 0.5B\n                                                     GPT 1B                                        GPT 1B         0.52                          GPT 1B\n                                                     nGPT 0.5B                                     nGPT 0.5B                                    nGPT 0.5B\n                                                     nGPT 1B                                       nGPT 1B                                      nGPT 1B\n                     0.45                                           0.35                                           0.5\n                        0      100     200     300     400     500    0       100     200     300     400     500    0    100     200     300     400     500\n                                 Training tokens in billions                   Training tokens in billions                  Training tokens in billions\n                      0.8         4K context: wsc273                0.584K context: lambada openai seqlen1024     0.64       4K context: average\nValueValue0.56                                                                                                    0.62\n\n\n\n                     0.75                                           0.54                                           0.6\n\n\n\n                                                                    0.52\n                                                                                                                  0.58\n\n\n\n                      0.7                                            0.5\n                                                                                                                  0.56\n                                                                    0.48\n                                                                                                                  0.54\n                     0.65                                           0.46\n\n\n\n                                                                                                                  0.52\n                                                                    0.44\n\n\n\n                      0.6                            GPT 0.5B       0.42                           GPT 0.5B        0.5                          GPT 0.5B\n                                                     GPT 1B                                        GPT 1B                                       GPT 1B\n                                                     nGPT 0.5B       0.4                           nGPT 0.5B      0.48                          nGPT 0.5B\n                                                     nGPT 1B                                       nGPT 1B                                      nGPT 1B\n                     0.55                                           0.38                                          0.46\n                        0      100     200     300     400     500    0       100     200     300     400     500    0    100     200     300     400     500\n                                 Training tokens in billions                   Training tokens in billions                  Training tokens in billionsValueValue\n\n\n\n                   Figure 3: Models trained with 4k context length. Final performance (y-axis) on a set of downstream\n                   tasks and their average value (Bottom-Right) for different computation budgets in tokens (x-axis).\n\n\n\n                   Figure 2 illustrates how the performance gap between nGPT and GPT scales across three axes:\n                   total token budget, context length, and network size. Training the 0.5B and 1B nGPT models is\n                   approximately 4x, 10x, and 20x faster at context lengths of 1k, 4k, and 8k tokens, respectively.\n                   Figure 3 shows a similar pattern across downstream tasks, confirming that the acceleration is not\n                   only reflected in perplexity but also in task performance. Figures 8 and 9 in the Appendix provide\n                   results for 1k and 8k context lengths. We observe some saturation for the longest runs of nGPT,\n                   suggesting that the model capacity is nearly reached for this number of trainable model parameters.\n\n\n\n                   3.2     INSPECTION OF NETWORK PARAMETERS\n\n\n\n                   Figure 4 shows that, while nGPT maintains a fixed norm for embeddings (by design), GPT ex-\n                   hibits significant variation. The distribution of eigenvalues, computed from the covariance matrix\n                   of embeddings and normalized by their median, reveals that GPT’s input embeddings have a higher\n                   condition number, especially in the 1B model. The distribution of pairwise dot products between\n                   embeddings indicates that even in nGPT, embeddings are not uniformly distributed across the hy-\n                   persphere (where the dot product would approach 0), but instead form clusters—possibly reflecting\n                   natural patterns in language data. Dot products in GPT tend to have higher values due to its em-\n                   beddings forming a hyper-ellipsoid, as suggested by the spread of vector norms. The ill-conditioned\n                   nature of GPT’s input embeddings could lead to computational issues involving these embeddings.\n                   Figure 5 shows the median condition numbers (across heads) for attention and MLP matrices at\n                   different layer depths—24 layers for the 0.5B model and 36 layers for the 1B model. GPT mod-\n                   els exhibit significantly higher condition numbers in their attention matrices compared to nGPT. A\n                   closer inspection of these matrices (the 3rd and 4th layers are in Figure 11 and Figure 12 of Ap-\n                   pendix) suggests that they degenerate into lower-rank matrices, potentially reducing the learning\n                   capacity of these blocks. One could argue that the elevated condition numbers are influenced by the\n                   norms of the vectors in these matrices. Our post-training normalization of these matrices is depicted\n\n\n\n                                                                                        7",
    "md": "# Normalized Transformer\n\n| 4K context: arc easy | 4K context: hellaswag | 4K context: winogrande |\n|:---:|:---:|:---:|\n| Graph 1 | Graph 2 | Graph 3 |\n\n| 4K context: wsc273 | 4K context: lambada openai seqlen1024 | 4K context: average |\n|:---:|:---:|:---:|\n| Graph 4 | Graph 5 | Graph 6 |\n\nFigure 3: Models trained with 4k context length. Final performance (y-axis) on a set of downstream tasks and their average value (Bottom-Right) for different computation budgets in tokens (x-axis).\n\nFigure 2 illustrates how the performance gap between nGPT and GPT scales across three axes: total token budget, context length, and network size. Training the 0.5B and 1B nGPT models is approximately 4x, 10x, and 20x faster at context lengths of 1k, 4k, and 8k tokens, respectively.\n\nFigure 3 shows a similar pattern across downstream tasks, confirming that the acceleration is not only reflected in perplexity but also in task performance. Figures 8 and 9 in the Appendix provide results for 1k and 8k context lengths. We observe some saturation for the longest runs of nGPT, suggesting that the model capacity is nearly reached for this number of trainable model parameters.\n\n## 3.2 INSPECTION OF NETWORK PARAMETERS\n\nFigure 4 shows that, while nGPT maintains a fixed norm for embeddings (by design), GPT exhibits significant variation. The distribution of eigenvalues, computed from the covariance matrix of embeddings and normalized by their median, reveals that GPT's input embeddings have a higher condition number, especially in the 1B model. The distribution of pairwise dot products between embeddings indicates that even in nGPT, embeddings are not uniformly distributed across the hypersphere (where the dot product would approach 0), but instead form clusters—possibly reflecting natural patterns in language data. Dot products in GPT tend to have higher values due to its embeddings forming a hyper-ellipsoid, as suggested by the spread of vector norms. The ill-conditioned nature of GPT's input embeddings could lead to computational issues involving these embeddings.\n\nFigure 5 shows the median condition numbers (across heads) for attention and MLP matrices at different layer depths—24 layers for the 0.5B model and 36 layers for the 1B model. GPT models exhibit significantly higher condition numbers in their attention matrices compared to nGPT. A closer inspection of these matrices (the 3rd and 4th layers are in Figure 11 and Figure 12 of Appendix) suggests that they degenerate into lower-rank matrices, potentially reducing the learning capacity of these blocks. One could argue that the elevated condition numbers are influenced by the norms of the vectors in these matrices. Our post-training normalization of these matrices is depicted",
    "images": [
      {
        "name": "page_7.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "4K context: arc easy",
            "4K context: hellaswag",
            "4K context: winogrande"
          ],
          [
            "Graph 1",
            "Graph 2",
            "Graph 3"
          ]
        ],
        "md": "| 4K context: arc easy | 4K context: hellaswag | 4K context: winogrande |\n|:---:|:---:|:---:|\n| Graph 1 | Graph 2 | Graph 3 |",
        "isPerfectTable": true,
        "csv": "\"4K context: arc easy\",\"4K context: hellaswag\",\"4K context: winogrande\"\n\"Graph 1\",\"Graph 2\",\"Graph 3\"",
        "bBox": {
          "x": 151,
          "y": 89,
          "w": 66.89,
          "h": 5.76
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "4K context: wsc273",
            "4K context: lambada openai seqlen1024",
            "4K context: average"
          ],
          [
            "Graph 4",
            "Graph 5",
            "Graph 6"
          ]
        ],
        "md": "| 4K context: wsc273 | 4K context: lambada openai seqlen1024 | 4K context: average |\n|:---:|:---:|:---:|\n| Graph 4 | Graph 5 | Graph 6 |",
        "isPerfectTable": true,
        "csv": "\"4K context: wsc273\",\"4K context: lambada openai seqlen1024\",\"4K context: average\"\n\"Graph 4\",\"Graph 5\",\"Graph 6\"",
        "bBox": {
          "x": 122,
          "y": 217,
          "w": 110.76,
          "h": 25.96
        }
      },
      {
        "type": "text",
        "value": "Figure 3: Models trained with 4k context length. Final performance (y-axis) on a set of downstream tasks and their average value (Bottom-Right) for different computation budgets in tokens (x-axis).\n\nFigure 2 illustrates how the performance gap between nGPT and GPT scales across three axes: total token budget, context length, and network size. Training the 0.5B and 1B nGPT models is approximately 4x, 10x, and 20x faster at context lengths of 1k, 4k, and 8k tokens, respectively.\n\nFigure 3 shows a similar pattern across downstream tasks, confirming that the acceleration is not only reflected in perplexity but also in task performance. Figures 8 and 9 in the Appendix provide results for 1k and 8k context lengths. We observe some saturation for the longest runs of nGPT, suggesting that the model capacity is nearly reached for this number of trainable model parameters.",
        "md": "Figure 3: Models trained with 4k context length. Final performance (y-axis) on a set of downstream tasks and their average value (Bottom-Right) for different computation budgets in tokens (x-axis).\n\nFigure 2 illustrates how the performance gap between nGPT and GPT scales across three axes: total token budget, context length, and network size. Training the 0.5B and 1B nGPT models is approximately 4x, 10x, and 20x faster at context lengths of 1k, 4k, and 8k tokens, respectively.\n\nFigure 3 shows a similar pattern across downstream tasks, confirming that the acceleration is not only reflected in perplexity but also in task performance. Figures 8 and 9 in the Appendix provide results for 1k and 8k context lengths. We observe some saturation for the longest runs of nGPT, suggesting that the model capacity is nearly reached for this number of trainable model parameters.",
        "bBox": {
          "x": null,
          "y": 112,
          "w": 396.34,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "3.2 INSPECTION OF NETWORK PARAMETERS",
        "md": "## 3.2 INSPECTION OF NETWORK PARAMETERS",
        "bBox": {
          "x": 108,
          "y": 538,
          "w": 169.55,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "Figure 4 shows that, while nGPT maintains a fixed norm for embeddings (by design), GPT exhibits significant variation. The distribution of eigenvalues, computed from the covariance matrix of embeddings and normalized by their median, reveals that GPT's input embeddings have a higher condition number, especially in the 1B model. The distribution of pairwise dot products between embeddings indicates that even in nGPT, embeddings are not uniformly distributed across the hypersphere (where the dot product would approach 0), but instead form clusters—possibly reflecting natural patterns in language data. Dot products in GPT tend to have higher values due to its embeddings forming a hyper-ellipsoid, as suggested by the spread of vector norms. The ill-conditioned nature of GPT's input embeddings could lead to computational issues involving these embeddings.\n\nFigure 5 shows the median condition numbers (across heads) for attention and MLP matrices at different layer depths—24 layers for the 0.5B model and 36 layers for the 1B model. GPT models exhibit significantly higher condition numbers in their attention matrices compared to nGPT. A closer inspection of these matrices (the 3rd and 4th layers are in Figure 11 and Figure 12 of Appendix) suggests that they degenerate into lower-rank matrices, potentially reducing the learning capacity of these blocks. One could argue that the elevated condition numbers are influenced by the norms of the vectors in these matrices. Our post-training normalization of these matrices is depicted",
        "md": "Figure 4 shows that, while nGPT maintains a fixed norm for embeddings (by design), GPT exhibits significant variation. The distribution of eigenvalues, computed from the covariance matrix of embeddings and normalized by their median, reveals that GPT's input embeddings have a higher condition number, especially in the 1B model. The distribution of pairwise dot products between embeddings indicates that even in nGPT, embeddings are not uniformly distributed across the hypersphere (where the dot product would approach 0), but instead form clusters—possibly reflecting natural patterns in language data. Dot products in GPT tend to have higher values due to its embeddings forming a hyper-ellipsoid, as suggested by the spread of vector norms. The ill-conditioned nature of GPT's input embeddings could lead to computational issues involving these embeddings.\n\nFigure 5 shows the median condition numbers (across heads) for attention and MLP matrices at different layer depths—24 layers for the 0.5B model and 36 layers for the 1B model. GPT models exhibit significantly higher condition numbers in their attention matrices compared to nGPT. A closer inspection of these matrices (the 3rd and 4th layers are in Figure 11 and Figure 12 of Appendix) suggests that they degenerate into lower-rank matrices, potentially reducing the learning capacity of these blocks. One could argue that the elevated condition numbers are influenced by the norms of the vectors in these matrices. Our post-training normalization of these matrices is depicted",
        "bBox": {
          "x": null,
          "y": 112,
          "w": 396.3,
          "h": 9.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": "total token budget, context length, and network size. Training the 0.5B and 1B nGPT models is"
      },
      {
        "text": "only reflected in perplexity but also in task performance. Figures 8 and 9 in the Appendix provide"
      },
      {
        "text": "results for 1k and 8k context lengths. We observe some saturation for the longest runs of nGPT,"
      },
      {
        "text": "results for 1k and 8k context lengths. We observe some saturation for the longest runs of nGPT,"
      },
      {
        "text": "hibits significant variation. The distribution of eigenvalues, computed from the covariance matrix"
      },
      {
        "text": "different layer depths—24 layers for the 0.5B model and 36 layers for the 1B model. GPT mod-"
      },
      {
        "text": ""
      },
      {
        "text": ""
      }
    ]
  },
  {
    "page": 8,
    "text": "                                                   Normalized Transformer\n\n\n\n                                                                     2.5Distribution of norms of input embeddings         101Sorted Eigenvalues of input embeddings            1Pairwise dot-products of input embeddings\n                                                                                                          GPT 0.5B                                             GPT 0.5B                                           GPT 0.5B\n                                                                                                          GPT 1B                                               GPT 1B        0.9                                  GPT 1B\n                                                                                                          nGPT 0.5B                                            nGPT 0.5B                                          nGPT 0.5B\n                                                                      2                                   nGPT 1B                                              nGPT 1B       0.8                                  nGPT 1B\n\n\n\n                                                                                                                                                                             0.7\n\n\n\n                                                                     1.5                                                                                                     0.6\n\n\n\n                                                                                                                          100                                                0.5Condition numberCondition numberNorm valueNorm value\n                                                                      1                                                                                                      0.4\n\n\n\n                                                                                                                                                                             0.3\n\n\n\n                                                                     0.5                                                                                                     0.2\n\n\n\n                                                                                                                                                                             0.1\n\n\n\n                                                                      00        0.2      0.4      0.6      0.8       1   10-10       0.2      0.4      0.6      0.8       1    00       0.2      0.4      0.6      0.8        1\n                                                                                      Normalized rank                                      Normalized rank                                    Normalized rank\n                                                                     2.8Distribution of norms of output embeddings        101Sorted Eigenvalues of output embeddings          Pairwise dot-products of output embeddings\n                                                                                                                                                                               1\n                                                                                                          GPT 0.5B                                             GPT 0.5B                                           GPT 0.5B\n                                                                     2.6                                  GPT 1B                                               GPT 1B        0.9                                  GPT 1B\n                                                                                                          nGPT 0.5B                                            nGPT 0.5B                                          nGPT 0.5B\n                                                                     2.4                                  nGPT 1B                                              nGPT 1B       0.8                                  nGPT 1B\n\n\n\n                                                                     2.2                                                                                                     0.7\nEigenvalue / median(Eigenvalues)Eigenvalue / median(Eigenvalues)Condition numberCondition number2                                                                            0.6\n                                                                     1.8                                                  100                                                0.5\n\n\n\n                                                                     1.6                                                                                                     0.4\n\n\n\n                                                                     1.4                                                                                                     0.3\n\n\n\n                                                                     1.2                                                                                                     0.2\n\n\n\n                                                                      1                                                                                                      0.1\n\n\n\n                                                                     0.80       0.2      0.4      0.6      0.8       1   10-10       0.2      0.4      0.6      0.8       1    00       0.2      0.4      0.6      0.8        1\n                                                                                      Normalized rank                                      Normalized rank                                    Normalized rank\n\n\n\n                                                   Figure 4:            Left: Distribution of norms of vectors from input (Top line) and output (Bottom line)\n                                                   embedding matrices.                         Middle: Distribution of eigenvalues divided by its median value.                                                                   Right:Condition numberCondition numberValueValuePairwise distribution of dot products between embeddings. Models are trained for 100k iterations.\n\n\n\n                                                                    107   Condition number of Q at each layer             107   Condition number of K at each layer          107   Condition number of V at each layer\n                                                                                                          GPT 0.5B                                             GPT 0.5B                                           GPT 0.5B\n                                                                    106                                   GPT 1B          106                                  GPT 1B        106                                  GPT 1B\n                                                                                                          nGPT 0.5B                                            nGPT 0.5B                                          nGPT 0.5B\n                                                                                                          nGPT 1B                                              nGPT 1B                                            nGPT 1B\n                                                                    105                                                   105                                                105\n\n\n\n                                                                    104                                                   104                                                104\n\n\n\n                                                                    103                                                   103                                                103\n\n\n\n                                                                    102                                                   102                                                102\n\n\n\n                                                                    101                                                   101                                                101\n\n\n\n                                                                    1000        0.2      0.4      0.6      0.8       1    1000       0.2      0.4      0.6      0.8       1  1000       0.2      0.4      0.6      0.8        1\n                                                                                   Normalized layer index                               Normalized layer index                             Normalized layer index\n                                                                    107Condition number of MLP11 at each layer            107Condition number of MLP12 at each layer         107 Condition number of MLP2 at each layer\n                                                                                                          GPT 0.5B                                             GPT 0.5B                                           GPT 0.5B\n                                                                    106                                   GPT 1B          106                                  GPT 1B        106                                  GPT 1B\n                                                                                                          nGPT 0.5B                                            nGPT 0.5B                                          nGPT 0.5B\n                                                                                                          nGPT 1B                                              nGPT 1B                                            nGPT 1B\n                                                                    105                                                   105                                                105\n\n\n\n                                                                    104                                                   104                                                104\n\n\n\n                                                                    103                                                   103                                                103\n\n\n\n                                                                    102                                                   102                                                102\n\n\n\n                                                                    101                                                   101                                                101\n\n\n\n                                                                    1000        0.2      0.4      0.6      0.8       1    1000       0.2      0.4      0.6      0.8       1  1000       0.2      0.4      0.6      0.8        1\n                                                                                   Normalized layer index                               Normalized layer index                             Normalized layer index\n\n\n\n                                                   Figure 5:            Median condition numbers for attention and MLP matrices at different layer depth (24\n                                                   and 36 layers for 0.5B and 1B models, respectively). Models are trained for 100k iterations.\n\n\n\n                                                   by the dotted lines in Figure 10 of the Appendix. While the adjusted condition numbers are reduced,\n                                                   they remain higher than those for nGPT, indicating potential rank deficiency. The need for such\n                                                   normalization highlights one of the issues that nGPT is specifically designed to address.\n\n\n\n                                                                                                                                                 8",
    "md": "# Normalized Transformer\n\n## Figure 4: Left: Distribution of norms of vectors from input (Top line) and output (Bottom line) embedding matrices. Middle: Distribution of eigenvalues divided by its median value. Right: Pairwise distribution of dot products between embeddings. Models are trained for 100k iterations.\n\n| Distribution of norms of input embeddings | Sorted Eigenvalues of input embeddings | Pairwise dot-products of input embeddings |\n|-------------------------------------------|----------------------------------------|-------------------------------------------|\n| [Graph showing distribution of norms for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B] | [Graph showing sorted eigenvalues for the same models] | [Graph showing pairwise dot-products for the same models] |\n\n| Distribution of norms of output embeddings | Sorted Eigenvalues of output embeddings | Pairwise dot-products of output embeddings |\n|---------------------------------------------|------------------------------------------|---------------------------------------------|\n| [Graph showing distribution of norms for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B] | [Graph showing sorted eigenvalues for the same models] | [Graph showing pairwise dot-products for the same models] |\n\n## Figure 5: Median condition numbers for attention and MLP matrices at different layer depth (24 and 36 layers for 0.5B and 1B models, respectively). Models are trained for 100k iterations.\n\n| Condition number of Q at each layer | Condition number of K at each layer | Condition number of V at each layer |\n|-------------------------------------|-------------------------------------|-------------------------------------|\n| [Graph showing condition numbers for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B] | [Graph showing condition numbers for the same models] | [Graph showing condition numbers for the same models] |\n\n| Condition number of MLP11 at each layer | Condition number of MLP12 at each layer | Condition number of MLP2 at each layer |\n|------------------------------------------|------------------------------------------|---------------------------------------|\n| [Graph showing condition numbers for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B] | [Graph showing condition numbers for the same models] | [Graph showing condition numbers for the same models] |\n\nby the dotted lines in Figure 10 of the Appendix. While the adjusted condition numbers are reduced, they remain higher than those for nGPT, indicating potential rank deficiency. The need for such normalization highlights one of the issues that nGPT is specifically designed to address.\n\n8",
    "images": [
      {
        "name": "page_8.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "Figure 4: Left: Distribution of norms of vectors from input (Top line) and output (Bottom line) embedding matrices. Middle: Distribution of eigenvalues divided by its median value. Right: Pairwise distribution of dot products between embeddings. Models are trained for 100k iterations.",
        "md": "## Figure 4: Left: Distribution of norms of vectors from input (Top line) and output (Bottom line) embedding matrices. Middle: Distribution of eigenvalues divided by its median value. Right: Pairwise distribution of dot products between embeddings. Models are trained for 100k iterations.",
        "bBox": {
          "x": 107,
          "y": 141,
          "w": 390.57,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Distribution of norms of input embeddings",
            "Sorted Eigenvalues of input embeddings",
            "Pairwise dot-products of input embeddings"
          ],
          [
            "[Graph showing distribution of norms for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B]",
            "[Graph showing sorted eigenvalues for the same models]",
            "[Graph showing pairwise dot-products for the same models]"
          ]
        ],
        "md": "| Distribution of norms of input embeddings | Sorted Eigenvalues of input embeddings | Pairwise dot-products of input embeddings |\n|-------------------------------------------|----------------------------------------|-------------------------------------------|\n| [Graph showing distribution of norms for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B] | [Graph showing sorted eigenvalues for the same models] | [Graph showing pairwise dot-products for the same models] |",
        "isPerfectTable": true,
        "csv": "\"Distribution of norms of input embeddings\",\"Sorted Eigenvalues of input embeddings\",\"Pairwise dot-products of input embeddings\"\n\"[Graph showing distribution of norms for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B]\",\"[Graph showing sorted eigenvalues for the same models]\",\"[Graph showing pairwise dot-products for the same models]\"",
        "bBox": {
          "x": null,
          "y": 97,
          "w": 21.19,
          "h": 272.9
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Distribution of norms of output embeddings",
            "Sorted Eigenvalues of output embeddings",
            "Pairwise dot-products of output embeddings"
          ],
          [
            "[Graph showing distribution of norms for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B]",
            "[Graph showing sorted eigenvalues for the same models]",
            "[Graph showing pairwise dot-products for the same models]"
          ]
        ],
        "md": "| Distribution of norms of output embeddings | Sorted Eigenvalues of output embeddings | Pairwise dot-products of output embeddings |\n|---------------------------------------------|------------------------------------------|---------------------------------------------|\n| [Graph showing distribution of norms for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B] | [Graph showing sorted eigenvalues for the same models] | [Graph showing pairwise dot-products for the same models] |",
        "isPerfectTable": true,
        "csv": "\"Distribution of norms of output embeddings\",\"Sorted Eigenvalues of output embeddings\",\"Pairwise dot-products of output embeddings\"\n\"[Graph showing distribution of norms for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B]\",\"[Graph showing sorted eigenvalues for the same models]\",\"[Graph showing pairwise dot-products for the same models]\"",
        "bBox": {
          "x": 145,
          "y": 97,
          "w": 106.19,
          "h": 117.9
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "Figure 5: Median condition numbers for attention and MLP matrices at different layer depth (24 and 36 layers for 0.5B and 1B models, respectively). Models are trained for 100k iterations.",
        "md": "## Figure 5: Median condition numbers for attention and MLP matrices at different layer depth (24 and 36 layers for 0.5B and 1B models, respectively). Models are trained for 100k iterations.",
        "bBox": {
          "x": 108,
          "y": 111,
          "w": 366.09,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Condition number of Q at each layer",
            "Condition number of K at each layer",
            "Condition number of V at each layer"
          ],
          [
            "[Graph showing condition numbers for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B]",
            "[Graph showing condition numbers for the same models]",
            "[Graph showing condition numbers for the same models]"
          ]
        ],
        "md": "| Condition number of Q at each layer | Condition number of K at each layer | Condition number of V at each layer |\n|-------------------------------------|-------------------------------------|-------------------------------------|\n| [Graph showing condition numbers for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B] | [Graph showing condition numbers for the same models] | [Graph showing condition numbers for the same models] |",
        "isPerfectTable": true,
        "csv": "\"Condition number of Q at each layer\",\"Condition number of K at each layer\",\"Condition number of V at each layer\"\n\"[Graph showing condition numbers for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B]\",\"[Graph showing condition numbers for the same models]\",\"[Graph showing condition numbers for the same models]\"",
        "bBox": {
          "x": null,
          "y": 97,
          "w": 84.34,
          "h": 49.9
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Condition number of MLP11 at each layer",
            "Condition number of MLP12 at each layer",
            "Condition number of MLP2 at each layer"
          ],
          [
            "[Graph showing condition numbers for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B]",
            "[Graph showing condition numbers for the same models]",
            "[Graph showing condition numbers for the same models]"
          ]
        ],
        "md": "| Condition number of MLP11 at each layer | Condition number of MLP12 at each layer | Condition number of MLP2 at each layer |\n|------------------------------------------|------------------------------------------|---------------------------------------|\n| [Graph showing condition numbers for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B] | [Graph showing condition numbers for the same models] | [Graph showing condition numbers for the same models] |",
        "isPerfectTable": true,
        "csv": "\"Condition number of MLP11 at each layer\",\"Condition number of MLP12 at each layer\",\"Condition number of MLP2 at each layer\"\n\"[Graph showing condition numbers for GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B]\",\"[Graph showing condition numbers for the same models]\",\"[Graph showing condition numbers for the same models]\"",
        "bBox": {
          "x": null,
          "y": 97,
          "w": 93.59,
          "h": 49.9
        }
      },
      {
        "type": "text",
        "value": "by the dotted lines in Figure 10 of the Appendix. While the adjusted condition numbers are reduced, they remain higher than those for nGPT, indicating potential rank deficiency. The need for such normalization highlights one of the issues that nGPT is specifically designed to address.\n\n8",
        "md": "by the dotted lines in Figure 10 of the Appendix. While the adjusted condition numbers are reduced, they remain higher than those for nGPT, indicating potential rank deficiency. The need for such normalization highlights one of the issues that nGPT is specifically designed to address.\n\n8",
        "bBox": {
          "x": null,
          "y": 142,
          "w": 396.21,
          "h": 9.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": "they remain higher than those for nGPT, indicating potential rank deficiency. The need for such"
      }
    ]
  },
  {
    "page": 9,
    "text": " Normalized Transformer\n\n\n\n               1    Eigen learning rates of Attention  3    Scaling su for matrix Wu of MLP     3             Scaling sqk\n                                          nGPT 0.5B                               nGPT 0.5B                                nGPT 0.5B\n              0.9                         nGPT 1B                                 nGPT 1B                                  nGPT 1B\n                                                      2.5                                      2.5\n              0.8\n\n\n\n              0.7\n                                                       2                                        2\n              0.6\n              0.5                                     1.5                                      1.5mean(abs(Value))mean(abs(Value))\n              0.4\n\n\n\n                                                       1                                        1\n              0.3\n\n\n\n              0.2\n                                                      0.5                                      0.5\n\n\n\n              0.1\n\n\n\n               00     0.2    0.4    0.6    0.8     1   00      0.2    0.4    0.6   0.8     1    00     0.2    0.4    0.6    0.8    1\n                         Normalized layer index                  Normalized layer index                  Normalized layer index\n                      Eigen learning rates of MLP           Scaling s for matrix W of MLP      Distribution of scaling sz for 32k embeddings\n               1                                       3                                      0.04\n                                          nGPT 0.5B                               nGPT 0.5B                                nGPT 0.5B\n              0.9                         nGPT 1B                                 nGPT 1B     0.035                        nGPT 1B\n                                                      2.5\n              0.8\n                                                                                              0.03\n              0.7                                      2\n                                                                                              0.025mean(abs(Value))mean(abs(Value))0.6\n\n\n\n              0.5                                     1.5                                     0.02\n\n\n\n              0.4                                                                             0.015\n                                                       1\n              0.3\n                                                                                              0.01\n              0.2                                     0.5\n                                                                                              0.005\n              0.1\n\n\n\n               00     0.2    0.4    0.6    0.8     1   00      0.2    0.4    0.6   0.8     1    0  20   40    60   80   100  120   140\n                         Normalized layer index                  Normalized layer index                         Value\n\n\n\n Figure 6: (Left): Eigen learning rates of Attention and MLP blocks. (Middle): Scaling factors ap-\nplied to the intermediate states of MLP. (Right): Scaling factors applied before the QK dot product;Estimated Probability Densitymean(abs(Value))distribution of per-vector scalings applied to logits. Models are trained for 100k iterations.\n\n\n\nAn important contribution of this work is the decoupling of predictions made by the Attention and\nMLP blocks from their impact on the hidden state h. These contributions are controlled by the\neigen learning rates αA and αM. Their interpretation is straightforward: if αA,i for an embedding\n dimension i ∈ Rdmodel is 0.2, then the update follows hi ← (1 − 0.2)hi + 0.2hA,i. Thus, they directly\n quantify the contribution of hA,i into h. Figure 6 shows the average absolute values of hA and hM\n at each layer. Notably, the network learns to take only modest steps (20%-30%) in the direction\n suggested by hA and hM. The average magnitude of αA decreases from 0.25 in the 0.5B network\n (24 layers) to 0.20 in the 1B network (36 layers). Meanwhile, αM decreases from 0.37 to 0.32,\n possibly because MLP blocks have more parameters, making their suggestions more precise.\n The scaling factors su, sν and sqk remain relatively stable across layers. The value of sν can be\n interpreted as a measure of the non-linearity of the SiLU function, which behaves like ReLU for\n large sν and approximates a linear unit for values near 0 (see also Appendix A.1). The distribution\n of sz is primarily characterized by its mean, which influences the temperature of the softmax during\n cross-entropy calculations. The introduced scaling factors sqk , su, sν and sz seem to compensate\n for the removal of magnitude information when normalizing matrices and embeddings.\n\n\n\n 3.3     ABLATION STUDIES\n\n\n\n Appendix A.8 summarizes numerous ablation experiments. An important finding is that having fixed\n (non-learnable) values for sqk, su, sν and a single global learnable value for sz leads to only a slight\n degradation in accuracy. Therefore, our presented general case can be simplified and become easier\n to interpret. Appendix A.7 demonstrates that nGPT can handle longer contexts without requiring\n any modifications to RoPE.\n\n\n\n 4     RELATED WORK\n Wang & Isola (2020) provides a comprehensive overview of the arguments for representation learn-\n ing on the hypersphere. Spherical representations are associated with more stable training in the\n latent space of variational autoencoders (Xu & Durrett, 2018) and in embeddings used for face ver-\n\n\n\n                                                                       9",
    "md": "# Normalized Transformer\n\nFigure 6: (Left): Eigen learning rates of Attention and MLP blocks. (Middle): Scaling factors applied to the intermediate states of MLP. (Right): Scaling factors applied before the QK dot product; distribution of per-vector scalings applied to logits. Models are trained for 100k iterations.\n\n| Eigen learning rates of Attention | Scaling su for matrix Wu of MLP | Scaling sqk |\n|-----------------------------------|--------------------------------|-------------|\n| nGPT 0.5B                         | nGPT 0.5B                      | nGPT 0.5B   |\n| nGPT 1B                           | nGPT 1B                        | nGPT 1B     |\n\n| Eigen learning rates of MLP | Scaling sv for matrix Wv of MLP | Distribution of scaling sz for 32k embeddings |\n|-----------------------------|--------------------------------|-----------------------------------------------|\n| nGPT 0.5B                   | nGPT 0.5B                      | nGPT 0.5B                                     |\n| nGPT 1B                     | nGPT 1B                        | nGPT 1B                                       |\n\nAn important contribution of this work is the decoupling of predictions made by the Attention and MLP blocks from their impact on the hidden state h. These contributions are controlled by the eigen learning rates αA and αM. Their interpretation is straightforward: if αA,i for an embedding dimension i ∈ R^dmodel is 0.2, then the update follows hi ← (1 − 0.2)hi + 0.2hA,i. Thus, they directly quantify the contribution of hA,i into h. Figure 6 shows the average absolute values of hA and hM at each layer. Notably, the network learns to take only modest steps (20%-30%) in the direction suggested by hA and hM. The average magnitude of αA decreases from 0.25 in the 0.5B network (24 layers) to 0.20 in the 1B network (36 layers). Meanwhile, αM decreases from 0.37 to 0.32, possibly because MLP blocks have more parameters, making their suggestions more precise.\n\nThe scaling factors su, sν and sqk remain relatively stable across layers. The value of sν can be interpreted as a measure of the non-linearity of the SiLU function, which behaves like ReLU for large sν and approximates a linear unit for values near 0 (see also Appendix A.1). The distribution of sz is primarily characterized by its mean, which influences the temperature of the softmax during cross-entropy calculations. The introduced scaling factors sqk, su, sν and sz seem to compensate for the removal of magnitude information when normalizing matrices and embeddings.\n\n## 3.3 ABLATION STUDIES\n\nAppendix A.8 summarizes numerous ablation experiments. An important finding is that having fixed (non-learnable) values for sqk, su, sν and a single global learnable value for sz leads to only a slight degradation in accuracy. Therefore, our presented general case can be simplified and become easier to interpret. Appendix A.7 demonstrates that nGPT can handle longer contexts without requiring any modifications to RoPE.\n\n## 4 RELATED WORK\n\nWang & Isola (2020) provides a comprehensive overview of the arguments for representation learning on the hypersphere. Spherical representations are associated with more stable training in the latent space of variational autoencoders (Xu & Durrett, 2018) and in embeddings used for face ver-",
    "images": [
      {
        "name": "page_9.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "Figure 6: (Left): Eigen learning rates of Attention and MLP blocks. (Middle): Scaling factors applied to the intermediate states of MLP. (Right): Scaling factors applied before the QK dot product; distribution of per-vector scalings applied to logits. Models are trained for 100k iterations.",
        "md": "Figure 6: (Left): Eigen learning rates of Attention and MLP blocks. (Middle): Scaling factors applied to the intermediate states of MLP. (Right): Scaling factors applied before the QK dot product; distribution of per-vector scalings applied to logits. Models are trained for 100k iterations.",
        "bBox": {
          "x": 107,
          "y": 90,
          "w": 396.25,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Eigen learning rates of Attention",
            "Scaling su for matrix Wu of MLP",
            "Scaling sqk"
          ],
          [
            "nGPT 0.5B",
            "nGPT 0.5B",
            "nGPT 0.5B"
          ],
          [
            "nGPT 1B",
            "nGPT 1B",
            "nGPT 1B"
          ]
        ],
        "md": "| Eigen learning rates of Attention | Scaling su for matrix Wu of MLP | Scaling sqk |\n|-----------------------------------|--------------------------------|-------------|\n| nGPT 0.5B                         | nGPT 0.5B                      | nGPT 0.5B   |\n| nGPT 1B                           | nGPT 1B                        | nGPT 1B     |",
        "isPerfectTable": true,
        "csv": "\"Eigen learning rates of Attention\",\"Scaling su for matrix Wu of MLP\",\"Scaling sqk\"\n\"nGPT 0.5B\",\"nGPT 0.5B\",\"nGPT 0.5B\"\n\"nGPT 1B\",\"nGPT 1B\",\"nGPT 1B\"",
        "bBox": {
          "x": 145,
          "y": 88,
          "w": 91.63,
          "h": 6.92
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Eigen learning rates of MLP",
            "Scaling sv for matrix Wv of MLP",
            "Distribution of scaling sz for 32k embeddings"
          ],
          [
            "nGPT 0.5B",
            "nGPT 0.5B",
            "nGPT 0.5B"
          ],
          [
            "nGPT 1B",
            "nGPT 1B",
            "nGPT 1B"
          ]
        ],
        "md": "| Eigen learning rates of MLP | Scaling sv for matrix Wv of MLP | Distribution of scaling sz for 32k embeddings |\n|-----------------------------|--------------------------------|-----------------------------------------------|\n| nGPT 0.5B                   | nGPT 0.5B                      | nGPT 0.5B                                     |\n| nGPT 1B                     | nGPT 1B                        | nGPT 1B                                       |",
        "isPerfectTable": true,
        "csv": "\"Eigen learning rates of MLP\",\"Scaling sv for matrix Wv of MLP\",\"Distribution of scaling sz for 32k embeddings\"\n\"nGPT 0.5B\",\"nGPT 0.5B\",\"nGPT 0.5B\"\n\"nGPT 1B\",\"nGPT 1B\",\"nGPT 1B\"",
        "bBox": {
          "x": 145,
          "y": 93,
          "w": 126.25,
          "h": 124.9
        }
      },
      {
        "type": "text",
        "value": "An important contribution of this work is the decoupling of predictions made by the Attention and MLP blocks from their impact on the hidden state h. These contributions are controlled by the eigen learning rates αA and αM. Their interpretation is straightforward: if αA,i for an embedding dimension i ∈ R^dmodel is 0.2, then the update follows hi ← (1 − 0.2)hi + 0.2hA,i. Thus, they directly quantify the contribution of hA,i into h. Figure 6 shows the average absolute values of hA and hM at each layer. Notably, the network learns to take only modest steps (20%-30%) in the direction suggested by hA and hM. The average magnitude of αA decreases from 0.25 in the 0.5B network (24 layers) to 0.20 in the 1B network (36 layers). Meanwhile, αM decreases from 0.37 to 0.32, possibly because MLP blocks have more parameters, making their suggestions more precise.\n\nThe scaling factors su, sν and sqk remain relatively stable across layers. The value of sν can be interpreted as a measure of the non-linearity of the SiLU function, which behaves like ReLU for large sν and approximates a linear unit for values near 0 (see also Appendix A.1). The distribution of sz is primarily characterized by its mean, which influences the temperature of the softmax during cross-entropy calculations. The introduced scaling factors sqk, su, sν and sz seem to compensate for the removal of magnitude information when normalizing matrices and embeddings.",
        "md": "An important contribution of this work is the decoupling of predictions made by the Attention and MLP blocks from their impact on the hidden state h. These contributions are controlled by the eigen learning rates αA and αM. Their interpretation is straightforward: if αA,i for an embedding dimension i ∈ R^dmodel is 0.2, then the update follows hi ← (1 − 0.2)hi + 0.2hA,i. Thus, they directly quantify the contribution of hA,i into h. Figure 6 shows the average absolute values of hA and hM at each layer. Notably, the network learns to take only modest steps (20%-30%) in the direction suggested by hA and hM. The average magnitude of αA decreases from 0.25 in the 0.5B network (24 layers) to 0.20 in the 1B network (36 layers). Meanwhile, αM decreases from 0.37 to 0.32, possibly because MLP blocks have more parameters, making their suggestions more precise.\n\nThe scaling factors su, sν and sqk remain relatively stable across layers. The value of sν can be interpreted as a measure of the non-linearity of the SiLU function, which behaves like ReLU for large sν and approximates a linear unit for values near 0 (see also Appendix A.1). The distribution of sz is primarily characterized by its mean, which influences the temperature of the softmax during cross-entropy calculations. The introduced scaling factors sqk, su, sν and sz seem to compensate for the removal of magnitude information when normalizing matrices and embeddings.",
        "bBox": {
          "x": 107,
          "y": 93,
          "w": 397.06,
          "h": 11.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "3.3 ABLATION STUDIES",
        "md": "## 3.3 ABLATION STUDIES",
        "bBox": {
          "x": 108,
          "y": 93,
          "w": 152.7,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "Appendix A.8 summarizes numerous ablation experiments. An important finding is that having fixed (non-learnable) values for sqk, su, sν and a single global learnable value for sz leads to only a slight degradation in accuracy. Therefore, our presented general case can be simplified and become easier to interpret. Appendix A.7 demonstrates that nGPT can handle longer contexts without requiring any modifications to RoPE.",
        "md": "Appendix A.8 summarizes numerous ablation experiments. An important finding is that having fixed (non-learnable) values for sqk, su, sν and a single global learnable value for sz leads to only a slight degradation in accuracy. Therefore, our presented general case can be simplified and become easier to interpret. Appendix A.7 demonstrates that nGPT can handle longer contexts without requiring any modifications to RoPE.",
        "bBox": {
          "x": 108,
          "y": 325,
          "w": 396.3,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "4 RELATED WORK",
        "md": "## 4 RELATED WORK",
        "bBox": {
          "x": 108,
          "y": 685,
          "w": 84.18,
          "h": 11.96
        }
      },
      {
        "type": "text",
        "value": "Wang & Isola (2020) provides a comprehensive overview of the arguments for representation learning on the hypersphere. Spherical representations are associated with more stable training in the latent space of variational autoencoders (Xu & Durrett, 2018) and in embeddings used for face ver-",
        "md": "Wang & Isola (2020) provides a comprehensive overview of the arguments for representation learning on the hypersphere. Spherical representations are associated with more stable training in the latent space of variational autoencoders (Xu & Durrett, 2018) and in embeddings used for face ver-",
        "bBox": {
          "x": 108,
          "y": 93,
          "w": 396.1,
          "h": 9.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": "at each layer. Notably, the network learns to take only modest steps (20%-30%) in the direction"
      },
      {
        "text": "is primarily characterized by its mean, which influences the temperature of the softmax during"
      },
      {
        "text": "(non-learnable) values for"
      },
      {
        "text": "any modifications to RoPE."
      },
      {
        "text": "Wang & Isola (2020) provides a comprehensive overview of the arguments for representation learn-"
      },
      {
        "text": "ing on the hypersphere. Spherical representations are associated with more stable training in the"
      },
      {
        "text": ""
      },
      {
        "text": ""
      }
    ]
  },
  {
    "page": 10,
    "text": "Normalized Transformer\n\n\n\nification (Wang et al., 2017). Notably, when embeddings are well clustered, they tend to be linearly\nseparable from the rest of the embedding space (Wang & Isola, 2020). Mettes et al. (2019) demon-\nstrated that classification and regression can be unified by placing prototype embeddings uniformly\non a hypersphere, allowing for separation with large margins a priori. Wang & Isola (2020) found a\nstrong empirical correlation between downstream task performance and both the alignment (close-\nness) and uniformity of embeddings on the hypersphere.\nSince all embeddings in nGPT lie on the hypersphere, any update that causes the hidden state h to\ndeviate from the manifold is followed by a normalization step. This normalization can be interpreted\nas a retraction in the context of Riemannian optimization. One might attempt to approximate nGPT’s\nupdate in GPT by applying RMSNorm both at the beginning and end of the block (Xiong et al.,\n2020). However, this approach does not guarantee a fixed norm for the hidden state, nor does it\nensure that the recombination approximates SLERP or LERP.\nThe normalization in equations 15 and 16 closely resembles the QK normalization by Henry et al.\n(2020). In nGPT, this process can be viewed as restoring q and k of the i-th head to a (dmodel/nheads)-\ndimensional hypersphere after the projection of h by Wq and Wk, respectively. Since h and the\nembedding dimensions of Wq and Wk are already normalized, the norms of q and k are also\ncomparable, making their normalization potentially unnecessary. We explored this scenario (i.e.,\nomitting the normalization of q and k) in our ablation studies (see Appendix A.8), where the results\nshowed only a slight degradation. The performance drop was comparable to the computational time\nsavings per step. Therefore, removing the normalization of q and k in nGPT is a viable option.\n\n\n\n5     DISCUSSION AND CONCLUSION\nThis work builds on numerous key findings and observations made in the field which directly\n(Wang & Isola, 2020; Xu & Durrett, 2018; Wang et al., 2017) and indirectly (Salimans & Kingma,\n2016; Franke et al., 2023; Kodryan et al., 2022; Kosson et al., 2023) support representation learning\non the hypersphere. One of our main contributions is the normalization of the embedding dimen-\nsions of all matrices, ensuring they reside on the same hypersphere. Crucially, we observed that such\nnormalization alone would constrain the inputs of non-linear units, and, thus, the scaling factors for\nthese units should be introduced.\nIn line with recent studies suggesting that transformers implicitly perform gradient descent as meta-\noptimizers (Von Oswald et al., 2023; Dai et al., 2022), we explicitly demonstrate how this process\noccurs in the normalized Transformer: i) the transformation blocks provide gradient information, ii)\nthis information is multiplied by eigen learning rates to adjust the hidden state, and iii) the commonly\nused normalization can be interpreted as a retraction step in Riemannian optimization, projecting the\npoint back onto the hypersphere. We believe we are the first to decouple the eigen learning rates\nfrom the rest of the network, recognizing them as trainable parameters that can be interpreted as the\ndiagonal elements of a variable-metric matrix. In other words, the normalized Transformer functions\nas a variable-metric optimizer, searching for output solutions using data-driven gradient information\nestimated in its attention and MLP blocks.\nThe spherical representation provides valuable insights into the internals of nGPT, enabling the\ncollection and analysis of statistics about its normalized components. Most importantly, it allows\nfor the application of mathematical techniques specifically designed for dealing with hyperspheres.\nWe believe that the reported acceleration, by a factor from 4 to 20, is only the first step towards\nuncovering new algorithms and architectures that could emerge from nGPT. Future work should\nexplore scaling nGPT to larger network sizes, real-world datasets, and a broader range of tasks.\nFor instance, the extension of nGPT to encoder-decoder and hybrid architectures (Dao & Gu, 2024;\nDe et al., 2024) is straightforward.\n\n\n\nREFERENCES\nNaman Agarwal, Nicolas Boumal, Brian Bullins, and Coralia Cartis. Adaptive regularization with\n   cubics on manifolds. Mathematical Programming, 188, 2021.\nMaksym Andriushchenko, Francesco D’Angelo, Aditya Varre, and Nicolas Flammarion. Why do\n   we need weight decay in modern deep learning? arXiv:2310.04415, 2023.\n\n\n\n                                                               10",
    "md": "# Normalized Transformer\n\nification (Wang et al., 2017). Notably, when embeddings are well clustered, they tend to be linearly separable from the rest of the embedding space (Wang & Isola, 2020). Mettes et al. (2019) demonstrated that classification and regression can be unified by placing prototype embeddings uniformly on a hypersphere, allowing for separation with large margins a priori. Wang & Isola (2020) found a strong empirical correlation between downstream task performance and both the alignment (closeness) and uniformity of embeddings on the hypersphere.\n\nSince all embeddings in nGPT lie on the hypersphere, any update that causes the hidden state h to deviate from the manifold is followed by a normalization step. This normalization can be interpreted as a retraction in the context of Riemannian optimization. One might attempt to approximate nGPT's update in GPT by applying RMSNorm both at the beginning and end of the block (Xiong et al., 2020). However, this approach does not guarantee a fixed norm for the hidden state, nor does it ensure that the recombination approximates SLERP or LERP.\n\nThe normalization in equations 15 and 16 closely resembles the QK normalization by Henry et al. (2020). In nGPT, this process can be viewed as restoring q and k of the i-th head to a (dmodel/nheads)-dimensional hypersphere after the projection of h by Wq and Wk, respectively. Since h and the embedding dimensions of Wq and Wk are already normalized, the norms of q and k are also comparable, making their normalization potentially unnecessary. We explored this scenario (i.e., omitting the normalization of q and k) in our ablation studies (see Appendix A.8), where the results showed only a slight degradation. The performance drop was comparable to the computational time savings per step. Therefore, removing the normalization of q and k in nGPT is a viable option.\n\n## 5 DISCUSSION AND CONCLUSION\n\nThis work builds on numerous key findings and observations made in the field which directly (Wang & Isola, 2020; Xu & Durrett, 2018; Wang et al., 2017) and indirectly (Salimans & Kingma, 2016; Franke et al., 2023; Kodryan et al., 2022; Kosson et al., 2023) support representation learning on the hypersphere. One of our main contributions is the normalization of the embedding dimensions of all matrices, ensuring they reside on the same hypersphere. Crucially, we observed that such normalization alone would constrain the inputs of non-linear units, and, thus, the scaling factors for these units should be introduced.\n\nIn line with recent studies suggesting that transformers implicitly perform gradient descent as meta-optimizers (Von Oswald et al., 2023; Dai et al., 2022), we explicitly demonstrate how this process occurs in the normalized Transformer: i) the transformation blocks provide gradient information, ii) this information is multiplied by eigen learning rates to adjust the hidden state, and iii) the commonly used normalization can be interpreted as a retraction step in Riemannian optimization, projecting the point back onto the hypersphere. We believe we are the first to decouple the eigen learning rates from the rest of the network, recognizing them as trainable parameters that can be interpreted as the diagonal elements of a variable-metric matrix. In other words, the normalized Transformer functions as a variable-metric optimizer, searching for output solutions using data-driven gradient information estimated in its attention and MLP blocks.\n\nThe spherical representation provides valuable insights into the internals of nGPT, enabling the collection and analysis of statistics about its normalized components. Most importantly, it allows for the application of mathematical techniques specifically designed for dealing with hyperspheres. We believe that the reported acceleration, by a factor from 4 to 20, is only the first step towards uncovering new algorithms and architectures that could emerge from nGPT. Future work should explore scaling nGPT to larger network sizes, real-world datasets, and a broader range of tasks. For instance, the extension of nGPT to encoder-decoder and hybrid architectures (Dao & Gu, 2024; De et al., 2024) is straightforward.\n\n## REFERENCES\n\nNaman Agarwal, Nicolas Boumal, Brian Bullins, and Coralia Cartis. Adaptive regularization with\ncubics on manifolds. Mathematical Programming, 188, 2021.\n\nMaksym Andriushchenko, Francesco D'Angelo, Aditya Varre, and Nicolas Flammarion. Why do\nwe need weight decay in modern deep learning? arXiv:2310.04415, 2023.",
    "images": [
      {
        "name": "page_10.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "ification (Wang et al., 2017). Notably, when embeddings are well clustered, they tend to be linearly separable from the rest of the embedding space (Wang & Isola, 2020). Mettes et al. (2019) demonstrated that classification and regression can be unified by placing prototype embeddings uniformly on a hypersphere, allowing for separation with large margins a priori. Wang & Isola (2020) found a strong empirical correlation between downstream task performance and both the alignment (closeness) and uniformity of embeddings on the hypersphere.\n\nSince all embeddings in nGPT lie on the hypersphere, any update that causes the hidden state h to deviate from the manifold is followed by a normalization step. This normalization can be interpreted as a retraction in the context of Riemannian optimization. One might attempt to approximate nGPT's update in GPT by applying RMSNorm both at the beginning and end of the block (Xiong et al., 2020). However, this approach does not guarantee a fixed norm for the hidden state, nor does it ensure that the recombination approximates SLERP or LERP.\n\nThe normalization in equations 15 and 16 closely resembles the QK normalization by Henry et al. (2020). In nGPT, this process can be viewed as restoring q and k of the i-th head to a (dmodel/nheads)-dimensional hypersphere after the projection of h by Wq and Wk, respectively. Since h and the embedding dimensions of Wq and Wk are already normalized, the norms of q and k are also comparable, making their normalization potentially unnecessary. We explored this scenario (i.e., omitting the normalization of q and k) in our ablation studies (see Appendix A.8), where the results showed only a slight degradation. The performance drop was comparable to the computational time savings per step. Therefore, removing the normalization of q and k in nGPT is a viable option.",
        "md": "ification (Wang et al., 2017). Notably, when embeddings are well clustered, they tend to be linearly separable from the rest of the embedding space (Wang & Isola, 2020). Mettes et al. (2019) demonstrated that classification and regression can be unified by placing prototype embeddings uniformly on a hypersphere, allowing for separation with large margins a priori. Wang & Isola (2020) found a strong empirical correlation between downstream task performance and both the alignment (closeness) and uniformity of embeddings on the hypersphere.\n\nSince all embeddings in nGPT lie on the hypersphere, any update that causes the hidden state h to deviate from the manifold is followed by a normalization step. This normalization can be interpreted as a retraction in the context of Riemannian optimization. One might attempt to approximate nGPT's update in GPT by applying RMSNorm both at the beginning and end of the block (Xiong et al., 2020). However, this approach does not guarantee a fixed norm for the hidden state, nor does it ensure that the recombination approximates SLERP or LERP.\n\nThe normalization in equations 15 and 16 closely resembles the QK normalization by Henry et al. (2020). In nGPT, this process can be viewed as restoring q and k of the i-th head to a (dmodel/nheads)-dimensional hypersphere after the projection of h by Wq and Wk, respectively. Since h and the embedding dimensions of Wq and Wk are already normalized, the norms of q and k are also comparable, making their normalization potentially unnecessary. We explored this scenario (i.e., omitting the normalization of q and k) in our ablation studies (see Appendix A.8), where the results showed only a slight degradation. The performance drop was comparable to the computational time savings per step. Therefore, removing the normalization of q and k in nGPT is a viable option.",
        "bBox": {
          "x": 107,
          "y": 92,
          "w": 397.3,
          "h": 11.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "5 DISCUSSION AND CONCLUSION",
        "md": "## 5 DISCUSSION AND CONCLUSION",
        "bBox": {
          "x": 108,
          "y": 342,
          "w": 160.47,
          "h": 11.96
        }
      },
      {
        "type": "text",
        "value": "This work builds on numerous key findings and observations made in the field which directly (Wang & Isola, 2020; Xu & Durrett, 2018; Wang et al., 2017) and indirectly (Salimans & Kingma, 2016; Franke et al., 2023; Kodryan et al., 2022; Kosson et al., 2023) support representation learning on the hypersphere. One of our main contributions is the normalization of the embedding dimensions of all matrices, ensuring they reside on the same hypersphere. Crucially, we observed that such normalization alone would constrain the inputs of non-linear units, and, thus, the scaling factors for these units should be introduced.\n\nIn line with recent studies suggesting that transformers implicitly perform gradient descent as meta-optimizers (Von Oswald et al., 2023; Dai et al., 2022), we explicitly demonstrate how this process occurs in the normalized Transformer: i) the transformation blocks provide gradient information, ii) this information is multiplied by eigen learning rates to adjust the hidden state, and iii) the commonly used normalization can be interpreted as a retraction step in Riemannian optimization, projecting the point back onto the hypersphere. We believe we are the first to decouple the eigen learning rates from the rest of the network, recognizing them as trainable parameters that can be interpreted as the diagonal elements of a variable-metric matrix. In other words, the normalized Transformer functions as a variable-metric optimizer, searching for output solutions using data-driven gradient information estimated in its attention and MLP blocks.\n\nThe spherical representation provides valuable insights into the internals of nGPT, enabling the collection and analysis of statistics about its normalized components. Most importantly, it allows for the application of mathematical techniques specifically designed for dealing with hyperspheres. We believe that the reported acceleration, by a factor from 4 to 20, is only the first step towards uncovering new algorithms and architectures that could emerge from nGPT. Future work should explore scaling nGPT to larger network sizes, real-world datasets, and a broader range of tasks. For instance, the extension of nGPT to encoder-decoder and hybrid architectures (Dao & Gu, 2024; De et al., 2024) is straightforward.",
        "md": "This work builds on numerous key findings and observations made in the field which directly (Wang & Isola, 2020; Xu & Durrett, 2018; Wang et al., 2017) and indirectly (Salimans & Kingma, 2016; Franke et al., 2023; Kodryan et al., 2022; Kosson et al., 2023) support representation learning on the hypersphere. One of our main contributions is the normalization of the embedding dimensions of all matrices, ensuring they reside on the same hypersphere. Crucially, we observed that such normalization alone would constrain the inputs of non-linear units, and, thus, the scaling factors for these units should be introduced.\n\nIn line with recent studies suggesting that transformers implicitly perform gradient descent as meta-optimizers (Von Oswald et al., 2023; Dai et al., 2022), we explicitly demonstrate how this process occurs in the normalized Transformer: i) the transformation blocks provide gradient information, ii) this information is multiplied by eigen learning rates to adjust the hidden state, and iii) the commonly used normalization can be interpreted as a retraction step in Riemannian optimization, projecting the point back onto the hypersphere. We believe we are the first to decouple the eigen learning rates from the rest of the network, recognizing them as trainable parameters that can be interpreted as the diagonal elements of a variable-metric matrix. In other words, the normalized Transformer functions as a variable-metric optimizer, searching for output solutions using data-driven gradient information estimated in its attention and MLP blocks.\n\nThe spherical representation provides valuable insights into the internals of nGPT, enabling the collection and analysis of statistics about its normalized components. Most importantly, it allows for the application of mathematical techniques specifically designed for dealing with hyperspheres. We believe that the reported acceleration, by a factor from 4 to 20, is only the first step towards uncovering new algorithms and architectures that could emerge from nGPT. Future work should explore scaling nGPT to larger network sizes, real-world datasets, and a broader range of tasks. For instance, the extension of nGPT to encoder-decoder and hybrid architectures (Dao & Gu, 2024; De et al., 2024) is straightforward.",
        "bBox": {
          "x": 107,
          "y": 36,
          "w": 396.45,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "REFERENCES",
        "md": "## REFERENCES",
        "bBox": {
          "x": 108,
          "y": 670,
          "w": 65.88,
          "h": 11.96
        }
      },
      {
        "type": "text",
        "value": "Naman Agarwal, Nicolas Boumal, Brian Bullins, and Coralia Cartis. Adaptive regularization with\ncubics on manifolds. Mathematical Programming, 188, 2021.\n\nMaksym Andriushchenko, Francesco D'Angelo, Aditya Varre, and Nicolas Flammarion. Why do\nwe need weight decay in modern deep learning? arXiv:2310.04415, 2023.",
        "md": "Naman Agarwal, Nicolas Boumal, Brian Bullins, and Coralia Cartis. Adaptive regularization with\ncubics on manifolds. Mathematical Programming, 188, 2021.\n\nMaksym Andriushchenko, Francesco D'Angelo, Aditya Varre, and Nicolas Flammarion. Why do\nwe need weight decay in modern deep learning? arXiv:2310.04415, 2023.",
        "bBox": {
          "x": 107,
          "y": 342,
          "w": 395.94,
          "h": 11.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": "strated that classification and regression can be unified by placing prototype embeddings uniformly"
      },
      {
        "text": "strated that classification and regression can be unified by placing prototype embeddings uniformly"
      },
      {
        "text": "strated that classification and regression can be unified by placing prototype embeddings uniformly"
      },
      {
        "text": "strated that classification and regression can be unified by placing prototype embeddings uniformly"
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": "2020). However, this approach does not guarantee a fixed norm for the hidden state, nor does it"
      },
      {
        "text": "2020). However, this approach does not guarantee a fixed norm for the hidden state, nor does it"
      },
      {
        "text": "(2020). In nGPT, this process can be viewed as restoring"
      },
      {
        "text": "(2020). In nGPT, this process can be viewed as restoring"
      },
      {
        "text": ""
      },
      {
        "text": "dimensional hypersphere after the projection of"
      },
      {
        "text": "showed only a slight degradation. The performance drop was comparable to the computational time"
      },
      {
        "text": "2016; Franke et al., 2023; Kodryan et al., 2022; Kosson et al., 2023) support representation learning"
      },
      {
        "text": "2016; Franke et al., 2023; Kodryan et al., 2022; Kosson et al., 2023) support representation learning"
      },
      {
        "text": "2016; Franke et al., 2023; Kodryan et al., 2022; Kosson et al., 2023) support representation learning"
      },
      {
        "text": "2016; Franke et al., 2023; Kodryan et al., 2022; Kosson et al., 2023) support representation learning"
      },
      {
        "text": "2016; Franke et al., 2023; Kodryan et al., 2022; Kosson et al., 2023) support representation learning"
      },
      {
        "text": "2016; Franke et al., 2023; Kodryan et al., 2022; Kosson et al., 2023) support representation learning"
      },
      {
        "text": "2016; Franke et al., 2023; Kodryan et al., 2022; Kosson et al., 2023) support representation learning"
      },
      {
        "text": "2016; Franke et al., 2023; Kodryan et al., 2022; Kosson et al., 2023) support representation learning"
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": "occurs in the normalized Transformer: i) the transformation blocks provide gradient information, ii)"
      },
      {
        "text": "occurs in the normalized Transformer: i) the transformation blocks provide gradient information, ii)"
      },
      {
        "text": "occurs in the normalized Transformer: i) the transformation blocks provide gradient information, ii)"
      },
      {
        "text": "occurs in the normalized Transformer: i) the transformation blocks provide gradient information, ii)"
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": "De et al., 2024) is straightforward."
      },
      {
        "text": ""
      }
    ]
  },
  {
    "page": 11,
    "text": "Normalized Transformer\n\n\n\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can\n   GPT learn in-context? Language models implicitly perform gradient descent as meta-optimizers.\n   arXiv:2212.10559, 2022.\nTri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms\n   through structured state space duality. arXiv:2405.21060, 2024.\nSoham De, Samuel L Smith, Anushan Fernando, et al. Griffin: Mixing gated linear recurrences with\n   local attention for efficient language models. arXiv:2402.19427, 2024.\nJ¨org K. H. Franke, Michael Hefenbrock, Gregor Koehler, and Frank Hutter. Constrained parameter\n   regularization. arXiv:2311.09058, 2023.\nAaron         Gokaslan           and        Vanya          Cohen.                       OpenWebText        corpus.\n   http://Skylion007.github.io/OpenWebTextCorpus, 2019.\nAlex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization\n   for Transformers. arXiv:2010.04245, 2020.\nAndrej Karpathy. NanoGPT. https://github.com/karpathy/nanoGPT, 2023.\nDiederik P Kingma. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.\nMaxim Kodryan, Ekaterina Lobacheva, Maksim Nakhodnov, and Dmitry P Vetrov. Training scale-\n   invariant neural networks on the sphere can happen in three regimes. NeurIPS, 2022.\nAtli Kosson, Bettina Messmer, and Martin Jaggi. Rotational equilibrium: How weight decay bal-\n   ances learning across neural networks. arXiv:2305.17212, 2023.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\nPascal Mettes, Elise Van der Pol, and Cees Snoek. Hyperspherical prototype networks. NeurIPS,\n   2019.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n   models are unsupervised multitask learners. https://openai.com/, 2018.\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate\n   training of deep neural networks. NeurIPS, 2016.\nNoam Shazeer. Gated linear units (glu). arXiv:2002.05202, 2020.\nKen Shoemake. Animating rotation with quaternion curves. In Proc. of the 12th annual conference\n   on Computer graphics and interactive techniques, 1985.\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: En-\n   hanced transformer with rotary position embedding. Neurocomputing, 2024.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n   Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\nJohannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo˜ao Sacramento, Alexander Mordv-\n   intsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient\n   descent. In ICML, 2023.\nFeng Wang, Xiang Xiang, Jian Cheng, and Alan Loddon Yuille. Normface: L2 hypersphere em-\n   bedding for face verification. In Proc. of the 25th ACM nternational conference on Multimedia,\n   2017.\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-\n   ment and uniformity on the hypersphere. In ICML, 2020.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, et al. On layer normalization in the Transformer\n   architecture. In ICML, 2020.\nJiacheng Xu and Greg Durrett.                Spherical latent spaces for stable variational autoencoders.\n   arXiv:1808.10805, 2018.\n\n\n\n                                                           11",
    "md": "Normalized Transformer\n\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can\nGPT learn in-context? Language models implicitly perform gradient descent as meta-optimizers.\narXiv:2212.10559, 2022.\n\nTri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms\nthrough structured state space duality. arXiv:2405.21060, 2024.\n\nSoham De, Samuel L Smith, Anushan Fernando, et al. Griffin: Mixing gated linear recurrences with\nlocal attention for efficient language models. arXiv:2402.19427, 2024.\n\nJörg K. H. Franke, Michael Hefenbrock, Gregor Koehler, and Frank Hutter. Constrained parameter\nregularization. arXiv:2311.09058, 2023.\n\nAaron Gokaslan and Vanya Cohen. OpenWebText corpus.\nhttp://Skylion007.github.io/OpenWebTextCorpus, 2019.\n\nAlex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization\nfor Transformers. arXiv:2010.04245, 2020.\n\nAndrej Karpathy. NanoGPT. https://github.com/karpathy/nanoGPT, 2023.\n\nDiederik P Kingma. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.\n\nMaxim Kodryan, Ekaterina Lobacheva, Maksim Nakhodnov, and Dmitry P Vetrov. Training scale-\ninvariant neural networks on the sphere can happen in three regimes. NeurIPS, 2022.\n\nAtli Kosson, Bettina Messmer, and Martin Jaggi. Rotational equilibrium: How weight decay bal-\nances learning across neural networks. arXiv:2305.17212, 2023.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n\nPascal Mettes, Elise Van der Pol, and Cees Snoek. Hyperspherical prototype networks. NeurIPS,\n2019.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. https://openai.com/, 2018.\n\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate\ntraining of deep neural networks. NeurIPS, 2016.\n\nNoam Shazeer. Gated linear units (glu). arXiv:2002.05202, 2020.\n\nKen Shoemake. Animating rotation with quaternion curves. In Proc. of the 12th annual conference\non Computer graphics and interactive techniques, 1985.\n\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: En-\nhanced transformer with rotary position embedding. Neurocomputing, 2024.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n\nJohannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordv-\nintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient\ndescent. In ICML, 2023.\n\nFeng Wang, Xiang Xiang, Jian Cheng, and Alan Loddon Yuille. Normface: L2 hypersphere em-\nbedding for face verification. In Proc. of the 25th ACM international conference on Multimedia,\n2017.\n\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-\nment and uniformity on the hypersphere. In ICML, 2020.\n\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, et al. On layer normalization in the Transformer\narchitecture. In ICML, 2020.\n\nJiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders.\narXiv:1808.10805, 2018.\n\n11",
    "images": [
      {
        "name": "page_11.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "text",
        "value": "Normalized Transformer\n\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can\nGPT learn in-context? Language models implicitly perform gradient descent as meta-optimizers.\narXiv:2212.10559, 2022.\n\nTri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms\nthrough structured state space duality. arXiv:2405.21060, 2024.\n\nSoham De, Samuel L Smith, Anushan Fernando, et al. Griffin: Mixing gated linear recurrences with\nlocal attention for efficient language models. arXiv:2402.19427, 2024.\n\nJörg K. H. Franke, Michael Hefenbrock, Gregor Koehler, and Frank Hutter. Constrained parameter\nregularization. arXiv:2311.09058, 2023.\n\nAaron Gokaslan and Vanya Cohen. OpenWebText corpus.\nhttp://Skylion007.github.io/OpenWebTextCorpus, 2019.\n\nAlex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization\nfor Transformers. arXiv:2010.04245, 2020.\n\nAndrej Karpathy. NanoGPT. https://github.com/karpathy/nanoGPT, 2023.\n\nDiederik P Kingma. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.\n\nMaxim Kodryan, Ekaterina Lobacheva, Maksim Nakhodnov, and Dmitry P Vetrov. Training scale-\ninvariant neural networks on the sphere can happen in three regimes. NeurIPS, 2022.\n\nAtli Kosson, Bettina Messmer, and Martin Jaggi. Rotational equilibrium: How weight decay bal-\nances learning across neural networks. arXiv:2305.17212, 2023.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n\nPascal Mettes, Elise Van der Pol, and Cees Snoek. Hyperspherical prototype networks. NeurIPS,\n2019.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. https://openai.com/, 2018.\n\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate\ntraining of deep neural networks. NeurIPS, 2016.\n\nNoam Shazeer. Gated linear units (glu). arXiv:2002.05202, 2020.\n\nKen Shoemake. Animating rotation with quaternion curves. In Proc. of the 12th annual conference\non Computer graphics and interactive techniques, 1985.\n\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: En-\nhanced transformer with rotary position embedding. Neurocomputing, 2024.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n\nJohannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordv-\nintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient\ndescent. In ICML, 2023.\n\nFeng Wang, Xiang Xiang, Jian Cheng, and Alan Loddon Yuille. Normface: L2 hypersphere em-\nbedding for face verification. In Proc. of the 25th ACM international conference on Multimedia,\n2017.\n\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-\nment and uniformity on the hypersphere. In ICML, 2020.\n\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, et al. On layer normalization in the Transformer\narchitecture. In ICML, 2020.\n\nJiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders.\narXiv:1808.10805, 2018.\n\n11",
        "md": "Normalized Transformer\n\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can\nGPT learn in-context? Language models implicitly perform gradient descent as meta-optimizers.\narXiv:2212.10559, 2022.\n\nTri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms\nthrough structured state space duality. arXiv:2405.21060, 2024.\n\nSoham De, Samuel L Smith, Anushan Fernando, et al. Griffin: Mixing gated linear recurrences with\nlocal attention for efficient language models. arXiv:2402.19427, 2024.\n\nJörg K. H. Franke, Michael Hefenbrock, Gregor Koehler, and Frank Hutter. Constrained parameter\nregularization. arXiv:2311.09058, 2023.\n\nAaron Gokaslan and Vanya Cohen. OpenWebText corpus.\nhttp://Skylion007.github.io/OpenWebTextCorpus, 2019.\n\nAlex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization\nfor Transformers. arXiv:2010.04245, 2020.\n\nAndrej Karpathy. NanoGPT. https://github.com/karpathy/nanoGPT, 2023.\n\nDiederik P Kingma. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.\n\nMaxim Kodryan, Ekaterina Lobacheva, Maksim Nakhodnov, and Dmitry P Vetrov. Training scale-\ninvariant neural networks on the sphere can happen in three regimes. NeurIPS, 2022.\n\nAtli Kosson, Bettina Messmer, and Martin Jaggi. Rotational equilibrium: How weight decay bal-\nances learning across neural networks. arXiv:2305.17212, 2023.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n\nPascal Mettes, Elise Van der Pol, and Cees Snoek. Hyperspherical prototype networks. NeurIPS,\n2019.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. https://openai.com/, 2018.\n\nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate\ntraining of deep neural networks. NeurIPS, 2016.\n\nNoam Shazeer. Gated linear units (glu). arXiv:2002.05202, 2020.\n\nKen Shoemake. Animating rotation with quaternion curves. In Proc. of the 12th annual conference\non Computer graphics and interactive techniques, 1985.\n\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: En-\nhanced transformer with rotary position embedding. Neurocomputing, 2024.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n\nJohannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordv-\nintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient\ndescent. In ICML, 2023.\n\nFeng Wang, Xiang Xiang, Jian Cheng, and Alan Loddon Yuille. Normface: L2 hypersphere em-\nbedding for face verification. In Proc. of the 25th ACM international conference on Multimedia,\n2017.\n\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-\nment and uniformity on the hypersphere. In ICML, 2020.\n\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, et al. On layer normalization in the Transformer\narchitecture. In ICML, 2020.\n\nJiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders.\narXiv:1808.10805, 2018.\n\n11",
        "bBox": {
          "x": 107,
          "y": 36,
          "w": 396.9,
          "h": 9.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "url": "http://skylion007.github.io/OpenWebTextCorpus",
        "text": "http://Skylion007.github.io/OpenWebTextCorpus , 2019."
      },
      {
        "url": "https://github.com/karpathy/nanoGPT",
        "text": "https://github.com/karpathy/nanoGPT , 2023."
      },
      {
        "url": "https://openai.com/",
        "text": "https://openai.com/ , 2018."
      }
    ]
  },
  {
    "page": 12,
    "text": "Normalized Transformer\n\n\n\nA      APPENDIX\nA.1     RESCALING IN THE MLP BLOCK OF THE NORMALIZED TRANSFORMER\n\n\n\nWhen computing SwiGLU using equation 18, each element x of the vector v is an input to SiLU:\n\n\n\n                                       SiLU(x) = x · σ(x) = x ·          1 + e−x ,1                                    (23)\nwhere σ(x) is sigmoid. For x with large magnitude, SiLU(x) approximates ReLU(x): when\nx → −∞, SiLU(x) → 0, and when x → ∞, SiLU(x) ≈ x. The minimum of SiLU(xmin) ≈\n−0.278 is located at xmin ≈ −1.278. While the elements of v represent dot products of dmodel-\ndimensional vectors and are bounded in [−1, 1], their expected absolute value (when they are ran-≈ dmodel. Thus, we should rescale x by √dmodel, otherwise,\ndom) is E[| cos(θ)|] = π2 ·       √dmodel1       √0.7979\nfor very small x, we end up with SiLU(x) ≈ x/2. An alternative view is to note that since the\nvariance of each of the normalized vectors (h and a vector from Wv) is 1/dmodel, the variance of\n1 (suitable for the sigmoid part of SiLU) can be restored by rescaling of x by √dmodel. Based on\nthese two views, we rescale v by √dmodel to benefit from the non-linearity of SiLU.\nA.2     EIGEN LEARNING RATES\nIn the main text of the paper, we defined eigen learning rates as positive (α ← |α| is used during the\nforward pass). However, when they are not constrained to be positive, we obtain experimental results\nwhich are the same (up to numerical difference). This surprising observation can be explained as\nfollows. Both the attention and MLP blocks have transformation matrices at their outputs. When\nthe search is unconstrained, it is sufficient for Adam to flip (select) the sign of the i-th row of\nthe output transformation matrix Wo to change the sign of the corresponding i-th coordinate in\nhA. Thus, the transformation calculated as αAWo is the same as αA′W o′, where αA,i = −αA,i′\nand W o,′(i,:) = −Wo,(i,:). In other words, when unconstrained, we can arrive at exactly the same\ntransformation by flipping the signs in both α and Wo, which cancel each other. For simplicity and\nclearer interpretation, we suggest constraining α to be positive in the main paper.\nThere is, however, another interesting scenario when eigen learning rate could become negative. In\nquasi-Newton methods, B approximates the inverse Hessian matrix H−1 whose diagonal elements\nare positive values if the function is locally convex which is the assumption of the Newton method\n(H needs to be positive definite). However, the diagonal of H−1 can have negative values if the\nobjective function is non-convex and has saddle points. While quasi-Newton methods like BFGS\naim to ensure (e.g., via regularization) that B is positive-definite even on non-convex problem, some\nRiemannian optimization methods can exploit negative curvature of H−1 (Agarwal et al., 2021).\nWhen the diagonal of B is unconstrained, we perform a variable-metric step, acknowledging that\nthe function may be non-convex locally. We did not mention this in the main paper because, as noted\nearlier, the results with both constrained and unconstrained α are essentially the same.\n\n\n\nA.3     RIEMANNIAN OPTIMIZATION\n\n\n\nIf h − hA is viewed as the gradient g in the Euclidean space, then, aligning with the requirements\nof Riemannian optimization, the projection of g onto the tangent space of the hypersphere is\n\n\n\n                                               gproj ← h(hT hA) − hA                                                   (24)\nThe projection is equivalent to g when the dot product hT hA is 1. Depending on the alignment\nbetween h and hA, the projected gradient varies between h − hA (when the vectors are aligned) and\n−hA (when the vectors are orthogonal). The Riemannian variable-metric update then reads as:\n\n\n\n                                    h ← Norm( h − BA(h(hT hA) − hA) )                                                  (25)\n                                    h ← Norm( h − BM(h(hT hM) − hM) )                                                  (26)\n\n\n\n                                                            12",
    "md": "Normalized Transformer\n\n## A APPENDIX\n\n### A.1 RESCALING IN THE MLP BLOCK OF THE NORMALIZED TRANSFORMER\n\nWhen computing SwiGLU using equation 18, each element x of the vector v is an input to SiLU:\n\n$$\\text{SiLU}(x) = x \\cdot \\sigma(x) = x \\cdot \\frac{1}{1 + e^{-x}},\\tag{23}$$\n\nwhere σ(x) is sigmoid. For x with large magnitude, SiLU(x) approximates ReLU(x): when x → −∞, SiLU(x) → 0, and when x → ∞, SiLU(x) ≈ x. The minimum of SiLU(x_min) ≈ −0.278 is located at x_min ≈ −1.278. While the elements of v represent dot products of d_model-dimensional vectors and are bounded in [−1, 1], their expected absolute value (when they are random) is E[|cos(θ)|] = π/2 · 1/√d_model ≈ 0.7979/√d_model. Thus, we should rescale x by √d_model, otherwise, for very small x, we end up with SiLU(x) ≈ x/2. An alternative view is to note that since the variance of each of the normalized vectors (h and a vector from W_v) is 1/d_model, the variance of 1 (suitable for the sigmoid part of SiLU) can be restored by rescaling of x by √d_model. Based on these two views, we rescale v by √d_model to benefit from the non-linearity of SiLU.\n\n### A.2 EIGEN LEARNING RATES\n\nIn the main text of the paper, we defined eigen learning rates as positive (α ← |α| is used during the forward pass). However, when they are not constrained to be positive, we obtain experimental results which are the same (up to numerical difference). This surprising observation can be explained as follows. Both the attention and MLP blocks have transformation matrices at their outputs. When the search is unconstrained, it is sufficient for Adam to flip (select) the sign of the i-th row of the output transformation matrix W_o to change the sign of the corresponding i-th coordinate in h_A. Thus, the transformation calculated as α_AW_o is the same as α_A'W_o', where α_A,i = −α_A,i' and W_o',(i,:) = −W_o,(i,:). In other words, when unconstrained, we can arrive at exactly the same transformation by flipping the signs in both α and W_o, which cancel each other. For simplicity and clearer interpretation, we suggest constraining α to be positive in the main paper.\n\nThere is, however, another interesting scenario when eigen learning rate could become negative. In quasi-Newton methods, B approximates the inverse Hessian matrix H^(−1) whose diagonal elements are positive values if the function is locally convex which is the assumption of the Newton method (H needs to be positive definite). However, the diagonal of H^(−1) can have negative values if the objective function is non-convex and has saddle points. While quasi-Newton methods like BFGS aim to ensure (e.g., via regularization) that B is positive-definite even on non-convex problem, some Riemannian optimization methods can exploit negative curvature of H^(−1) (Agarwal et al., 2021). When the diagonal of B is unconstrained, we perform a variable-metric step, acknowledging that the function may be non-convex locally. We did not mention this in the main paper because, as noted earlier, the results with both constrained and unconstrained α are essentially the same.\n\n### A.3 RIEMANNIAN OPTIMIZATION\n\nIf h − h_A is viewed as the gradient g in the Euclidean space, then, aligning with the requirements of Riemannian optimization, the projection of g onto the tangent space of the hypersphere is\n\n$$g_{\\text{proj}} \\leftarrow h(h^T h_A) - h_A\\tag{24}$$\n\nThe projection is equivalent to g when the dot product h^T h_A is 1. Depending on the alignment between h and h_A, the projected gradient varies between h − h_A (when the vectors are aligned) and −h_A (when the vectors are orthogonal). The Riemannian variable-metric update then reads as:\n\n$$h \\leftarrow \\text{Norm}( h - B_A(h(h^T h_A) - h_A) )\\tag{25}$$\n\n$$h \\leftarrow \\text{Norm}( h - B_M(h(h^T h_M) - h_M) )\\tag{26}$$",
    "images": [
      {
        "name": "page_12.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "text",
        "value": "Normalized Transformer",
        "md": "Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 11.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "A APPENDIX",
        "md": "## A APPENDIX",
        "bBox": {
          "x": 108,
          "y": 92,
          "w": 52.39,
          "h": 11.96
        }
      },
      {
        "type": "heading",
        "lvl": 3,
        "value": "A.1 RESCALING IN THE MLP BLOCK OF THE NORMALIZED TRANSFORMER",
        "md": "### A.1 RESCALING IN THE MLP BLOCK OF THE NORMALIZED TRANSFORMER",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 298.36,
          "h": 11.96
        }
      },
      {
        "type": "text",
        "value": "When computing SwiGLU using equation 18, each element x of the vector v is an input to SiLU:\n\n$$\\text{SiLU}(x) = x \\cdot \\sigma(x) = x \\cdot \\frac{1}{1 + e^{-x}},\\tag{23}$$\n\nwhere σ(x) is sigmoid. For x with large magnitude, SiLU(x) approximates ReLU(x): when x → −∞, SiLU(x) → 0, and when x → ∞, SiLU(x) ≈ x. The minimum of SiLU(x_min) ≈ −0.278 is located at x_min ≈ −1.278. While the elements of v represent dot products of d_model-dimensional vectors and are bounded in [−1, 1], their expected absolute value (when they are random) is E[|cos(θ)|] = π/2 · 1/√d_model ≈ 0.7979/√d_model. Thus, we should rescale x by √d_model, otherwise, for very small x, we end up with SiLU(x) ≈ x/2. An alternative view is to note that since the variance of each of the normalized vectors (h and a vector from W_v) is 1/d_model, the variance of 1 (suitable for the sigmoid part of SiLU) can be restored by rescaling of x by √d_model. Based on these two views, we rescale v by √d_model to benefit from the non-linearity of SiLU.",
        "md": "When computing SwiGLU using equation 18, each element x of the vector v is an input to SiLU:\n\n$$\\text{SiLU}(x) = x \\cdot \\sigma(x) = x \\cdot \\frac{1}{1 + e^{-x}},\\tag{23}$$\n\nwhere σ(x) is sigmoid. For x with large magnitude, SiLU(x) approximates ReLU(x): when x → −∞, SiLU(x) → 0, and when x → ∞, SiLU(x) ≈ x. The minimum of SiLU(x_min) ≈ −0.278 is located at x_min ≈ −1.278. While the elements of v represent dot products of d_model-dimensional vectors and are bounded in [−1, 1], their expected absolute value (when they are random) is E[|cos(θ)|] = π/2 · 1/√d_model ≈ 0.7979/√d_model. Thus, we should rescale x by √d_model, otherwise, for very small x, we end up with SiLU(x) ≈ x/2. An alternative view is to note that since the variance of each of the normalized vectors (h and a vector from W_v) is 1/d_model, the variance of 1 (suitable for the sigmoid part of SiLU) can be restored by rescaling of x by √d_model. Based on these two views, we rescale v by √d_model to benefit from the non-linearity of SiLU.",
        "bBox": {
          "x": 107,
          "y": 92,
          "w": 396.66,
          "h": 13.96
        }
      },
      {
        "type": "heading",
        "lvl": 3,
        "value": "A.2 EIGEN LEARNING RATES",
        "md": "### A.2 EIGEN LEARNING RATES",
        "bBox": {
          "x": 108,
          "y": 92,
          "w": 103.95,
          "h": 11.96
        }
      },
      {
        "type": "text",
        "value": "In the main text of the paper, we defined eigen learning rates as positive (α ← |α| is used during the forward pass). However, when they are not constrained to be positive, we obtain experimental results which are the same (up to numerical difference). This surprising observation can be explained as follows. Both the attention and MLP blocks have transformation matrices at their outputs. When the search is unconstrained, it is sufficient for Adam to flip (select) the sign of the i-th row of the output transformation matrix W_o to change the sign of the corresponding i-th coordinate in h_A. Thus, the transformation calculated as α_AW_o is the same as α_A'W_o', where α_A,i = −α_A,i' and W_o',(i,:) = −W_o,(i,:). In other words, when unconstrained, we can arrive at exactly the same transformation by flipping the signs in both α and W_o, which cancel each other. For simplicity and clearer interpretation, we suggest constraining α to be positive in the main paper.\n\nThere is, however, another interesting scenario when eigen learning rate could become negative. In quasi-Newton methods, B approximates the inverse Hessian matrix H^(−1) whose diagonal elements are positive values if the function is locally convex which is the assumption of the Newton method (H needs to be positive definite). However, the diagonal of H^(−1) can have negative values if the objective function is non-convex and has saddle points. While quasi-Newton methods like BFGS aim to ensure (e.g., via regularization) that B is positive-definite even on non-convex problem, some Riemannian optimization methods can exploit negative curvature of H^(−1) (Agarwal et al., 2021). When the diagonal of B is unconstrained, we perform a variable-metric step, acknowledging that the function may be non-convex locally. We did not mention this in the main paper because, as noted earlier, the results with both constrained and unconstrained α are essentially the same.",
        "md": "In the main text of the paper, we defined eigen learning rates as positive (α ← |α| is used during the forward pass). However, when they are not constrained to be positive, we obtain experimental results which are the same (up to numerical difference). This surprising observation can be explained as follows. Both the attention and MLP blocks have transformation matrices at their outputs. When the search is unconstrained, it is sufficient for Adam to flip (select) the sign of the i-th row of the output transformation matrix W_o to change the sign of the corresponding i-th coordinate in h_A. Thus, the transformation calculated as α_AW_o is the same as α_A'W_o', where α_A,i = −α_A,i' and W_o',(i,:) = −W_o,(i,:). In other words, when unconstrained, we can arrive at exactly the same transformation by flipping the signs in both α and W_o, which cancel each other. For simplicity and clearer interpretation, we suggest constraining α to be positive in the main paper.\n\nThere is, however, another interesting scenario when eigen learning rate could become negative. In quasi-Newton methods, B approximates the inverse Hessian matrix H^(−1) whose diagonal elements are positive values if the function is locally convex which is the assumption of the Newton method (H needs to be positive definite). However, the diagonal of H^(−1) can have negative values if the objective function is non-convex and has saddle points. While quasi-Newton methods like BFGS aim to ensure (e.g., via regularization) that B is positive-definite even on non-convex problem, some Riemannian optimization methods can exploit negative curvature of H^(−1) (Agarwal et al., 2021). When the diagonal of B is unconstrained, we perform a variable-metric step, acknowledging that the function may be non-convex locally. We did not mention this in the main paper because, as noted earlier, the results with both constrained and unconstrained α are essentially the same.",
        "bBox": {
          "x": 107,
          "y": 92,
          "w": 396.82,
          "h": 11.96
        }
      },
      {
        "type": "heading",
        "lvl": 3,
        "value": "A.3 RIEMANNIAN OPTIMIZATION",
        "md": "### A.3 RIEMANNIAN OPTIMIZATION",
        "bBox": {
          "x": 108,
          "y": 92,
          "w": 121.71,
          "h": 11.96
        }
      },
      {
        "type": "text",
        "value": "If h − h_A is viewed as the gradient g in the Euclidean space, then, aligning with the requirements of Riemannian optimization, the projection of g onto the tangent space of the hypersphere is\n\n$$g_{\\text{proj}} \\leftarrow h(h^T h_A) - h_A\\tag{24}$$\n\nThe projection is equivalent to g when the dot product h^T h_A is 1. Depending on the alignment between h and h_A, the projected gradient varies between h − h_A (when the vectors are aligned) and −h_A (when the vectors are orthogonal). The Riemannian variable-metric update then reads as:\n\n$$h \\leftarrow \\text{Norm}( h - B_A(h(h^T h_A) - h_A) )\\tag{25}$$\n\n$$h \\leftarrow \\text{Norm}( h - B_M(h(h^T h_M) - h_M) )\\tag{26}$$",
        "md": "If h − h_A is viewed as the gradient g in the Euclidean space, then, aligning with the requirements of Riemannian optimization, the projection of g onto the tangent space of the hypersphere is\n\n$$g_{\\text{proj}} \\leftarrow h(h^T h_A) - h_A\\tag{24}$$\n\nThe projection is equivalent to g when the dot product h^T h_A is 1. Depending on the alignment between h and h_A, the projected gradient varies between h − h_A (when the vectors are aligned) and −h_A (when the vectors are orthogonal). The Riemannian variable-metric update then reads as:\n\n$$h \\leftarrow \\text{Norm}( h - B_A(h(h^T h_A) - h_A) )\\tag{25}$$\n\n$$h \\leftarrow \\text{Norm}( h - B_M(h(h^T h_M) - h_M) )\\tag{26}$$",
        "bBox": {
          "x": 107,
          "y": 92,
          "w": 368.52,
          "h": 11.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": ""
      },
      {
        "text": "is unconstrained, we perform a variable-metric step, acknowledging that"
      },
      {
        "text": "is unconstrained, we perform a variable-metric step, acknowledging that"
      }
    ]
  },
  {
    "page": 13,
    "text": "Normalized Transformer\n\n\n\n                                 Table 2: Model Parameters for GPT and nGPT\n\n\n\n               Model Parameter                                      0.5B Models            1.0B Models\n               Number of Layers (nlayers)                                   24                    36\n               Model Dimension (dmodel)                                   1024                  1280\n               Number of Attention Heads (nheads)                           16                    20\n               Key Dimension (dk)                                    dmodel/nheads         dmodel/nheads\n               MLP Dimension (dMLP)                                     4dmodel                4dmodel\n               Parameters in GPT                                        468.2M               1025.7M\n               Parameters in nGPT                                       468.4M               1026.1M\n\n\n\n                             Table 3: Optimization Parameters for GPT and nGPT\n\n\n\n      Optimization Parameter                      GPT                                   nGPT\n      Optimizer                                 AdamW               Adam (AdamW with weight decay 0.0)\n      Weight Decay                                 0.1                                     0.0\n      Number of Warmup Steps                      2000                                      0\n      Learning Rate Schedule              Cosine Annealing                       Cosine Annealing\n      Initial Learning Rate                problem-specific                       problem-specific\n      Final Learning Rate                           0                                       0\n\n\n\nThe normalization by Norm can be viewed as the retraction step in Riemannian optimization, map-\nping the updated solution back to the manifold.\nOur experimental results suggest that the impact of hT hM is negligible. Therefore, all experiments\nin the paper are based on equations 10 and 11.\n\n\n\nA.4     TIME COST PER STEP\n\n\n\nThe time cost per step for nGPT is approximately 80% higher with 4k context length, and 60%\nhigher with 8k context length. This overhead is not only due to nGPT having 6 normalization steps\n(2 of them are applied for q and k) per layer instead of 2, but also because nGPT’s normalizations\nare not yet fully optimized, unlike GPT, where normalization layers are fused with other operations.\nTraining on larger networks is expected to further reduce this performance gap, as the number of\nlayers (and thus the number of normalizations) increases only modestly with the number of network\nparameters. Appendix A.8 shows that we can remove the normalization of q and k with a minor\nnegative impact on results.\n\n\n\nA.5     EXPERIMENTAL SETUP\n\n\n\nIn all experiments, we use OpenWebText (Gokaslan & Cohen, 2019) dataset which, according to\nexperiments of Karpathy (2023), represents a good approximation of the OpenAI’s internal dataset\nused to train GPT-2 models. We are well aware that this dataset is not of the highest quality. How-\never, we believe that it is suitable for academic research, and, should improve the comparability of\nour findings with other research papers.\nWe trained our models using 64 A100 GPUs distributed across 8 nodes (8 GPUs per node). Global\nbatch size is 512. We use the LLaMA-2 tokenizer with 32k tokens. We use the same setup for the\n0.5B and 1.0B parameter models.\nAll matrix parameters are initialized by sampling from a zero-mean normal distribution with a stan-\ndard deviation of 0.02 for GPT and 1/√dmodel for nGPT. The standard deviation for the output\nmatrices was scaled by a factor of √2 × nlayer, as suggested by Radford et al. (2018). The initial-\nization of matrix parameters is not important for nGPT because they are normalized afterwards.\nThe base of RoPE is 10000. The initialization of the additional parameters introduced in nGPT is\ndescribed in Section 2.6.\n\n\n\n                                                            13",
    "md": "# Normalized Transformer\n\n## Table 2: Model Parameters for GPT and nGPT\n\n| Model Parameter | 0.5B Models | 1.0B Models |\n|-----------------|-------------|-------------|\n| Number of Layers (n_layers) | 24 | 36 |\n| Model Dimension (d_model) | 1024 | 1280 |\n| Number of Attention Heads (n_heads) | 16 | 20 |\n| Key Dimension (d_k) | d_model / n_heads | d_model / n_heads |\n| MLP Dimension (d_MLP) | 4d_model | 4d_model |\n| Parameters in GPT | 468.2M | 1025.7M |\n| Parameters in nGPT | 468.4M | 1026.1M |\n\n## Table 3: Optimization Parameters for GPT and nGPT\n\n| Optimization Parameter | GPT | nGPT |\n|------------------------|-----|------|\n| Optimizer | AdamW | Adam (AdamW with weight decay 0.0) |\n| Weight Decay | 0.1 | 0.0 |\n| Number of Warmup Steps | 2000 | 0 |\n| Learning Rate Schedule | Cosine Annealing | Cosine Annealing |\n| Initial Learning Rate | problem-specific | problem-specific |\n| Final Learning Rate | 0 | 0 |\n\nThe normalization by Norm can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold.\n\nOur experimental results suggest that the impact of h^T h_M is negligible. Therefore, all experiments in the paper are based on equations 10 and 11.\n\n### A.4 TIME COST PER STEP\n\nThe time cost per step for nGPT is approximately 80% higher with 4k context length, and 60% higher with 8k context length. This overhead is not only due to nGPT having 6 normalization steps (2 of them are applied for q and k) per layer instead of 2, but also because nGPT's normalizations are not yet fully optimized, unlike GPT, where normalization layers are fused with other operations. Training on larger networks is expected to further reduce this performance gap, as the number of layers (and thus the number of normalizations) increases only modestly with the number of network parameters. Appendix A.8 shows that we can remove the normalization of q and k with a minor negative impact on results.\n\n### A.5 EXPERIMENTAL SETUP\n\nIn all experiments, we use OpenWebText (Gokaslan & Cohen, 2019) dataset which, according to experiments of Karpathy (2023), represents a good approximation of the OpenAI's internal dataset used to train GPT-2 models. We are well aware that this dataset is not of the highest quality. However, we believe that it is suitable for academic research, and, should improve the comparability of our findings with other research papers.\n\nWe trained our models using 64 A100 GPUs distributed across 8 nodes (8 GPUs per node). Global batch size is 512. We use the LLaMA-2 tokenizer with 32k tokens. We use the same setup for the 0.5B and 1.0B parameter models.\n\nAll matrix parameters are initialized by sampling from a zero-mean normal distribution with a standard deviation of 0.02 for GPT and 1/√d_model for nGPT. The standard deviation for the output matrices was scaled by a factor of √2 × n_layer, as suggested by Radford et al. (2018). The initialization of matrix parameters is not important for nGPT because they are normalized afterwards. The base of RoPE is 10000. The initialization of the additional parameters introduced in nGPT is described in Section 2.6.",
    "images": [
      {
        "name": "page_13.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "Table 2: Model Parameters for GPT and nGPT",
        "md": "## Table 2: Model Parameters for GPT and nGPT",
        "bBox": {
          "x": 155,
          "y": 90,
          "w": 243.45,
          "h": 11.04
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Model Parameter",
            "0.5B Models",
            "1.0B Models"
          ],
          [
            "Number of Layers (n_layers)",
            "24",
            "36"
          ],
          [
            "Model Dimension (d_model)",
            "1024",
            "1280"
          ],
          [
            "Number of Attention Heads (n_heads)",
            "16",
            "20"
          ],
          [
            "Key Dimension (d_k)",
            "d_model / n_heads",
            "d_model / n_heads"
          ],
          [
            "MLP Dimension (d_MLP)",
            "4d_model",
            "4d_model"
          ],
          [
            "Parameters in GPT",
            "468.2M",
            "1025.7M"
          ],
          [
            "Parameters in nGPT",
            "468.4M",
            "1026.1M"
          ]
        ],
        "md": "| Model Parameter | 0.5B Models | 1.0B Models |\n|-----------------|-------------|-------------|\n| Number of Layers (n_layers) | 24 | 36 |\n| Model Dimension (d_model) | 1024 | 1280 |\n| Number of Attention Heads (n_heads) | 16 | 20 |\n| Key Dimension (d_k) | d_model / n_heads | d_model / n_heads |\n| MLP Dimension (d_MLP) | 4d_model | 4d_model |\n| Parameters in GPT | 468.2M | 1025.7M |\n| Parameters in nGPT | 468.4M | 1026.1M |",
        "isPerfectTable": true,
        "csv": "\"Model Parameter\",\"0.5B Models\",\"1.0B Models\"\n\"Number of Layers (n_layers)\",\"24\",\"36\"\n\"Model Dimension (d_model)\",\"1024\",\"1280\"\n\"Number of Attention Heads (n_heads)\",\"16\",\"20\"\n\"Key Dimension (d_k)\",\"d_model / n_heads\",\"d_model / n_heads\"\n\"MLP Dimension (d_MLP)\",\"4d_model\",\"4d_model\"\n\"Parameters in GPT\",\"468.2M\",\"1025.7M\"\n\"Parameters in nGPT\",\"468.4M\",\"1026.1M\"",
        "bBox": {
          "x": 155,
          "y": 116,
          "w": 89.44,
          "h": 11.04
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "Table 3: Optimization Parameters for GPT and nGPT",
        "md": "## Table 3: Optimization Parameters for GPT and nGPT",
        "bBox": {
          "x": 126,
          "y": 233,
          "w": 285.97,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Optimization Parameter",
            "GPT",
            "nGPT"
          ],
          [
            "Optimizer",
            "AdamW",
            "Adam (AdamW with weight decay 0.0)"
          ],
          [
            "Weight Decay",
            "0.1",
            "0.0"
          ],
          [
            "Number of Warmup Steps",
            "2000",
            "0"
          ],
          [
            "Learning Rate Schedule",
            "Cosine Annealing",
            "Cosine Annealing"
          ],
          [
            "Initial Learning Rate",
            "problem-specific",
            "problem-specific"
          ],
          [
            "Final Learning Rate",
            "0",
            "0"
          ]
        ],
        "md": "| Optimization Parameter | GPT | nGPT |\n|------------------------|-----|------|\n| Optimizer | AdamW | Adam (AdamW with weight decay 0.0) |\n| Weight Decay | 0.1 | 0.0 |\n| Number of Warmup Steps | 2000 | 0 |\n| Learning Rate Schedule | Cosine Annealing | Cosine Annealing |\n| Initial Learning Rate | problem-specific | problem-specific |\n| Final Learning Rate | 0 | 0 |",
        "isPerfectTable": true,
        "csv": "\"Optimization Parameter\",\"GPT\",\"nGPT\"\n\"Optimizer\",\"AdamW\",\"Adam (AdamW with weight decay 0.0)\"\n\"Weight Decay\",\"0.1\",\"0.0\"\n\"Number of Warmup Steps\",\"2000\",\"0\"\n\"Learning Rate Schedule\",\"Cosine Annealing\",\"Cosine Annealing\"\n\"Initial Learning Rate\",\"problem-specific\",\"problem-specific\"\n\"Final Learning Rate\",\"0\",\"0\"",
        "bBox": {
          "x": 126,
          "y": 158,
          "w": 157.88,
          "h": 109.96
        }
      },
      {
        "type": "text",
        "value": "The normalization by Norm can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold.\n\nOur experimental results suggest that the impact of h^T h_M is negligible. Therefore, all experiments in the paper are based on equations 10 and 11.",
        "md": "The normalization by Norm can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold.\n\nOur experimental results suggest that the impact of h^T h_M is negligible. Therefore, all experiments in the paper are based on equations 10 and 11.",
        "bBox": {
          "x": 107,
          "y": 296,
          "w": 299.98,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 3,
        "value": "A.4 TIME COST PER STEP",
        "md": "### A.4 TIME COST PER STEP",
        "bBox": {
          "x": 108,
          "y": 430,
          "w": 89.07,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "The time cost per step for nGPT is approximately 80% higher with 4k context length, and 60% higher with 8k context length. This overhead is not only due to nGPT having 6 normalization steps (2 of them are applied for q and k) per layer instead of 2, but also because nGPT's normalizations are not yet fully optimized, unlike GPT, where normalization layers are fused with other operations. Training on larger networks is expected to further reduce this performance gap, as the number of layers (and thus the number of normalizations) increases only modestly with the number of network parameters. Appendix A.8 shows that we can remove the normalization of q and k with a minor negative impact on results.",
        "md": "The time cost per step for nGPT is approximately 80% higher with 4k context length, and 60% higher with 8k context length. This overhead is not only due to nGPT having 6 normalization steps (2 of them are applied for q and k) per layer instead of 2, but also because nGPT's normalizations are not yet fully optimized, unlike GPT, where normalization layers are fused with other operations. Training on larger networks is expected to further reduce this performance gap, as the number of layers (and thus the number of normalizations) increases only modestly with the number of network parameters. Appendix A.8 shows that we can remove the normalization of q and k with a minor negative impact on results.",
        "bBox": {
          "x": 108,
          "y": 258,
          "w": 396.21,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 3,
        "value": "A.5 EXPERIMENTAL SETUP",
        "md": "### A.5 EXPERIMENTAL SETUP",
        "bBox": {
          "x": 108,
          "y": 554,
          "w": 96.75,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "In all experiments, we use OpenWebText (Gokaslan & Cohen, 2019) dataset which, according to experiments of Karpathy (2023), represents a good approximation of the OpenAI's internal dataset used to train GPT-2 models. We are well aware that this dataset is not of the highest quality. However, we believe that it is suitable for academic research, and, should improve the comparability of our findings with other research papers.\n\nWe trained our models using 64 A100 GPUs distributed across 8 nodes (8 GPUs per node). Global batch size is 512. We use the LLaMA-2 tokenizer with 32k tokens. We use the same setup for the 0.5B and 1.0B parameter models.\n\nAll matrix parameters are initialized by sampling from a zero-mean normal distribution with a standard deviation of 0.02 for GPT and 1/√d_model for nGPT. The standard deviation for the output matrices was scaled by a factor of √2 × n_layer, as suggested by Radford et al. (2018). The initialization of matrix parameters is not important for nGPT because they are normalized afterwards. The base of RoPE is 10000. The initialization of the additional parameters introduced in nGPT is described in Section 2.6.",
        "md": "In all experiments, we use OpenWebText (Gokaslan & Cohen, 2019) dataset which, according to experiments of Karpathy (2023), represents a good approximation of the OpenAI's internal dataset used to train GPT-2 models. We are well aware that this dataset is not of the highest quality. However, we believe that it is suitable for academic research, and, should improve the comparability of our findings with other research papers.\n\nWe trained our models using 64 A100 GPUs distributed across 8 nodes (8 GPUs per node). Global batch size is 512. We use the LLaMA-2 tokenizer with 32k tokens. We use the same setup for the 0.5B and 1.0B parameter models.\n\nAll matrix parameters are initialized by sampling from a zero-mean normal distribution with a standard deviation of 0.02 for GPT and 1/√d_model for nGPT. The standard deviation for the output matrices was scaled by a factor of √2 × n_layer, as suggested by Radford et al. (2018). The initialization of matrix parameters is not important for nGPT because they are normalized afterwards. The base of RoPE is 10000. The initialization of the additional parameters introduced in nGPT is described in Section 2.6.",
        "bBox": {
          "x": 108,
          "y": 158,
          "w": 396.21,
          "h": 11.04
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": "negative impact on results."
      },
      {
        "text": "experiments of Karpathy (2023), represents a good approximation of the OpenAI’s internal dataset"
      },
      {
        "text": "experiments of Karpathy (2023), represents a good approximation of the OpenAI’s internal dataset"
      },
      {
        "text": "used to train GPT-2 models. We are well aware that this dataset is not of the highest quality. How-"
      },
      {
        "text": "used to train GPT-2 models. We are well aware that this dataset is not of the highest quality. How-"
      },
      {
        "text": ""
      },
      {
        "text": ""
      },
      {
        "text": ""
      }
    ]
  },
  {
    "page": 14,
    "text": "Normalized Transformer\n\n\n\nA.6        SELECTION OF THE INITIAL LEARNING\n\n\n\nThe initial learning rate is the only hyperparameter we tune for both GPT and nGPT. Figure 7\ndemonstrates our trials to select the most suitable initial learning rate for GPT and nGPT. Our first\nexperiments started with the 0.5B model and 1k context length. After observing the general trend of\nthese curves, we reduced the number of experiments and followed the trends to minimize the totalFinal validation lossFinal validation losscompute used. Apart from estimating the optimal settings for the initial learning rate, our general\nobservation is that the optimal learning rates for GPT and nGPT tend to be similar for the same\nvalues of the validation loss. Longer runs are usually associated with the possibility of achieving\nlower values of the validation loss, and this typically requires lower initial learning rates.\nOne artifact we observed is the increasing sensitivity of the 1B nGPT model on 8k context length.\nWe found that the smoothness of the hyperparameter curves can be restored (see the dotted lines with√dmodel √dmodel)/0.1 ≈ 3.\nsquares) by increasing αA,init and αM,init from their default value of 1/                                                        to 0.1. This change\ndecreases the effective learning rate of Adam on these variables by a factor of (1/\nAs a result, the eigen learning rates are learned by Adam at a slower rate.Final validation lossFinal validation loss\n\n\n\n                   2.45       0.5B with 1K context           2.45        0.5B with 4K context          2.45        0.5B with 8K context\n\n\n\n                                                                                                               25k\n                   2.4                                        2.4                                       2.4    50k\n                                                                                                               100k\n                   2.35                                      2.35                                      2.35    3k\n                                                                                                               4k\n                   2.3                                        2.3                                       2.3    6k\n                   2.25                                      2.25                                      2.25    10k\n                                                     50k             13k                                       25k\n                   2.2                               100k     2.2    25k                                2.2    50k\n                                                     200k            50k                                       100k\n                   2.15                              400k    2.15    100k                              2.15\n                                                     800k            6k\n                   2.1                               25k      2.1    13k                                2.1\n                                                     50k             25k\n                   2.05                              100k    2.05    50k                               2.05Final validation lossFinal validation loss\n                                                     200k            100k\n                    2                                         2                                          2\n                        1           1.5      2     2.5   3        1       1.5   2    2.5 3 3.5 4 4.5 5      1            2       3     4   5 6\n                                Initial learning rate  10-3                Initial learning rate  10-3               Initial learning rate   10-3\n                   2.45        1B with 1K context            2.45         1B with 4K context           2.45         1B with 8K context\n                                                                     25k                                       25k\n                   2.4                                        2.4    50k                                2.4    50k\n                                                                     100k                                      100k\n                   2.35                                      2.35    200k                              2.35    200k\n                                                                     6k                                        6k\n                   2.3                                        2.3    13k                                2.3    10k\n                   2.25                              50k     2.25    20k                               2.25    13k\n                                                     100k            25k                                       25k\n                   2.2                               200k     2.2    50k                                2.2    50k\n                                                     400k            100k                                      100k\n                   2.15                              800k    2.15    200k                              2.15    25k Ainit01Minit01\n                                                     13k                                                       50k Ainit01Minit01\n                   2.1                               25k      2.1                                       2.1\n                                                     50k\n                   2.05                              100k    2.05                                      2.05\n                                                     200k\n                    2                                         2                                          2\n                        1           1.5      2     2.5   3        1       1.5   2    2.5 3 3.5 4 4.5 5      1       1.5    2   2.5 3 3.5 4 4.5 5\n                                Initial learning rate  10-3                Initial learning rate  10-3               Initial learning rate   10-3\n\n\n\nFigure 7: Final validation loss values for different initial learning rates for the 0.5B models (Top)\nand 1B models (Bottom). GPT is denoted by solid lines with circles, while nGPT is represented by\nthe dotted lines with stars. A specific case with a different setup, denoted by the dotted lines with\nsquares (Bottom Right), is discussed in the text.\n\n\n\n                                                                              14",
    "md": "# Normalized Transformer\n\n## A.6 SELECTION OF THE INITIAL LEARNING\n\nThe initial learning rate is the only hyperparameter we tune for both GPT and nGPT. Figure 7 demonstrates our trials to select the most suitable initial learning rate for GPT and nGPT. Our first experiments started with the 0.5B model and 1k context length. After observing the general trend of these curves, we reduced the number of experiments and followed the trends to minimize the total compute used. Apart from estimating the optimal settings for the initial learning rate, our general observation is that the optimal learning rates for GPT and nGPT tend to be similar for the same values of the validation loss. Longer runs are usually associated with the possibility of achieving lower values of the validation loss, and this typically requires lower initial learning rates.\n\nOne artifact we observed is the increasing sensitivity of the 1B nGPT model on 8k context length. We found that the smoothness of the hyperparameter curves can be restored (see the dotted lines with squares) by increasing αA,init and αM,init from their default value of 1/√dmodel to 0.1. This change decreases the effective learning rate of Adam on these variables by a factor of (1/√dmodel)/0.1 ≈ 3. As a result, the eigen learning rates are learned by Adam at a slower rate.\n\nFigure 7: Final validation loss values for different initial learning rates for the 0.5B models (Top) and 1B models (Bottom). GPT is denoted by solid lines with circles, while nGPT is represented by the dotted lines with stars. A specific case with a different setup, denoted by the dotted lines with squares (Bottom Right), is discussed in the text.\n\n| Model | Context | Initial Learning Rate (x10^-3) | Final Validation Loss |\n|-------|---------|-------------------------------|------------------------|\n| 0.5B  | 1K      | 1.0 - 3.0                     | 2.0 - 2.45             |\n| 0.5B  | 4K      | 1.0 - 5.0                     | 2.0 - 2.45             |\n| 0.5B  | 8K      | 1.0 - 6.0                     | 2.0 - 2.45             |\n| 1B    | 1K      | 1.0 - 3.0                     | 2.0 - 2.45             |\n| 1B    | 4K      | 1.0 - 5.0                     | 2.0 - 2.45             |\n| 1B    | 8K      | 1.0 - 5.0                     | 2.0 - 2.45             |\n\nNote: The table above is a general representation of the data shown in the graphs. The actual values may vary slightly.",
    "images": [
      {
        "name": "page_14.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "A.6 SELECTION OF THE INITIAL LEARNING",
        "md": "## A.6 SELECTION OF THE INITIAL LEARNING",
        "bBox": {
          "x": 108,
          "y": 92,
          "w": 163.79,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "The initial learning rate is the only hyperparameter we tune for both GPT and nGPT. Figure 7 demonstrates our trials to select the most suitable initial learning rate for GPT and nGPT. Our first experiments started with the 0.5B model and 1k context length. After observing the general trend of these curves, we reduced the number of experiments and followed the trends to minimize the total compute used. Apart from estimating the optimal settings for the initial learning rate, our general observation is that the optimal learning rates for GPT and nGPT tend to be similar for the same values of the validation loss. Longer runs are usually associated with the possibility of achieving lower values of the validation loss, and this typically requires lower initial learning rates.\n\nOne artifact we observed is the increasing sensitivity of the 1B nGPT model on 8k context length. We found that the smoothness of the hyperparameter curves can be restored (see the dotted lines with squares) by increasing αA,init and αM,init from their default value of 1/√dmodel to 0.1. This change decreases the effective learning rate of Adam on these variables by a factor of (1/√dmodel)/0.1 ≈ 3. As a result, the eigen learning rates are learned by Adam at a slower rate.\n\nFigure 7: Final validation loss values for different initial learning rates for the 0.5B models (Top) and 1B models (Bottom). GPT is denoted by solid lines with circles, while nGPT is represented by the dotted lines with stars. A specific case with a different setup, denoted by the dotted lines with squares (Bottom Right), is discussed in the text.",
        "md": "The initial learning rate is the only hyperparameter we tune for both GPT and nGPT. Figure 7 demonstrates our trials to select the most suitable initial learning rate for GPT and nGPT. Our first experiments started with the 0.5B model and 1k context length. After observing the general trend of these curves, we reduced the number of experiments and followed the trends to minimize the total compute used. Apart from estimating the optimal settings for the initial learning rate, our general observation is that the optimal learning rates for GPT and nGPT tend to be similar for the same values of the validation loss. Longer runs are usually associated with the possibility of achieving lower values of the validation loss, and this typically requires lower initial learning rates.\n\nOne artifact we observed is the increasing sensitivity of the 1B nGPT model on 8k context length. We found that the smoothness of the hyperparameter curves can be restored (see the dotted lines with squares) by increasing αA,init and αM,init from their default value of 1/√dmodel to 0.1. This change decreases the effective learning rate of Adam on these variables by a factor of (1/√dmodel)/0.1 ≈ 3. As a result, the eigen learning rates are learned by Adam at a slower rate.\n\nFigure 7: Final validation loss values for different initial learning rates for the 0.5B models (Top) and 1B models (Bottom). GPT is denoted by solid lines with circles, while nGPT is represented by the dotted lines with stars. A specific case with a different setup, denoted by the dotted lines with squares (Bottom Right), is discussed in the text.",
        "bBox": {
          "x": 107,
          "y": 113,
          "w": 397.25,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Model",
            "Context",
            "Initial Learning Rate (x10^-3)",
            "Final Validation Loss"
          ],
          [
            "0.5B",
            "1K",
            "1.0 - 3.0",
            "2.0 - 2.45"
          ],
          [
            "0.5B",
            "4K",
            "1.0 - 5.0",
            "2.0 - 2.45"
          ],
          [
            "0.5B",
            "8K",
            "1.0 - 6.0",
            "2.0 - 2.45"
          ],
          [
            "1B",
            "1K",
            "1.0 - 3.0",
            "2.0 - 2.45"
          ],
          [
            "1B",
            "4K",
            "1.0 - 5.0",
            "2.0 - 2.45"
          ],
          [
            "1B",
            "8K",
            "1.0 - 5.0",
            "2.0 - 2.45"
          ]
        ],
        "md": "| Model | Context | Initial Learning Rate (x10^-3) | Final Validation Loss |\n|-------|---------|-------------------------------|------------------------|\n| 0.5B  | 1K      | 1.0 - 3.0                     | 2.0 - 2.45             |\n| 0.5B  | 4K      | 1.0 - 5.0                     | 2.0 - 2.45             |\n| 0.5B  | 8K      | 1.0 - 6.0                     | 2.0 - 2.45             |\n| 1B    | 1K      | 1.0 - 3.0                     | 2.0 - 2.45             |\n| 1B    | 4K      | 1.0 - 5.0                     | 2.0 - 2.45             |\n| 1B    | 8K      | 1.0 - 5.0                     | 2.0 - 2.45             |",
        "isPerfectTable": true,
        "csv": "\"Model\",\"Context\",\"Initial Learning Rate (x10^-3)\",\"Final Validation Loss\"\n\"0.5B\",\"1K\",\"1.0 - 3.0\",\"2.0 - 2.45\"\n\"0.5B\",\"4K\",\"1.0 - 5.0\",\"2.0 - 2.45\"\n\"0.5B\",\"8K\",\"1.0 - 6.0\",\"2.0 - 2.45\"\n\"1B\",\"1K\",\"1.0 - 3.0\",\"2.0 - 2.45\"\n\"1B\",\"4K\",\"1.0 - 5.0\",\"2.0 - 2.45\"\n\"1B\",\"8K\",\"1.0 - 5.0\",\"2.0 - 2.45\"",
        "bBox": {
          "x": 153,
          "y": 151,
          "w": 54.2,
          "h": 226.61
        }
      },
      {
        "type": "text",
        "value": "Note: The table above is a general representation of the data shown in the graphs. The actual values may vary slightly.",
        "md": "Note: The table above is a general representation of the data shown in the graphs. The actual values may vary slightly.",
        "bBox": {
          "x": 0,
          "y": 0,
          "w": 612,
          "h": 792
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": "demonstrates our trials to select the most suitable initial learning rate for GPT and nGPT. Our first"
      }
    ]
  },
  {
    "page": 15,
    "text": "Normalized Transformer\n\n\n\n                          0.6            1K context: arc easy                     0.56            1K context: hellaswag                     0.61          1K context: winogrande\n\n\n\n                                                                                  0.54                                                       0.6\n                         0.58\n\n\n\n                                                                                  0.52                                                      0.59\n                         0.56\n                                                                                   0.5                                                      0.58ValueValueValueValue\n                         0.54                                                     0.48                                                      0.57\n\n\n\n                         0.52                                                     0.46                                                      0.56\n\n\n\n                                                                                  0.44                                                      0.55\n                          0.5\n                                                                GPT 0.5B          0.42                                    GPT 0.5B          0.54                                   GPT 0.5B\n                         0.48                                   GPT 1B                                                    GPT 1B                                                   GPT 1B\n                                                                nGPT 0.5B          0.4                                    nGPT 0.5B         0.53                                   nGPT 0.5B\n                                                                nGPT 1B                                                   nGPT 1B                                                  nGPT 1B\n                         0.460       100       200       300       400       500  0.380        100       200       300       400       500  0.520       100        200       300       400      500\n                                       Training tokens in billions                               Training tokens in billions                              Training tokens in billions\n                         0.74            1K context: wsc273                        0.6 1K context: lambada openai seqlen1024                0.62            1K context: average\n\n\n\n                                                                                  0.58\n                         0.72                                                                                                                0.6\n\n\n\n                                                                                  0.56ValueValueValueValue0.7                               0.58\n                                                                                  0.54\n\n\n\n                         0.68                                                     0.52                                                      0.56\n\n\n\n                         0.66                                                      0.5                                                      0.54\n\n\n\n                                                                                  0.48\n                         0.64                                                                                                               0.52\n                                                                GPT 0.5B          0.46                                    GPT 0.5B                                                 GPT 0.5B\n                         0.62                                   GPT 1B                                                    GPT 1B             0.5                                   GPT 1B\n                                                                nGPT 0.5B         0.44                                    nGPT 0.5B                                                nGPT 0.5B\n                                                                nGPT 1B                                                   nGPT 1B                                                  nGPT 1B\n                          0.60       100       200       300       400       500  0.420        100       200       300       400       500  0.480       100        200       300       400      500\n                                       Training tokens in billions                               Training tokens in billions                              Training tokens in billions\n\n\n\nFigure 8: Models trained with 1k context length. Final performance (y-axis) on a set of downstreamValueValueValueValuetasks and their average value (Bottom-Right) for different computation budgets in tokens (x-axis).\n\n\n\n                         0.62            8K context: arc easy                      0.6            8K context: hellaswag                     0.62          8K context: winogrande\n\n\n\n                          0.6\n                                                                                                                                             0.6\n                                                                                  0.55\n                         0.58\n\n\n\n                                                                                                                                            0.58\n                         0.56                                                      0.5\n\n\n\n                         0.54                                                                                                               0.56\n\n\n\n                         0.52                                                     0.45\n                                                                                                                                            0.54\n\n\n\n                          0.5                                   GPT 0.5B           0.4                                    GPT 0.5B                                                 GPT 0.5B\n                                                                GPT 1B                                                    GPT 1B            0.52                                   GPT 1B\n                         0.48                                   nGPT 0.5B                                                 nGPT 0.5B                                                nGPT 0.5B\n                                                                nGPT 1B                                                   nGPT 1B                                                  nGPT 1B\n                         0.460       200       400       600       800       1000 0.350        200       400       600       800      1000   0.50       200        400       600       800      1000\n                                       Training tokens in billions                               Training tokens in billions                              Training tokens in billions\n                         0.74            8K context: wsc273                       0.58 8K context: lambada openai seqlen1024                0.62            8K context: average\n\n\n\n                         0.72                                                     0.56                                                       0.6\n\n\n\n                          0.7                                                     0.54\n                                                                                                                                            0.58\n                         0.68                                                     0.52\n\n\n\n                                                                                                                                            0.56\n                         0.66                                                      0.5\n\n\n\n                         0.64                                                     0.48                                                      0.54\n\n\n\n                         0.62                                                     0.46                                                      0.52\n\n\n\n                          0.6                                                     0.44\n                                                                                                                                             0.5\n                         0.58                                   GPT 0.5B          0.42                                    GPT 0.5B                                                 GPT 0.5B\n                                                                GPT 1B                                                    GPT 1B                                                   GPT 1B\n                                                                nGPT 0.5B                                                 nGPT 0.5B         0.48                                   nGPT 0.5B\n                         0.56                                                      0.4\n                                                                nGPT 1B                                                   nGPT 1B                                                  nGPT 1B\n                         0.540       200       400       600       800       1000 0.380        200       400       600       800      1000  0.460       200        400       600       800      1000\n                                       Training tokens in billions                               Training tokens in billions                              Training tokens in billions\n\n\n\nFigure 9: Models trained with 8k context length. Final performance (y-axis) on a set of downstream\ntasks and their average value (Bottom-Right) for different computation budgets in tokens (x-axis).\n\n\n\n                                                                                                          15",
    "md": "# Normalized Transformer\n\n## Figure 8: Models trained with 1k context length. Final performance (y-axis) on a set of downstream tasks and their average value (Bottom-Right) for different computation budgets in tokens (x-axis).\n\n| Task | Graph |\n|------|-------|\n| 1K context: arc easy | Graph 1 |\n| 1K context: hellaswag | Graph 2 |\n| 1K context: winogrande | Graph 3 |\n| 1K context: wsc273 | Graph 4 |\n| 1K context: lambada openai seqlen1024 | Graph 5 |\n| 1K context: average | Graph 6 |\n\nEach graph shows the performance of four models (GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B) across different training token amounts (0-500 billion). The y-axis represents the performance metric, while the x-axis shows the number of training tokens in billions.\n\n## Figure 9: Models trained with 8k context length. Final performance (y-axis) on a set of downstream tasks and their average value (Bottom-Right) for different computation budgets in tokens (x-axis).\n\n| Task | Graph |\n|------|-------|\n| 8K context: arc easy | Graph 7 |\n| 8K context: hellaswag | Graph 8 |\n| 8K context: winogrande | Graph 9 |\n| 8K context: wsc273 | Graph 10 |\n| 8K context: lambada openai seqlen1024 | Graph 11 |\n| 8K context: average | Graph 12 |\n\nSimilar to Figure 8, each graph in Figure 9 shows the performance of four models (GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B) across different training token amounts (0-1000 billion). The y-axis represents the performance metric, while the x-axis shows the number of training tokens in billions.\n\nIn both figures, the graphs illustrate how the performance of different model sizes and architectures (GPT vs. nGPT) changes as the number of training tokens increases for various natural language processing tasks.",
    "images": [
      {
        "name": "page_15.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "Figure 8: Models trained with 1k context length. Final performance (y-axis) on a set of downstream tasks and their average value (Bottom-Right) for different computation budgets in tokens (x-axis).",
        "md": "## Figure 8: Models trained with 1k context length. Final performance (y-axis) on a set of downstream tasks and their average value (Bottom-Right) for different computation budgets in tokens (x-axis).",
        "bBox": {
          "x": null,
          "y": 151,
          "w": 396.19,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Task",
            "Graph"
          ],
          [
            "1K context: arc easy",
            "Graph 1"
          ],
          [
            "1K context: hellaswag",
            "Graph 2"
          ],
          [
            "1K context: winogrande",
            "Graph 3"
          ],
          [
            "1K context: wsc273",
            "Graph 4"
          ],
          [
            "1K context: lambada openai seqlen1024",
            "Graph 5"
          ],
          [
            "1K context: average",
            "Graph 6"
          ]
        ],
        "md": "| Task | Graph |\n|------|-------|\n| 1K context: arc easy | Graph 1 |\n| 1K context: hellaswag | Graph 2 |\n| 1K context: winogrande | Graph 3 |\n| 1K context: wsc273 | Graph 4 |\n| 1K context: lambada openai seqlen1024 | Graph 5 |\n| 1K context: average | Graph 6 |",
        "isPerfectTable": true,
        "csv": "\"Task\",\"Graph\"\n\"1K context: arc easy\",\"Graph 1\"\n\"1K context: hellaswag\",\"Graph 2\"\n\"1K context: winogrande\",\"Graph 3\"\n\"1K context: wsc273\",\"Graph 4\"\n\"1K context: lambada openai seqlen1024\",\"Graph 5\"\n\"1K context: average\",\"Graph 6\"",
        "bBox": {
          "x": 182,
          "y": 114,
          "w": 88.61,
          "h": 4.61
        }
      },
      {
        "type": "text",
        "value": "Each graph shows the performance of four models (GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B) across different training token amounts (0-500 billion). The y-axis represents the performance metric, while the x-axis shows the number of training tokens in billions.",
        "md": "Each graph shows the performance of four models (GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B) across different training token amounts (0-500 billion). The y-axis represents the performance metric, while the x-axis shows the number of training tokens in billions.",
        "bBox": {
          "x": 155,
          "y": 149,
          "w": 108,
          "h": 4.61
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "Figure 9: Models trained with 8k context length. Final performance (y-axis) on a set of downstream tasks and their average value (Bottom-Right) for different computation budgets in tokens (x-axis).",
        "md": "## Figure 9: Models trained with 8k context length. Final performance (y-axis) on a set of downstream tasks and their average value (Bottom-Right) for different computation budgets in tokens (x-axis).",
        "bBox": {
          "x": null,
          "y": 151,
          "w": 396.19,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Task",
            "Graph"
          ],
          [
            "8K context: arc easy",
            "Graph 7"
          ],
          [
            "8K context: hellaswag",
            "Graph 8"
          ],
          [
            "8K context: winogrande",
            "Graph 9"
          ],
          [
            "8K context: wsc273",
            "Graph 10"
          ],
          [
            "8K context: lambada openai seqlen1024",
            "Graph 11"
          ],
          [
            "8K context: average",
            "Graph 12"
          ]
        ],
        "md": "| Task | Graph |\n|------|-------|\n| 8K context: arc easy | Graph 7 |\n| 8K context: hellaswag | Graph 8 |\n| 8K context: winogrande | Graph 9 |\n| 8K context: wsc273 | Graph 10 |\n| 8K context: lambada openai seqlen1024 | Graph 11 |\n| 8K context: average | Graph 12 |",
        "isPerfectTable": true,
        "csv": "\"Task\",\"Graph\"\n\"8K context: arc easy\",\"Graph 7\"\n\"8K context: hellaswag\",\"Graph 8\"\n\"8K context: winogrande\",\"Graph 9\"\n\"8K context: wsc273\",\"Graph 10\"\n\"8K context: lambada openai seqlen1024\",\"Graph 11\"\n\"8K context: average\",\"Graph 12\"",
        "bBox": {
          "x": 182,
          "y": 442,
          "w": 88.61,
          "h": 4.61
        }
      },
      {
        "type": "text",
        "value": "Similar to Figure 8, each graph in Figure 9 shows the performance of four models (GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B) across different training token amounts (0-1000 billion). The y-axis represents the performance metric, while the x-axis shows the number of training tokens in billions.\n\nIn both figures, the graphs illustrate how the performance of different model sizes and architectures (GPT vs. nGPT) changes as the number of training tokens increases for various natural language processing tasks.",
        "md": "Similar to Figure 8, each graph in Figure 9 shows the performance of four models (GPT 0.5B, GPT 1B, nGPT 0.5B, nGPT 1B) across different training token amounts (0-1000 billion). The y-axis represents the performance metric, while the x-axis shows the number of training tokens in billions.\n\nIn both figures, the graphs illustrate how the performance of different model sizes and architectures (GPT vs. nGPT) changes as the number of training tokens increases for various natural language processing tasks.",
        "bBox": {
          "x": 155,
          "y": 149,
          "w": 108,
          "h": 4.61
        }
      }
    ],
    "status": "OK",
    "links": []
  },
  {
    "page": 16,
    "text": "                                        Normalized Transformer\n\n\n\n                                                                  107   Condition number of Q at each layer               107    Condition number of K at each layer                107   Condition number of V at each layer\n                                                                                                         GPT 0.5B                                                  GPT 0.5B                                                 GPT 0.5B\n                                                                  106                                    GPT 0.5B         106                                      GPT 0.5B         106                                     GPT 0.5B\n                                                                                                         nGPT 0.5B                                                 nGPT 0.5B                                                nGPT 0.5B\n                                                                                                         GPT 1B                                                    GPT 1B                                                   GPT 1B\n                                                                  105                                    GPT 1B           105                                      GPT 1B           105                                     GPT 1B\n                                                                                                         nGPT 1B                                                   nGPT 1B                                                  nGPT 1B\nCondition numberCondition number104                                                                                       104                                                       104\n\n\n\n                                                                  103                                                     103                                                       103\n\n\n\n                                                                  102                                                     102                                                       102\nNorm valueNorm value101                                                                                                   101                                                       101\n\n\n\n                                                                  1000                                                    1000                                                      1000\n                                                                              0.2      0.4       0.6       0.8       1                0.2       0.4       0.6       0.8        1                0.2       0.4      0.6       0.8        1\n                                                                                Normalized layer index                                    Normalized layer index                                   Normalized layer index\n                                                                  107Condition number of MLP11 at each layer              107Condition number of MLP12 at each layer                107Condition number of MLP2 at each layer\n                                                                                                         GPT 0.5B                                                  GPT 0.5B                                                 GPT 0.5B\n                                                                  106                                    GPT 0.5B         106                                      GPT 0.5B         106                                     GPT 0.5B\n                                                                                                         nGPT 0.5B                                                 nGPT 0.5B                                                nGPT 0.5B\n                                                                                                         GPT 1B                                                    GPT 1B                                                   GPT 1B\nCondition numberCondition numberGPT 1B                                                                                                                             GPT 1B                                                   GPT 1B\n                                                                  105                                    nGPT 1B          105                                      nGPT 1B          105                                     nGPT 1B\n\n\n\n                                                                  104                                                     104                                                       104\n\n\n\n                                                                  103                                                     103                                                       103\n\n\n\n                                                                  102                                                     102                                                       102\nSingular ValueSingular Value101                                                                                           101                                                       101\n\n\n\n                                                                  1000                                                    1000                                                      1000\n                                                                              0.2      0.4       0.6       0.8       1                0.2       0.4       0.6       0.8        1                0.2       0.4      0.6       0.8        1\n                                                                                Normalized layer index                                    Normalized layer index                                   Normalized layer index\n\n\n\n                                        Figure 10:                 Median condition numbers measured for attention and MLP matrices at different layerCondition numberCondition numberdepth (24 and 36 layers for 0.5B and 1B networks, respectively). The dotted lines are for the case\n                                        when GPT’s matrices are renormalized after training. Models are trained for 100k iterations.\n\n\n\n                                                                                        1.2             Distribution of norms of Q                      101      Sorted Singular Values for heads of Q\n\n\n\n                                                                                                                                                        100\n                                                                                          1\n                                                                                                                                       GPT 0.5B        10-1\n                                                                                                                                       GPT 1B\n                                                                                        0.8                                            nGPT 0.5B       10-2\n                                                                                                                                       nGPT 1B\n                                                                                                                                                       10-3\n                                                                                        0.6\n                                                                                                                                                       10-4\n\n\n\n                                                                                        0.4                                                            10-5\n\n\n\n                                                                                                                                                       10-6      GPT 0.5B cond=1625822.6972\n                                                                                        0.2                                                                      GPT 1B cond=171580.2367\n                                                                                                                                                       10-7      nGPT 0.5B cond=3.7593\n                                                                                                                                                                 nGPT 1B cond=13.0955\n                                                                                          00    0.1  0.2   0.3  0.4  0.5   0.6  0.7   0.8  0.9   1     10-80   0.1  0.2   0.3  0.4   0.5  0.6   0.7  0.8   0.9   1\n\n\n\n                                                                                                              Normalized rank                                                 Normalized rank\n                                                                                        1.2             Distribution of norms of K                      101       Sorted Singular Values for heads of K\n\n\n\n                                                                                                                                                        100\n                                                                                          1\n                                                                                                                                      GPT 0.5B         10-1\n                                                                                                                                      GPT 1B\n                                                                                        0.8                                           nGPT 0.5B        10-2\n                                                                                                                                      nGPT 1B\n                                                                                                                                                       10-3\n                                                                                        0.6\n                                                                                                                                                       10-4\n\n\n\n                                                                                        0.4                                                            10-5\n\n\n\n                                                                                                                                                       10-6\n                                                                                        0.2                                                                      GPT 0.5B cond=1878718.0663\n                                                                                                                                                                 GPT 1B cond=122846.4196\n                                                                                                                                                       10-7      nGPT 0.5B cond=4.4978\n                                                                                                                                                                 nGPT 1B cond=9.8658\n                                                                                          0\n                                                                                           0    0.1  0.2   0.3  0.4  0.5   0.6  0.7   0.8  0.9   1     10-80   0.1  0.2   0.3  0.4   0.5  0.6   0.7  0.8   0.9   1\n                                                                                                              Normalized rank                                                 Normalized rank\n\n\n\n                                        Figure 11:                Left: Distribution of the norms of vectors forming the complete attention matrices (not\n                                        per head). Right: Distribution of singular values for all heads. The results are shown for the 3rd\n                                        layer.\n\n\n\n                                                                                                                                                 16",
    "md": "# Normalized Transformer\n\n## Figure 10: Median condition numbers measured for attention and MLP matrices at different layer depth (24 and 36 layers for 0.5B and 1B networks, respectively). The dotted lines are for the case when GPT's matrices are renormalized after training. Models are trained for 100k iterations.\n\n| Condition number of Q at each layer | Condition number of K at each layer | Condition number of V at each layer |\n|-------------------------------------|-------------------------------------|-------------------------------------|\n| Q condition number  | K condition number  | V condition number  |\n\n| Condition number of MLP11 at each layer | Condition number of MLP12 at each layer | Condition number of MLP2 at each layer |\n|----------------------------------------|----------------------------------------|---------------------------------------|\n| MLP11 condition number | MLP12 condition number | MLP2 condition number |\n\nLegend:\n- GPT 0.5B\n- GPT 0.5B (renormalized)\n- nGPT 0.5B\n- GPT 1B\n- GPT 1B (renormalized)\n- nGPT 1B\n\nX-axis: Normalized layer index\nY-axis: Condition number\n\n## Figure 11: Left: Distribution of the norms of vectors forming the complete attention matrices (not per head). Right: Distribution of singular values for all heads. The results are shown for the 3rd layer.\n\n| Distribution of norms of Q | Sorted Singular Values for heads of Q |\n|----------------------------|--------------------------------------|\n| Q norms    | Q singular values    |\n\n| Distribution of norms of K | Sorted Singular Values for heads of K |\n|----------------------------|--------------------------------------|\n| K norms    | K singular values    |\n\nLegend:\n- GPT 0.5B\n- GPT 1B\n- nGPT 0.5B\n- nGPT 1B\n\nX-axis: Normalized rank\nY-axis (left): Norm value\nY-axis (right): Singular Value\n\nCondition numbers:\n- GPT 0.5B: 1625822.6972 (Q), 1878718.0663 (K)\n- GPT 1B: 171580.2367 (Q), 122846.4196 (K)\n- nGPT 0.5B: 3.7593 (Q), 4.4978 (K)\n- nGPT 1B: 13.0955 (Q), 9.8658 (K)\n\n16",
    "images": [
      {
        "name": "page_16.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "Figure 10: Median condition numbers measured for attention and MLP matrices at different layer depth (24 and 36 layers for 0.5B and 1B networks, respectively). The dotted lines are for the case when GPT's matrices are renormalized after training. Models are trained for 100k iterations.",
        "md": "## Figure 10: Median condition numbers measured for attention and MLP matrices at different layer depth (24 and 36 layers for 0.5B and 1B networks, respectively). The dotted lines are for the case when GPT's matrices are renormalized after training. Models are trained for 100k iterations.",
        "bBox": {
          "x": null,
          "y": 152,
          "w": 395.98,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Condition number of Q at each layer",
            "Condition number of K at each layer",
            "Condition number of V at each layer"
          ],
          [
            "Q condition number",
            "K condition number",
            "V condition number"
          ]
        ],
        "md": "| Condition number of Q at each layer | Condition number of K at each layer | Condition number of V at each layer |\n|-------------------------------------|-------------------------------------|-------------------------------------|\n| Q condition number  | K condition number  | V condition number  |",
        "isPerfectTable": true,
        "csv": "\"Condition number of Q at each layer\",\"Condition number of K at each layer\",\"Condition number of V at each layer\"\n\"Q condition number\",\"K condition number\",\"V condition number\"",
        "bBox": {
          "x": 165,
          "y": 113,
          "w": 79.38,
          "h": 4.61
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Condition number of MLP11 at each layer",
            "Condition number of MLP12 at each layer",
            "Condition number of MLP2 at each layer"
          ],
          [
            "MLP11 condition number",
            "MLP12 condition number",
            "MLP2 condition number"
          ]
        ],
        "md": "| Condition number of MLP11 at each layer | Condition number of MLP12 at each layer | Condition number of MLP2 at each layer |\n|----------------------------------------|----------------------------------------|---------------------------------------|\n| MLP11 condition number | MLP12 condition number | MLP2 condition number |",
        "isPerfectTable": true,
        "csv": "\"Condition number of MLP11 at each layer\",\"Condition number of MLP12 at each layer\",\"Condition number of MLP2 at each layer\"\n\"MLP11 condition number\",\"MLP12 condition number\",\"MLP2 condition number\"",
        "bBox": {
          "x": null,
          "y": 152,
          "w": 88.08,
          "h": 4.61
        }
      },
      {
        "type": "text",
        "value": "Legend:\n- GPT 0.5B\n- GPT 0.5B (renormalized)\n- nGPT 0.5B\n- GPT 1B\n- GPT 1B (renormalized)\n- nGPT 1B\n\nX-axis: Normalized layer index\nY-axis: Condition number",
        "md": "Legend:\n- GPT 0.5B\n- GPT 0.5B (renormalized)\n- nGPT 0.5B\n- GPT 1B\n- GPT 1B (renormalized)\n- nGPT 1B\n\nX-axis: Normalized layer index\nY-axis: Condition number",
        "bBox": {
          "x": 157,
          "y": 122,
          "w": 106.37,
          "h": 4.61
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "Figure 11: Left: Distribution of the norms of vectors forming the complete attention matrices (not per head). Right: Distribution of singular values for all heads. The results are shown for the 3rd layer.",
        "md": "## Figure 11: Left: Distribution of the norms of vectors forming the complete attention matrices (not per head). Right: Distribution of singular values for all heads. The results are shown for the 3rd layer.",
        "bBox": {
          "x": 108,
          "y": 216,
          "w": 395.7,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Distribution of norms of Q",
            "Sorted Singular Values for heads of Q"
          ],
          [
            "Q norms",
            "Q singular values"
          ]
        ],
        "md": "| Distribution of norms of Q | Sorted Singular Values for heads of Q |\n|----------------------------|--------------------------------------|\n| Q norms    | Q singular values    |",
        "isPerfectTable": true,
        "csv": "\"Distribution of norms of Q\",\"Sorted Singular Values for heads of Q\"\n\"Q norms\",\"Q singular values\"",
        "bBox": {
          "x": 225,
          "y": 309,
          "w": 76.05,
          "h": 145.22
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Distribution of norms of K",
            "Sorted Singular Values for heads of K"
          ],
          [
            "K norms",
            "K singular values"
          ]
        ],
        "md": "| Distribution of norms of K | Sorted Singular Values for heads of K |\n|----------------------------|--------------------------------------|\n| K norms    | K singular values    |",
        "isPerfectTable": true,
        "csv": "\"Distribution of norms of K\",\"Sorted Singular Values for heads of K\"\n\"K norms\",\"K singular values\"",
        "bBox": {
          "x": 225,
          "y": 309,
          "w": 75.82,
          "h": 251.22
        }
      },
      {
        "type": "text",
        "value": "Legend:\n- GPT 0.5B\n- GPT 1B\n- nGPT 0.5B\n- nGPT 1B\n\nX-axis: Normalized rank\nY-axis (left): Norm value\nY-axis (right): Singular Value\n\nCondition numbers:\n- GPT 0.5B: 1625822.6972 (Q), 1878718.0663 (K)\n- GPT 1B: 171580.2367 (Q), 122846.4196 (K)\n- nGPT 0.5B: 3.7593 (Q), 4.4978 (K)\n- nGPT 1B: 13.0955 (Q), 9.8658 (K)\n\n16",
        "md": "Legend:\n- GPT 0.5B\n- GPT 1B\n- nGPT 0.5B\n- nGPT 1B\n\nX-axis: Normalized rank\nY-axis (left): Norm value\nY-axis (right): Singular Value\n\nCondition numbers:\n- GPT 0.5B: 1625822.6972 (Q), 1878718.0663 (K)\n- GPT 1B: 171580.2367 (Q), 122846.4196 (K)\n- nGPT 0.5B: 3.7593 (Q), 4.4978 (K)\n- nGPT 1B: 13.0955 (Q), 9.8658 (K)\n\n16",
        "bBox": {
          "x": 157,
          "y": 122,
          "w": 106.37,
          "h": 9.96
        }
      }
    ],
    "status": "OK",
    "links": []
  },
  {
    "page": 17,
    "text": "                                                                         Normalized Transformer\n\n\n\n                                                                                                                                      5                 Distribution of norms of Q                                 101     Sorted Singular Values for heads of Q\n                                                                                                                                                                                            GPT 0.5B\n                                                                                                                                    4.5                                                     GPT 1B                 100\n                                                                                                                                                                                            nGPT 0.5B\n                                                                                                                                      4                                                     nGPT 1B               10-1\n                                                                                                                                    3.5\n\n\n\n                                                                                                                                                                                                                  10-2\n                                                                                                                                      3\n\n\n\n                                                                                                                                    2.5                                                                           10-3\n\n\n\n                                                                                                                                      2                                                                           10-4\n\n\n\n                                                                                                                                    1.5\n                                                                                                                                                                                                                  10-5\n                                                                                                                                      1                                                                                   GPT 0.5B cond=1500.1872\n                                                                                                                                                                                                                  10-6    GPT 1B cond=807005.0369\n                                                                                                                                    0.5                                                                                   nGPT 0.5B cond=3.043\n                                                                                                                                                                                                                          nGPT 1B cond=4.1919\n                                                                                                                                      0\n                                                                                                                                       0     0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9                           1      10-70  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9                          1\n                                                                                                                                                               Normalized rank                                                             Normalized rank\n                                                                                                                                                        Distribution of norms of K                                 101      Sorted Singular Values for heads of KNorm valueNorm value4\n                                                                                                                                                                                            GPT 0.5B\n                                                                                                                                    3.5                                                     GPT 1B                 100\n                                                                                                                                                                                            nGPT 0.5BPPLnGPT 1B   10-1\n                                                                                                                                      3\n                                                                                                                                    2.5                                                                           10-2\n\n\n\n                                                                                                                                      2                                                                           10-3\n\n\n\n                                                                                                                                    1.5                                                                           10-4\n\n\n\n                                                                                                                                      1                                                                           10-5\n                                                                                                                                                                                                                           GPT 0.5B cond=1378.6081\n                                                                                                                                    0.5                                                                           10-6     GPT 1B cond=722934.6929\n                                                                                                                                                                                                                           nGPT 0.5B cond=2.8612\n                                                                                                                                                                                                                           nGPT 1B cond=3.8907\n                                                                                                                                      0                                                                           10-7\n                                                                                                                                       0     0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9                           1          0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9                          1\n                                                                                                                                                               Normalized rank                                                             Normalized rank\nSingular ValueSingular ValueFigure 12: Left: Distribution of the norms of vectors forming the complete attention matrices (not\n                                                                         per head). Right: Distribution of singular values for all heads. The results are shown for the 4th\n                                                                         layer.\n\n\n\n                                                                         A.7              LENGTH EXTRAPOLATION ABILITY\n\n\n\n                                                                         We investigate the length extrapolation ability of nGPT by evaluating its perplexity on the PG19\n                                                                         dataset, as shown in Figure 13. In standard GPTs, perplexity tends to increase dramatically when\n                                                                         tested on sequences longer than pre-training length. In contrast, nGPT maintains a stable perplexity\n                                                                         range even at extrapolated lengths. This result demonstrates that nGPT can handle longer contexts\n                                                                         without requiring any modifications to RoPE, providing a clear advantage over standard GPTs in\n                                                                         terms of length extrapolation.\n\n\n\n                                                                                                                                                 1200                                                PG19 Perplexity\n                                                                                                                                                                         1K GPT 1B\n                                                                                                                                                 1000                    4K GPT 1B\n                                                                                                                                                                         8K GPT 1B\n                                                                                                                                                                         1K nGPT 1B\n                                                                                                                                                   800                   4K nGPT 1B\n                                                                                                                                                                         8K nGPT 1B\n                                                                                                                                                   600\n                                                                                                                                                   400\n                                                                                                                                                   200\n                                                                                                                                                        0      1K                   2K                   4KLength          8K                 16K                  32K\n                                                                                                                                                  Figure 13: PG19 perplexity from 1K to 32K.\n\n\n\n                                                                                                                                                                                                           17",
    "md": "# Normalized Transformer\n\n## Distribution of norms of Q\n\n| Model    | Line Color |\n|----------|------------|\n| GPT 0.5B | Blue       |\n| GPT 1B   | Orange     |\n| nGPT 0.5B| Green      |\n| nGPT 1B  | Red        |\n\nThe graph shows the distribution of norms of Q for different models across normalized ranks from 0 to 1. The nGPT models show a more uniform distribution compared to the GPT models.\n\n## Sorted Singular Values for heads of Q\n\n| Model    | Line Color | Condition Number |\n|----------|------------|------------------|\n| GPT 0.5B | Blue       | 1500.1872        |\n| GPT 1B   | Orange     | 807005.0369      |\n| nGPT 0.5B| Green      | 3.043            |\n| nGPT 1B  | Red        | 4.1919           |\n\nThe graph displays sorted singular values for heads of Q on a logarithmic scale. The nGPT models show significantly lower condition numbers compared to the GPT models.\n\n## Distribution of norms of K\n\nSimilar to the distribution of Q, this graph shows the distribution of norms of K for the same models.\n\n## Sorted Singular Values for heads of K\n\n| Model    | Line Color | Condition Number |\n|----------|------------|------------------|\n| GPT 0.5B | Blue       | 1378.6081        |\n| GPT 1B   | Orange     | 722934.6929      |\n| nGPT 0.5B| Green      | 2.8612           |\n| nGPT 1B  | Red        | 3.8907           |\n\nThis graph is similar to the one for Q, showing sorted singular values for heads of K. Again, nGPT models demonstrate much lower condition numbers.\n\nFigure 12: Left: Distribution of the norms of vectors forming the complete attention matrices (not per head). Right: Distribution of singular values for all heads. The results are shown for the 4th layer.\n\n## A.7 LENGTH EXTRAPOLATION ABILITY\n\nWe investigate the length extrapolation ability of nGPT by evaluating its perplexity on the PG19 dataset, as shown in Figure 13. In standard GPTs, perplexity tends to increase dramatically when tested on sequences longer than pre-training length. In contrast, nGPT maintains a stable perplexity range even at extrapolated lengths. This result demonstrates that nGPT can handle longer contexts without requiring any modifications to RoPE, providing a clear advantage over standard GPTs in terms of length extrapolation.\n\n## PG19 Perplexity\n\n| Model     | Line Color |\n|-----------|------------|\n| 1K GPT 1B | Red        |\n| 4K GPT 1B | Orange     |\n| 8K GPT 1B | Yellow     |\n| 1K nGPT 1B| Green      |\n| 4K nGPT 1B| Blue       |\n| 8K nGPT 1B| Purple     |\n\nThe graph shows PG19 perplexity for different models from 1K to 32K sequence lengths. The GPT models show a dramatic increase in perplexity at longer lengths, while the nGPT models maintain relatively stable perplexity across all lengths.\n\nFigure 13: PG19 perplexity from 1K to 32K.\n\n17",
    "images": [
      {
        "name": "page_17.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "Distribution of norms of Q",
        "md": "## Distribution of norms of Q",
        "bBox": {
          "x": 225,
          "y": 87,
          "w": 52.57,
          "h": 4.22
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Model",
            "Line Color"
          ],
          [
            "GPT 0.5B",
            "Blue"
          ],
          [
            "GPT 1B",
            "Orange"
          ],
          [
            "nGPT 0.5B",
            "Green"
          ],
          [
            "nGPT 1B",
            "Red"
          ]
        ],
        "md": "| Model    | Line Color |\n|----------|------------|\n| GPT 0.5B | Blue       |\n| GPT 1B   | Orange     |\n| nGPT 0.5B| Green      |\n| nGPT 1B  | Red        |",
        "isPerfectTable": true,
        "csv": "\"Model\",\"Line Color\"\n\"GPT 0.5B\",\"Blue\"\n\"GPT 1B\",\"Orange\"\n\"nGPT 0.5B\",\"Green\"\n\"nGPT 1B\",\"Red\"",
        "bBox": {
          "x": 196,
          "y": 90,
          "w": 18.28,
          "h": 9.32
        }
      },
      {
        "type": "text",
        "value": "The graph shows the distribution of norms of Q for different models across normalized ranks from 0 to 1. The nGPT models show a more uniform distribution compared to the GPT models.",
        "md": "The graph shows the distribution of norms of Q for different models across normalized ranks from 0 to 1. The nGPT models show a more uniform distribution compared to the GPT models.",
        "bBox": {
          "x": 198,
          "y": 87,
          "w": 79.57,
          "h": 9.32
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "Sorted Singular Values for heads of Q",
        "md": "## Sorted Singular Values for heads of Q",
        "bBox": {
          "x": 329,
          "y": 87,
          "w": 76.05,
          "h": 4.22
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Model",
            "Line Color",
            "Condition Number"
          ],
          [
            "GPT 0.5B",
            "Blue",
            "1500.1872"
          ],
          [
            "GPT 1B",
            "Orange",
            "807005.0369"
          ],
          [
            "nGPT 0.5B",
            "Green",
            "3.043"
          ],
          [
            "nGPT 1B",
            "Red",
            "4.1919"
          ]
        ],
        "md": "| Model    | Line Color | Condition Number |\n|----------|------------|------------------|\n| GPT 0.5B | Blue       | 1500.1872        |\n| GPT 1B   | Orange     | 807005.0369      |\n| nGPT 0.5B| Green      | 3.043            |\n| nGPT 1B  | Red        | 4.1919           |",
        "isPerfectTable": true,
        "csv": "\"Model\",\"Line Color\",\"Condition Number\"\n\"GPT 0.5B\",\"Blue\",\"1500.1872\"\n\"GPT 1B\",\"Orange\",\"807005.0369\"\n\"nGPT 0.5B\",\"Green\",\"3.043\"\n\"nGPT 1B\",\"Red\",\"4.1919\"",
        "bBox": {
          "x": 196,
          "y": 90,
          "w": 18.28,
          "h": 9.32
        }
      },
      {
        "type": "text",
        "value": "The graph displays sorted singular values for heads of Q on a logarithmic scale. The nGPT models show significantly lower condition numbers compared to the GPT models.",
        "md": "The graph displays sorted singular values for heads of Q on a logarithmic scale. The nGPT models show significantly lower condition numbers compared to the GPT models.",
        "bBox": {
          "x": 329,
          "y": 87,
          "w": 76.05,
          "h": 4.22
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "Distribution of norms of K",
        "md": "## Distribution of norms of K",
        "bBox": {
          "x": 225,
          "y": 193,
          "w": 52.33,
          "h": 4.22
        }
      },
      {
        "type": "text",
        "value": "Similar to the distribution of Q, this graph shows the distribution of norms of K for the same models.",
        "md": "Similar to the distribution of Q, this graph shows the distribution of norms of K for the same models.",
        "bBox": {
          "x": 225,
          "y": 193,
          "w": 52.33,
          "h": 4.22
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "Sorted Singular Values for heads of K",
        "md": "## Sorted Singular Values for heads of K",
        "bBox": {
          "x": 330,
          "y": 193,
          "w": 75.82,
          "h": 4.22
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Model",
            "Line Color",
            "Condition Number"
          ],
          [
            "GPT 0.5B",
            "Blue",
            "1378.6081"
          ],
          [
            "GPT 1B",
            "Orange",
            "722934.6929"
          ],
          [
            "nGPT 0.5B",
            "Green",
            "2.8612"
          ],
          [
            "nGPT 1B",
            "Red",
            "3.8907"
          ]
        ],
        "md": "| Model    | Line Color | Condition Number |\n|----------|------------|------------------|\n| GPT 0.5B | Blue       | 1378.6081        |\n| GPT 1B   | Orange     | 722934.6929      |\n| nGPT 0.5B| Green      | 2.8612           |\n| nGPT 1B  | Red        | 3.8907           |",
        "isPerfectTable": true,
        "csv": "\"Model\",\"Line Color\",\"Condition Number\"\n\"GPT 0.5B\",\"Blue\",\"1378.6081\"\n\"GPT 1B\",\"Orange\",\"722934.6929\"\n\"nGPT 0.5B\",\"Green\",\"2.8612\"\n\"nGPT 1B\",\"Red\",\"3.8907\"",
        "bBox": {
          "x": 196,
          "y": 90,
          "w": 18.28,
          "h": 9.32
        }
      },
      {
        "type": "text",
        "value": "This graph is similar to the one for Q, showing sorted singular values for heads of K. Again, nGPT models demonstrate much lower condition numbers.\n\nFigure 12: Left: Distribution of the norms of vectors forming the complete attention matrices (not per head). Right: Distribution of singular values for all heads. The results are shown for the 4th layer.",
        "md": "This graph is similar to the one for Q, showing sorted singular values for heads of K. Again, nGPT models demonstrate much lower condition numbers.\n\nFigure 12: Left: Distribution of the norms of vectors forming the complete attention matrices (not per head). Right: Distribution of singular values for all heads. The results are shown for the 4th layer.",
        "bBox": {
          "x": 108,
          "y": 107,
          "w": 395.97,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "A.7 LENGTH EXTRAPOLATION ABILITY",
        "md": "## A.7 LENGTH EXTRAPOLATION ABILITY",
        "bBox": {
          "x": 108,
          "y": 371,
          "w": 150.55,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "We investigate the length extrapolation ability of nGPT by evaluating its perplexity on the PG19 dataset, as shown in Figure 13. In standard GPTs, perplexity tends to increase dramatically when tested on sequences longer than pre-training length. In contrast, nGPT maintains a stable perplexity range even at extrapolated lengths. This result demonstrates that nGPT can handle longer contexts without requiring any modifications to RoPE, providing a clear advantage over standard GPTs in terms of length extrapolation.",
        "md": "We investigate the length extrapolation ability of nGPT by evaluating its perplexity on the PG19 dataset, as shown in Figure 13. In standard GPTs, perplexity tends to increase dramatically when tested on sequences longer than pre-training length. In contrast, nGPT maintains a stable perplexity range even at extrapolated lengths. This result demonstrates that nGPT can handle longer contexts without requiring any modifications to RoPE, providing a clear advantage over standard GPTs in terms of length extrapolation.",
        "bBox": {
          "x": 108,
          "y": 125,
          "w": 396.3,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "PG19 Perplexity",
        "md": "## PG19 Perplexity",
        "bBox": {
          "x": 198,
          "y": 161,
          "w": 49.67,
          "h": 8.74
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Model",
            "Line Color"
          ],
          [
            "1K GPT 1B",
            "Red"
          ],
          [
            "4K GPT 1B",
            "Orange"
          ],
          [
            "8K GPT 1B",
            "Yellow"
          ],
          [
            "1K nGPT 1B",
            "Green"
          ],
          [
            "4K nGPT 1B",
            "Blue"
          ],
          [
            "8K nGPT 1B",
            "Purple"
          ]
        ],
        "md": "| Model     | Line Color |\n|-----------|------------|\n| 1K GPT 1B | Red        |\n| 4K GPT 1B | Orange     |\n| 8K GPT 1B | Yellow     |\n| 1K nGPT 1B| Green      |\n| 4K nGPT 1B| Blue       |\n| 8K nGPT 1B| Purple     |",
        "isPerfectTable": true,
        "csv": "\"Model\",\"Line Color\"\n\"1K GPT 1B\",\"Red\"\n\"4K GPT 1B\",\"Orange\"\n\"8K GPT 1B\",\"Yellow\"\n\"1K nGPT 1B\",\"Green\"\n\"4K nGPT 1B\",\"Blue\"\n\"8K nGPT 1B\",\"Purple\"",
        "bBox": {
          "x": 198,
          "y": 99,
          "w": 94.15,
          "h": 9.32
        }
      },
      {
        "type": "text",
        "value": "The graph shows PG19 perplexity for different models from 1K to 32K sequence lengths. The GPT models show a dramatic increase in perplexity at longer lengths, while the nGPT models maintain relatively stable perplexity across all lengths.\n\nFigure 13: PG19 perplexity from 1K to 32K.\n\n17",
        "md": "The graph shows PG19 perplexity for different models from 1K to 32K sequence lengths. The GPT models show a dramatic increase in perplexity at longer lengths, while the nGPT models maintain relatively stable perplexity across all lengths.\n\nFigure 13: PG19 perplexity from 1K to 32K.\n\n17",
        "bBox": {
          "x": 198,
          "y": 125,
          "w": 178.77,
          "h": 9.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": "tested on sequences longer than pre-training length. In contrast, nGPT maintains a stable perplexity"
      }
    ]
  },
  {
    "page": 18,
    "text": "Normalized Transformer\n\n\n\nA.8      ABLATION STUDIES\n\n\n\nWe perform a series of ablations in nGPT, focusing on the selection of scaling factors sqk, su (sv ),\nsz , as well as evaluating the necessity of normalization. For the scaling factors, we first examine the\nimpact of different initializations of these parameters. Additionally, we explore whether these factors\ncan be simplified to a learnable scalar or a fixed value. For normalization, we analyze whether the\nremoval of QK-normalization is a feasible alternative. To ensure a fair comparison, all our ablation\nmodels have size 0.5B and context length 1k and are trained with learning rate 1 × 10−3 for 100k\niterations (∼52B tokens).\nTable 4 presents various combinations of sinit and sscale for each scaling factor, as well as the\ndownstream task accuracy and validation loss. Additionally, we report the mean value of each scal-\ning factor distribution to show the final converged scaling value after training. For sqk, we observe\nthat the mean value (Mean(s)) is relatievly stable with value around 1 across most initialization set-\ntings, except when using smaller sinit and larger sscale, which results in a Mean(s) value less than\n1. For su and sv , changes in initialization lead to significant shifts in Mean(s), accompanied by\nincreases in final validation loss. For sz , we see a substantial degradation in both accuracy and loss\nunder certain initializations. Overall, these results suggest that the baseline hyperparameter settings\nare robust but additional tuning of su, sv, and sz could potentially further improve performance.\nIn Table 5, we modify each learnable per-element vector of the scaling factors sqk, su (sv), sz , and\nthe eigen learning rates αA, αM to a single learned scalar or a fixed value. This ablation helps us\ndetermine whether these tuning parameters can be simplified in nGPT. From the table, we observe\nthat most changes result in only negligible degradation (≤ 0.3%) in validation loss, and some even\nlead to slight improvements in accuracy. Notably, replacing the per-element vector sqk with a single\nscalar has minimal impact on the mean value, while the mean value of su, sv , sz , αA, and αM\nshow a slight increase. Furthermore, even when fixing sqk, su, and sv, the model still achieves\ncomparable accuracy and validation loss to the baseline using the per-element vectors.\nIn Table 6, we investigate the impact of removing certain normalization operations in nGPT to\npotentially reduce training time per step. We remove QK normalization by eliminating the normal-\nization terms Norm in equations 15 and 16. The results show that this modification lead to similar\naccuracy and loss compared to the baseline, indicating that they are effective options for reducing\ncomputational overhead without compromising performance.\n\n\n\n                                                               18",
    "md": "Normalized Transformer\n\n## A.8 ABLATION STUDIES\n\nWe perform a series of ablations in nGPT, focusing on the selection of scaling factors $s_{qk}$, $s_u$ ($s_v$), $s_z$, as well as evaluating the necessity of normalization. For the scaling factors, we first examine the impact of different initializations of these parameters. Additionally, we explore whether these factors can be simplified to a learnable scalar or a fixed value. For normalization, we analyze whether the removal of QK-normalization is a feasible alternative. To ensure a fair comparison, all our ablation models have size 0.5B and context length 1k and are trained with learning rate 1 × 10^-3 for 100k iterations (~52B tokens).\n\nTable 4 presents various combinations of $s_{init}$ and $s_{scale}$ for each scaling factor, as well as the downstream task accuracy and validation loss. Additionally, we report the mean value of each scaling factor distribution to show the final converged scaling value after training. For $s_{qk}$, we observe that the mean value (Mean(s)) is relatively stable with value around 1 across most initialization settings, except when using smaller $s_{init}$ and larger $s_{scale}$, which results in a Mean(s) value less than 1. For $s_u$ and $s_v$, changes in initialization lead to significant shifts in Mean(s), accompanied by increases in final validation loss. For $s_z$, we see a substantial degradation in both accuracy and loss under certain initializations. Overall, these results suggest that the baseline hyperparameter settings are robust but additional tuning of $s_u$, $s_v$, and $s_z$ could potentially further improve performance.\n\nIn Table 5, we modify each learnable per-element vector of the scaling factors $s_{qk}$, $s_u$ ($s_v$), $s_z$, and the eigen learning rates $α_A$, $α_M$ to a single learned scalar or a fixed value. This ablation helps us determine whether these tuning parameters can be simplified in nGPT. From the table, we observe that most changes result in only negligible degradation (≤ 0.3%) in validation loss, and some even lead to slight improvements in accuracy. Notably, replacing the per-element vector $s_{qk}$ with a single scalar has minimal impact on the mean value, while the mean value of $s_u$, $s_v$, $s_z$, $α_A$, and $α_M$ show a slight increase. Furthermore, even when fixing $s_{qk}$, $s_u$, and $s_v$, the model still achieves comparable accuracy and validation loss to the baseline using the per-element vectors.\n\nIn Table 6, we investigate the impact of removing certain normalization operations in nGPT to potentially reduce training time per step. We remove QK normalization by eliminating the normalization terms Norm in equations 15 and 16. The results show that this modification lead to similar accuracy and loss compared to the baseline, indicating that they are effective options for reducing computational overhead without compromising performance.",
    "images": [
      {
        "name": "page_18.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "text",
        "value": "Normalized Transformer",
        "md": "Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "A.8 ABLATION STUDIES",
        "md": "## A.8 ABLATION STUDIES",
        "bBox": {
          "x": 108,
          "y": 92,
          "w": 83.71,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "We perform a series of ablations in nGPT, focusing on the selection of scaling factors $s_{qk}$, $s_u$ ($s_v$), $s_z$, as well as evaluating the necessity of normalization. For the scaling factors, we first examine the impact of different initializations of these parameters. Additionally, we explore whether these factors can be simplified to a learnable scalar or a fixed value. For normalization, we analyze whether the removal of QK-normalization is a feasible alternative. To ensure a fair comparison, all our ablation models have size 0.5B and context length 1k and are trained with learning rate 1 × 10^-3 for 100k iterations (~52B tokens).\n\nTable 4 presents various combinations of $s_{init}$ and $s_{scale}$ for each scaling factor, as well as the downstream task accuracy and validation loss. Additionally, we report the mean value of each scaling factor distribution to show the final converged scaling value after training. For $s_{qk}$, we observe that the mean value (Mean(s)) is relatively stable with value around 1 across most initialization settings, except when using smaller $s_{init}$ and larger $s_{scale}$, which results in a Mean(s) value less than 1. For $s_u$ and $s_v$, changes in initialization lead to significant shifts in Mean(s), accompanied by increases in final validation loss. For $s_z$, we see a substantial degradation in both accuracy and loss under certain initializations. Overall, these results suggest that the baseline hyperparameter settings are robust but additional tuning of $s_u$, $s_v$, and $s_z$ could potentially further improve performance.\n\nIn Table 5, we modify each learnable per-element vector of the scaling factors $s_{qk}$, $s_u$ ($s_v$), $s_z$, and the eigen learning rates $α_A$, $α_M$ to a single learned scalar or a fixed value. This ablation helps us determine whether these tuning parameters can be simplified in nGPT. From the table, we observe that most changes result in only negligible degradation (≤ 0.3%) in validation loss, and some even lead to slight improvements in accuracy. Notably, replacing the per-element vector $s_{qk}$ with a single scalar has minimal impact on the mean value, while the mean value of $s_u$, $s_v$, $s_z$, $α_A$, and $α_M$ show a slight increase. Furthermore, even when fixing $s_{qk}$, $s_u$, and $s_v$, the model still achieves comparable accuracy and validation loss to the baseline using the per-element vectors.\n\nIn Table 6, we investigate the impact of removing certain normalization operations in nGPT to potentially reduce training time per step. We remove QK normalization by eliminating the normalization terms Norm in equations 15 and 16. The results show that this modification lead to similar accuracy and loss compared to the baseline, indicating that they are effective options for reducing computational overhead without compromising performance.",
        "md": "We perform a series of ablations in nGPT, focusing on the selection of scaling factors $s_{qk}$, $s_u$ ($s_v$), $s_z$, as well as evaluating the necessity of normalization. For the scaling factors, we first examine the impact of different initializations of these parameters. Additionally, we explore whether these factors can be simplified to a learnable scalar or a fixed value. For normalization, we analyze whether the removal of QK-normalization is a feasible alternative. To ensure a fair comparison, all our ablation models have size 0.5B and context length 1k and are trained with learning rate 1 × 10^-3 for 100k iterations (~52B tokens).\n\nTable 4 presents various combinations of $s_{init}$ and $s_{scale}$ for each scaling factor, as well as the downstream task accuracy and validation loss. Additionally, we report the mean value of each scaling factor distribution to show the final converged scaling value after training. For $s_{qk}$, we observe that the mean value (Mean(s)) is relatively stable with value around 1 across most initialization settings, except when using smaller $s_{init}$ and larger $s_{scale}$, which results in a Mean(s) value less than 1. For $s_u$ and $s_v$, changes in initialization lead to significant shifts in Mean(s), accompanied by increases in final validation loss. For $s_z$, we see a substantial degradation in both accuracy and loss under certain initializations. Overall, these results suggest that the baseline hyperparameter settings are robust but additional tuning of $s_u$, $s_v$, and $s_z$ could potentially further improve performance.\n\nIn Table 5, we modify each learnable per-element vector of the scaling factors $s_{qk}$, $s_u$ ($s_v$), $s_z$, and the eigen learning rates $α_A$, $α_M$ to a single learned scalar or a fixed value. This ablation helps us determine whether these tuning parameters can be simplified in nGPT. From the table, we observe that most changes result in only negligible degradation (≤ 0.3%) in validation loss, and some even lead to slight improvements in accuracy. Notably, replacing the per-element vector $s_{qk}$ with a single scalar has minimal impact on the mean value, while the mean value of $s_u$, $s_v$, $s_z$, $α_A$, and $α_M$ show a slight increase. Furthermore, even when fixing $s_{qk}$, $s_u$, and $s_v$, the model still achieves comparable accuracy and validation loss to the baseline using the per-element vectors.\n\nIn Table 6, we investigate the impact of removing certain normalization operations in nGPT to potentially reduce training time per step. We remove QK normalization by eliminating the normalization terms Norm in equations 15 and 16. The results show that this modification lead to similar accuracy and loss compared to the baseline, indicating that they are effective options for reducing computational overhead without compromising performance.",
        "bBox": {
          "x": 107,
          "y": 135,
          "w": 396.78,
          "h": 9.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": ""
      },
      {
        "text": "the eigen learning rates"
      },
      {
        "text": "potentially reduce training time per step. We remove QK normalization by eliminating the normal-"
      },
      {
        "text": "accuracy and loss compared to the baseline, indicating that they are effective options for reducing"
      },
      {
        "text": "accuracy and loss compared to the baseline, indicating that they are effective options for reducing"
      }
    ]
  },
  {
    "page": 19,
    "text": " Normalized Transformer\n\n\n\n Table 4: Ablations of sinit and sscale. Our default setup is the first row of each section. Mean(s)\nis the mean value of the learned distribution after training. Avg. Acc is the average accuracy of our\nselected five downstream tasks, Valid. Loss is the final validation loss, and we use dmodel = 1024.\n\n\n\n                                  sinit            sscale         Mean(s)         Avg. Acc (%) ↑         Valid. Loss ↓\n                   sqk              1          1/√dmodel√dmodel      1.51                54.44                2.252\n                   sqk            0.33         1/√dmodel             1.36                54.67               -0.33%\n                   sqk            0.05         1/                    1.01                53.69               -0.09%\n                   sqk              1                1               1.38                54.19               -0.09%\n                   sqk            0.33               1               1.11                54.89               -0.08%\n                   sqk            0.05               1               0.29                52.41               +1.94%\n                su & sv       1/√1model              1          1.24 & 1.12              54.44                2.252\n                su & sv            d              √1model       0.03 & 0.03              53.51               +0.92%\n                su & sv       1/√1model        1/√dmodeld       2.75 & 11.2              53.90               +0.05%\n                su & sv            d           1/√dmodel        0.40 & 0.25              54.39               +0.48%\n                    sz          √dmodel1       1/√dmodel             60.8                54.44                2.252\n                    sz                         1/                   106.1                52.60               +1.06%\n                    sz          √dmodel1             1               23.6                52.71               +3.12%\n                    sz                               1               63.6                51.84               +1.66%\n\n\n\n Table 5: Ablations of replacing learnable per-element vector (scaling factors and eigen learning rate)\n with a single learned scalar or a fixed value. The number of the learned vector, learned scalar and\n fixed value are the mean values across all layers.\n\n\n\n sqk                        su & sv                    sz                  αA & αM               Avg. Acc (%) ↑         Valid. Loss ↓\n vector = 1.47      vector = 1.12 & 1.24        vector = 60.80       vector = 0.22 & 0.33              54.44                2.252\n scalar = 1.49      vector = 1.13 & 1.23        vector = 62.11       vector = 0.22 & 0.33              54.05               +0.22%\n vector = 1.46           scalar = 1.46          vector = 60.70       vector = 0.22 & 0.33              54.23               +0.07%\n vector = 1.47      vector = 1.12 & 1.24        scalar = 95.65       vector = 0.22 & 0.33              53.69               +0.20%\n vector = 1.40      vector = 1.13 & 1.13        vector = 60.90       scalar = 0.30 & 0.36              54.86               +0.22%\n scalar = 1.51           scalar = 1.47          vector = 61.18       vector = 0.22 & 0.32              54.52               +0.17%\n scalar = 1.52           scalar = 1.68          scalar = 96.65       vector = 0.22 & 0.30              52.59               +0.30%\n scalar = 1.49           scalar = 1.17          scalar = 88.64       scalar = 0.30 & 0.37              53.61               +0.62%\n value = 1.00       vector = 1.12 & 1.15        vector = 60.69       vector = 0.24 & 0.35              54.17               +0.11%\n vector = 1.45           value = 1.00           vector = 61.53       vector = 0.23 & 0.35              55.51               +0.05%\n value = 1.00            value = 1.00           vector = 61.26       vector = 0.24 & 0.36              53.63               +0.20%\n value = 1.00            value = 1.00           scalar = 96.15       vector = 0.22 & 0.35              53.37               +0.40%\n\n\n\n Table 6: Ablations on the removal of QK normalization in nGPT which involves taking out the\n normalization terms Norm in equations 15 and 16.\n\n\n\n                                 ARC-E      HellaSwag WinoGrande WSC273 LAMBADA                        Avg. Acc (%) ↑     Valid. Loss ↓\n   Baseline nGPT                  53.07       46.03           55.96          67.03         50.13            54.44             2.252\n   Baseline nGPT - QK norm        52.65       46.07           56.27          68.86         49.68            54.71            +0.12%\n\n\n\n                                                                   19",
    "md": "# Normalized Transformer\n\nTable 4: Ablations of $s_{init}$ and $s_{scale}$. Our default setup is the first row of each section. Mean(s) is the mean value of the learned distribution after training. Avg. Acc is the average accuracy of our selected five downstream tasks, Valid. Loss is the final validation loss, and we use $d_{model} = 1024$.\n\n| | $s_{init}$ | $s_{scale}$ | Mean(s) | Avg. Acc (%) ↑ | Valid. Loss ↓ |\n|---|---|---|---|---|---|\n| $s_{qk}$ | 1 | $1/\\sqrt{d_{model}}$ | 1.51 | 54.44 | 2.252 |\n| $s_{qk}$ | 0.33 | $1/\\sqrt{d_{model}}$ | 1.36 | 54.67 | -0.33% |\n| $s_{qk}$ | 0.05 | $1/\\sqrt{d_{model}}$ | 1.01 | 53.69 | -0.09% |\n| $s_{qk}$ | 1 | 1 | 1.38 | 54.19 | -0.09% |\n| $s_{qk}$ | 0.33 | 1 | 1.11 | 54.89 | -0.08% |\n| $s_{qk}$ | 0.05 | 1 | 0.29 | 52.41 | +1.94% |\n| $s_u$ & $s_v$ | 1 | 1 | 1.24 & 1.12 | 54.44 | 2.252 |\n| $s_u$ & $s_v$ | $1/\\sqrt{d_{model}}$ | 1 | 0.03 & 0.03 | 53.51 | +0.92% |\n| $s_u$ & $s_v$ | 1 | $1/\\sqrt{d_{model}}$ | 2.75 & 11.2 | 53.90 | +0.05% |\n| $s_u$ & $s_v$ | $1/\\sqrt{d_{model}}$ | $1/\\sqrt{d_{model}}$ | 0.40 & 0.25 | 54.39 | +0.48% |\n| $s_z$ | 1 | $1/\\sqrt{d_{model}}$ | 60.8 | 54.44 | 2.252 |\n| $s_z$ | $\\sqrt{d_{model}}$ | $1/\\sqrt{d_{model}}$ | 106.1 | 52.60 | +1.06% |\n| $s_z$ | 1 | 1 | 23.6 | 52.71 | +3.12% |\n| $s_z$ | $\\sqrt{d_{model}}$ | 1 | 63.6 | 51.84 | +1.66% |\n\nTable 5: Ablations of replacing learnable per-element vector (scaling factors and eigen learning rate) with a single learned scalar or a fixed value. The number of the learned vector, learned scalar and fixed value are the mean values across all layers.\n\n| $s_{qk}$ | $s_u$ & $s_v$ | $s_z$ | $\\alpha_A$ & $\\alpha_M$ | Avg. Acc (%) ↑ | Valid. Loss ↓ |\n|---|---|---|---|---|---|\n| vector = 1.47 | vector = 1.12 & 1.24 | vector = 60.80 | vector = 0.22 & 0.33 | 54.44 | 2.252 |\n| scalar = 1.49 | vector = 1.13 & 1.23 | vector = 62.11 | vector = 0.22 & 0.33 | 54.05 | +0.22% |\n| vector = 1.46 | scalar = 1.46 | vector = 60.70 | vector = 0.22 & 0.33 | 54.23 | +0.07% |\n| vector = 1.47 | vector = 1.12 & 1.24 | scalar = 95.65 | vector = 0.22 & 0.33 | 53.69 | +0.20% |\n| vector = 1.40 | vector = 1.13 & 1.13 | vector = 60.90 | scalar = 0.30 & 0.36 | 54.86 | +0.22% |\n| scalar = 1.51 | scalar = 1.47 | vector = 61.18 | vector = 0.22 & 0.32 | 54.52 | +0.17% |\n| scalar = 1.52 | scalar = 1.68 | scalar = 96.65 | vector = 0.22 & 0.30 | 52.59 | +0.30% |\n| scalar = 1.49 | scalar = 1.17 | scalar = 88.64 | scalar = 0.30 & 0.37 | 53.61 | +0.62% |\n| value = 1.00 | vector = 1.12 & 1.15 | vector = 60.69 | vector = 0.24 & 0.35 | 54.17 | +0.11% |\n| vector = 1.45 | value = 1.00 | vector = 61.53 | vector = 0.23 & 0.35 | 55.51 | +0.05% |\n| value = 1.00 | value = 1.00 | vector = 61.26 | vector = 0.24 & 0.36 | 53.63 | +0.20% |\n| value = 1.00 | value = 1.00 | scalar = 96.15 | vector = 0.22 & 0.35 | 53.37 | +0.40% |\n\nTable 6: Ablations on the removal of QK normalization in nGPT which involves taking out the normalization terms Norm in equations 15 and 16.\n\n| | ARC-E | HellaSwag | WinoGrande | WSC273 | LAMBADA | Avg. Acc (%) ↑ | Valid. Loss ↓ |\n|---|---|---|---|---|---|---|---|\n| Baseline nGPT | 53.07 | 46.03 | 55.96 | 67.03 | 50.13 | 54.44 | 2.252 |\n| Baseline nGPT - QK norm | 52.65 | 46.07 | 56.27 | 68.86 | 49.68 | 54.71 | +0.12% |",
    "images": [
      {
        "name": "page_19.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "Table 4: Ablations of $s_{init}$ and $s_{scale}$. Our default setup is the first row of each section. Mean(s) is the mean value of the learned distribution after training. Avg. Acc is the average accuracy of our selected five downstream tasks, Valid. Loss is the final validation loss, and we use $d_{model} = 1024$.",
        "md": "Table 4: Ablations of $s_{init}$ and $s_{scale}$. Our default setup is the first row of each section. Mean(s) is the mean value of the learned distribution after training. Avg. Acc is the average accuracy of our selected five downstream tasks, Valid. Loss is the final validation loss, and we use $d_{model} = 1024$.",
        "bBox": {
          "x": 107,
          "y": 101,
          "w": 396.48,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "",
            "$s_{init}$",
            "$s_{scale}$",
            "Mean(s)",
            "Avg. Acc (%) ↑",
            "Valid. Loss ↓"
          ],
          [
            "$s_{qk}$",
            "1",
            "$1/\\sqrt{d_{model}}$",
            "1.51",
            "54.44",
            "2.252"
          ],
          [
            "$s_{qk}$",
            "0.33",
            "$1/\\sqrt{d_{model}}$",
            "1.36",
            "54.67",
            "-0.33%"
          ],
          [
            "$s_{qk}$",
            "0.05",
            "$1/\\sqrt{d_{model}}$",
            "1.01",
            "53.69",
            "-0.09%"
          ],
          [
            "$s_{qk}$",
            "1",
            "1",
            "1.38",
            "54.19",
            "-0.09%"
          ],
          [
            "$s_{qk}$",
            "0.33",
            "1",
            "1.11",
            "54.89",
            "-0.08%"
          ],
          [
            "$s_{qk}$",
            "0.05",
            "1",
            "0.29",
            "52.41",
            "+1.94%"
          ],
          [
            "$s_u$ & $s_v$",
            "1",
            "1",
            "1.24 & 1.12",
            "54.44",
            "2.252"
          ],
          [
            "$s_u$ & $s_v$",
            "$1/\\sqrt{d_{model}}$",
            "1",
            "0.03 & 0.03",
            "53.51",
            "+0.92%"
          ],
          [
            "$s_u$ & $s_v$",
            "1",
            "$1/\\sqrt{d_{model}}$",
            "2.75 & 11.2",
            "53.90",
            "+0.05%"
          ],
          [
            "$s_u$ & $s_v$",
            "$1/\\sqrt{d_{model}}$",
            "$1/\\sqrt{d_{model}}$",
            "0.40 & 0.25",
            "54.39",
            "+0.48%"
          ],
          [
            "$s_z$",
            "1",
            "$1/\\sqrt{d_{model}}$",
            "60.8",
            "54.44",
            "2.252"
          ],
          [
            "$s_z$",
            "$\\sqrt{d_{model}}$",
            "$1/\\sqrt{d_{model}}$",
            "106.1",
            "52.60",
            "+1.06%"
          ],
          [
            "$s_z$",
            "1",
            "1",
            "23.6",
            "52.71",
            "+3.12%"
          ],
          [
            "$s_z$",
            "$\\sqrt{d_{model}}$",
            "1",
            "63.6",
            "51.84",
            "+1.66%"
          ]
        ],
        "md": "| | $s_{init}$ | $s_{scale}$ | Mean(s) | Avg. Acc (%) ↑ | Valid. Loss ↓ |\n|---|---|---|---|---|---|\n| $s_{qk}$ | 1 | $1/\\sqrt{d_{model}}$ | 1.51 | 54.44 | 2.252 |\n| $s_{qk}$ | 0.33 | $1/\\sqrt{d_{model}}$ | 1.36 | 54.67 | -0.33% |\n| $s_{qk}$ | 0.05 | $1/\\sqrt{d_{model}}$ | 1.01 | 53.69 | -0.09% |\n| $s_{qk}$ | 1 | 1 | 1.38 | 54.19 | -0.09% |\n| $s_{qk}$ | 0.33 | 1 | 1.11 | 54.89 | -0.08% |\n| $s_{qk}$ | 0.05 | 1 | 0.29 | 52.41 | +1.94% |\n| $s_u$ & $s_v$ | 1 | 1 | 1.24 & 1.12 | 54.44 | 2.252 |\n| $s_u$ & $s_v$ | $1/\\sqrt{d_{model}}$ | 1 | 0.03 & 0.03 | 53.51 | +0.92% |\n| $s_u$ & $s_v$ | 1 | $1/\\sqrt{d_{model}}$ | 2.75 & 11.2 | 53.90 | +0.05% |\n| $s_u$ & $s_v$ | $1/\\sqrt{d_{model}}$ | $1/\\sqrt{d_{model}}$ | 0.40 & 0.25 | 54.39 | +0.48% |\n| $s_z$ | 1 | $1/\\sqrt{d_{model}}$ | 60.8 | 54.44 | 2.252 |\n| $s_z$ | $\\sqrt{d_{model}}$ | $1/\\sqrt{d_{model}}$ | 106.1 | 52.60 | +1.06% |\n| $s_z$ | 1 | 1 | 23.6 | 52.71 | +3.12% |\n| $s_z$ | $\\sqrt{d_{model}}$ | 1 | 63.6 | 51.84 | +1.66% |",
        "isPerfectTable": true,
        "csv": "\"\",\"$s_{init}$\",\"$s_{scale}$\",\"Mean(s)\",\"Avg. Acc (%) ↑\",\"Valid. Loss ↓\"\n\"$s_{qk}$\",\"1\",\"$1/\\sqrt{d_{model}}$\",\"1.51\",\"54.44\",\"2.252\"\n\"$s_{qk}$\",\"0.33\",\"$1/\\sqrt{d_{model}}$\",\"1.36\",\"54.67\",\"-0.33%\"\n\"$s_{qk}$\",\"0.05\",\"$1/\\sqrt{d_{model}}$\",\"1.01\",\"53.69\",\"-0.09%\"\n\"$s_{qk}$\",\"1\",\"1\",\"1.38\",\"54.19\",\"-0.09%\"\n\"$s_{qk}$\",\"0.33\",\"1\",\"1.11\",\"54.89\",\"-0.08%\"\n\"$s_{qk}$\",\"0.05\",\"1\",\"0.29\",\"52.41\",\"+1.94%\"\n\"$s_u$ & $s_v$\",\"1\",\"1\",\"1.24 & 1.12\",\"54.44\",\"2.252\"\n\"$s_u$ & $s_v$\",\"$1/\\sqrt{d_{model}}$\",\"1\",\"0.03 & 0.03\",\"53.51\",\"+0.92%\"\n\"$s_u$ & $s_v$\",\"1\",\"$1/\\sqrt{d_{model}}$\",\"2.75 & 11.2\",\"53.90\",\"+0.05%\"\n\"$s_u$ & $s_v$\",\"$1/\\sqrt{d_{model}}$\",\"$1/\\sqrt{d_{model}}$\",\"0.40 & 0.25\",\"54.39\",\"+0.48%\"\n\"$s_z$\",\"1\",\"$1/\\sqrt{d_{model}}$\",\"60.8\",\"54.44\",\"2.252\"\n\"$s_z$\",\"$\\sqrt{d_{model}}$\",\"$1/\\sqrt{d_{model}}$\",\"106.1\",\"52.60\",\"+1.06%\"\n\"$s_z$\",\"1\",\"1\",\"23.6\",\"52.71\",\"+3.12%\"\n\"$s_z$\",\"$\\sqrt{d_{model}}$\",\"1\",\"63.6\",\"51.84\",\"+1.66%\"",
        "bBox": {
          "x": 205,
          "y": 135,
          "w": 147.32,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "Table 5: Ablations of replacing learnable per-element vector (scaling factors and eigen learning rate) with a single learned scalar or a fixed value. The number of the learned vector, learned scalar and fixed value are the mean values across all layers.",
        "md": "Table 5: Ablations of replacing learnable per-element vector (scaling factors and eigen learning rate) with a single learned scalar or a fixed value. The number of the learned vector, learned scalar and fixed value are the mean values across all layers.",
        "bBox": {
          "x": 108,
          "y": 223,
          "w": 396.08,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "$s_{qk}$",
            "$s_u$ & $s_v$",
            "$s_z$",
            "$\\alpha_A$ & $\\alpha_M$",
            "Avg. Acc (%) ↑",
            "Valid. Loss ↓"
          ],
          [
            "vector = 1.47",
            "vector = 1.12 & 1.24",
            "vector = 60.80",
            "vector = 0.22 & 0.33",
            "54.44",
            "2.252"
          ],
          [
            "scalar = 1.49",
            "vector = 1.13 & 1.23",
            "vector = 62.11",
            "vector = 0.22 & 0.33",
            "54.05",
            "+0.22%"
          ],
          [
            "vector = 1.46",
            "scalar = 1.46",
            "vector = 60.70",
            "vector = 0.22 & 0.33",
            "54.23",
            "+0.07%"
          ],
          [
            "vector = 1.47",
            "vector = 1.12 & 1.24",
            "scalar = 95.65",
            "vector = 0.22 & 0.33",
            "53.69",
            "+0.20%"
          ],
          [
            "vector = 1.40",
            "vector = 1.13 & 1.13",
            "vector = 60.90",
            "scalar = 0.30 & 0.36",
            "54.86",
            "+0.22%"
          ],
          [
            "scalar = 1.51",
            "scalar = 1.47",
            "vector = 61.18",
            "vector = 0.22 & 0.32",
            "54.52",
            "+0.17%"
          ],
          [
            "scalar = 1.52",
            "scalar = 1.68",
            "scalar = 96.65",
            "vector = 0.22 & 0.30",
            "52.59",
            "+0.30%"
          ],
          [
            "scalar = 1.49",
            "scalar = 1.17",
            "scalar = 88.64",
            "scalar = 0.30 & 0.37",
            "53.61",
            "+0.62%"
          ],
          [
            "value = 1.00",
            "vector = 1.12 & 1.15",
            "vector = 60.69",
            "vector = 0.24 & 0.35",
            "54.17",
            "+0.11%"
          ],
          [
            "vector = 1.45",
            "value = 1.00",
            "vector = 61.53",
            "vector = 0.23 & 0.35",
            "55.51",
            "+0.05%"
          ],
          [
            "value = 1.00",
            "value = 1.00",
            "vector = 61.26",
            "vector = 0.24 & 0.36",
            "53.63",
            "+0.20%"
          ],
          [
            "value = 1.00",
            "value = 1.00",
            "scalar = 96.15",
            "vector = 0.22 & 0.35",
            "53.37",
            "+0.40%"
          ]
        ],
        "md": "| $s_{qk}$ | $s_u$ & $s_v$ | $s_z$ | $\\alpha_A$ & $\\alpha_M$ | Avg. Acc (%) ↑ | Valid. Loss ↓ |\n|---|---|---|---|---|---|\n| vector = 1.47 | vector = 1.12 & 1.24 | vector = 60.80 | vector = 0.22 & 0.33 | 54.44 | 2.252 |\n| scalar = 1.49 | vector = 1.13 & 1.23 | vector = 62.11 | vector = 0.22 & 0.33 | 54.05 | +0.22% |\n| vector = 1.46 | scalar = 1.46 | vector = 60.70 | vector = 0.22 & 0.33 | 54.23 | +0.07% |\n| vector = 1.47 | vector = 1.12 & 1.24 | scalar = 95.65 | vector = 0.22 & 0.33 | 53.69 | +0.20% |\n| vector = 1.40 | vector = 1.13 & 1.13 | vector = 60.90 | scalar = 0.30 & 0.36 | 54.86 | +0.22% |\n| scalar = 1.51 | scalar = 1.47 | vector = 61.18 | vector = 0.22 & 0.32 | 54.52 | +0.17% |\n| scalar = 1.52 | scalar = 1.68 | scalar = 96.65 | vector = 0.22 & 0.30 | 52.59 | +0.30% |\n| scalar = 1.49 | scalar = 1.17 | scalar = 88.64 | scalar = 0.30 & 0.37 | 53.61 | +0.62% |\n| value = 1.00 | vector = 1.12 & 1.15 | vector = 60.69 | vector = 0.24 & 0.35 | 54.17 | +0.11% |\n| vector = 1.45 | value = 1.00 | vector = 61.53 | vector = 0.23 & 0.35 | 55.51 | +0.05% |\n| value = 1.00 | value = 1.00 | vector = 61.26 | vector = 0.24 & 0.36 | 53.63 | +0.20% |\n| value = 1.00 | value = 1.00 | scalar = 96.15 | vector = 0.22 & 0.35 | 53.37 | +0.40% |",
        "isPerfectTable": true,
        "csv": "\"$s_{qk}$\",\"$s_u$ & $s_v$\",\"$s_z$\",\"$\\alpha_A$ & $\\alpha_M$\",\"Avg. Acc (%) ↑\",\"Valid. Loss ↓\"\n\"vector = 1.47\",\"vector = 1.12 & 1.24\",\"vector = 60.80\",\"vector = 0.22 & 0.33\",\"54.44\",\"2.252\"\n\"scalar = 1.49\",\"vector = 1.13 & 1.23\",\"vector = 62.11\",\"vector = 0.22 & 0.33\",\"54.05\",\"+0.22%\"\n\"vector = 1.46\",\"scalar = 1.46\",\"vector = 60.70\",\"vector = 0.22 & 0.33\",\"54.23\",\"+0.07%\"\n\"vector = 1.47\",\"vector = 1.12 & 1.24\",\"scalar = 95.65\",\"vector = 0.22 & 0.33\",\"53.69\",\"+0.20%\"\n\"vector = 1.40\",\"vector = 1.13 & 1.13\",\"vector = 60.90\",\"scalar = 0.30 & 0.36\",\"54.86\",\"+0.22%\"\n\"scalar = 1.51\",\"scalar = 1.47\",\"vector = 61.18\",\"vector = 0.22 & 0.32\",\"54.52\",\"+0.17%\"\n\"scalar = 1.52\",\"scalar = 1.68\",\"scalar = 96.65\",\"vector = 0.22 & 0.30\",\"52.59\",\"+0.30%\"\n\"scalar = 1.49\",\"scalar = 1.17\",\"scalar = 88.64\",\"scalar = 0.30 & 0.37\",\"53.61\",\"+0.62%\"\n\"value = 1.00\",\"vector = 1.12 & 1.15\",\"vector = 60.69\",\"vector = 0.24 & 0.35\",\"54.17\",\"+0.11%\"\n\"vector = 1.45\",\"value = 1.00\",\"vector = 61.53\",\"vector = 0.23 & 0.35\",\"55.51\",\"+0.05%\"\n\"value = 1.00\",\"value = 1.00\",\"vector = 61.26\",\"vector = 0.24 & 0.36\",\"53.63\",\"+0.20%\"\n\"value = 1.00\",\"value = 1.00\",\"scalar = 96.15\",\"vector = 0.22 & 0.35\",\"53.37\",\"+0.40%\"",
        "bBox": {
          "x": 110,
          "y": 135,
          "w": 289.32,
          "h": 8.65
        }
      },
      {
        "type": "text",
        "value": "Table 6: Ablations on the removal of QK normalization in nGPT which involves taking out the normalization terms Norm in equations 15 and 16.",
        "md": "Table 6: Ablations on the removal of QK normalization in nGPT which involves taking out the normalization terms Norm in equations 15 and 16.",
        "bBox": {
          "x": 108,
          "y": 150,
          "w": 396.1,
          "h": 9.96
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "",
            "ARC-E",
            "HellaSwag",
            "WinoGrande",
            "WSC273",
            "LAMBADA",
            "Avg. Acc (%) ↑",
            "Valid. Loss ↓"
          ],
          [
            "Baseline nGPT",
            "53.07",
            "46.03",
            "55.96",
            "67.03",
            "50.13",
            "54.44",
            "2.252"
          ],
          [
            "Baseline nGPT - QK norm",
            "52.65",
            "46.07",
            "56.27",
            "68.86",
            "49.68",
            "54.71",
            "+0.12%"
          ]
        ],
        "md": "| | ARC-E | HellaSwag | WinoGrande | WSC273 | LAMBADA | Avg. Acc (%) ↑ | Valid. Loss ↓ |\n|---|---|---|---|---|---|---|---|\n| Baseline nGPT | 53.07 | 46.03 | 55.96 | 67.03 | 50.13 | 54.44 | 2.252 |\n| Baseline nGPT - QK norm | 52.65 | 46.07 | 56.27 | 68.86 | 49.68 | 54.71 | +0.12% |",
        "isPerfectTable": true,
        "csv": "\"\",\"ARC-E\",\"HellaSwag\",\"WinoGrande\",\"WSC273\",\"LAMBADA\",\"Avg. Acc (%) ↑\",\"Valid. Loss ↓\"\n\"Baseline nGPT\",\"53.07\",\"46.03\",\"55.96\",\"67.03\",\"50.13\",\"54.44\",\"2.252\"\n\"Baseline nGPT - QK norm\",\"52.65\",\"46.07\",\"56.27\",\"68.86\",\"49.68\",\"54.71\",\"+0.12%\"",
        "bBox": {
          "x": 114,
          "y": 135,
          "w": 141.32,
          "h": 419.24
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": ""
      },
      {
        "text": ""
      }
    ]
  },
  {
    "page": 20,
    "text": "Normalized Transformer\n\n\n\nA.9      ANALYSIS OF SCALING PARAMETERS\n\n\n\nIn nGPT, we introduce a total six trainable parameters: the eigen learning rates αA and αM , along\nwith the scaling factors sqk, su, sv and sz . Since these parameters are updated using gradients, we\nare interested in understanding their learned distributions after training. In Figure 14, we present\nthe histograms of these saved weights combining across layers and analyze their distributions under\ndifferent context lengths, model sizes, learning rates, and numbers of training tokens.\nFor αA and αM , we observe that their distributions remain stable across various learning rates.\nHowever, increasing the context length or the number of training tokens tends to shift the distribu-\ntions to the right, indicating that the hidden state h requires more transformation from the Attention\nand MLP blocks to handle the larger inputs. Additionally, the mean values for these parameters tend\nto be smaller in larger models because we have more layers to update the hidden state.\nRegarding the scaling factors, sqk is quite stable under all the conditions, suggesting that a fixed\nvalue, similar to the softmax scaling factor in original transformer, may be sufficient. Interestingly,\nwe find that sqk has a high density close to zero, indicating the sparsity of attention in nGPT. For\nsu and sv, their distributions shift left as the context length increases, but shift right as the model\nsize or the number of training tokens increases. Furthermore, these two factors become flatter when\nusing larger learning rate. Lastly, for sz , we see higher mean values for longer context lengths,\nlarger model sizes, and more training tokens, suggesting that the model may learn to use a lower\ntemperature to make the final word distribution sharper.\n\n\n\n                                                                20",
    "md": "# Normalized Transformer\n\n## A.9 ANALYSIS OF SCALING PARAMETERS\n\nIn nGPT, we introduce a total six trainable parameters: the eigen learning rates αA and αM, along with the scaling factors sqk, su, sv and sz. Since these parameters are updated using gradients, we are interested in understanding their learned distributions after training. In Figure 14, we present the histograms of these saved weights combining across layers and analyze their distributions under different context lengths, model sizes, learning rates, and numbers of training tokens.\n\nFor αA and αM, we observe that their distributions remain stable across various learning rates. However, increasing the context length or the number of training tokens tends to shift the distributions to the right, indicating that the hidden state h requires more transformation from the Attention and MLP blocks to handle the larger inputs. Additionally, the mean values for these parameters tend to be smaller in larger models because we have more layers to update the hidden state.\n\nRegarding the scaling factors, sqk is quite stable under all the conditions, suggesting that a fixed value, similar to the softmax scaling factor in original transformer, may be sufficient. Interestingly, we find that sqk has a high density close to zero, indicating the sparsity of attention in nGPT. For su and sv, their distributions shift left as the context length increases, but shift right as the model size or the number of training tokens increases. Furthermore, these two factors become flatter when using larger learning rate. Lastly, for sz, we see higher mean values for longer context lengths, larger model sizes, and more training tokens, suggesting that the model may learn to use a lower temperature to make the final word distribution sharper.",
    "images": [
      {
        "name": "page_20.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "heading",
        "lvl": 2,
        "value": "A.9 ANALYSIS OF SCALING PARAMETERS",
        "md": "## A.9 ANALYSIS OF SCALING PARAMETERS",
        "bBox": {
          "x": 108,
          "y": 92,
          "w": 157.03,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "In nGPT, we introduce a total six trainable parameters: the eigen learning rates αA and αM, along with the scaling factors sqk, su, sv and sz. Since these parameters are updated using gradients, we are interested in understanding their learned distributions after training. In Figure 14, we present the histograms of these saved weights combining across layers and analyze their distributions under different context lengths, model sizes, learning rates, and numbers of training tokens.\n\nFor αA and αM, we observe that their distributions remain stable across various learning rates. However, increasing the context length or the number of training tokens tends to shift the distributions to the right, indicating that the hidden state h requires more transformation from the Attention and MLP blocks to handle the larger inputs. Additionally, the mean values for these parameters tend to be smaller in larger models because we have more layers to update the hidden state.\n\nRegarding the scaling factors, sqk is quite stable under all the conditions, suggesting that a fixed value, similar to the softmax scaling factor in original transformer, may be sufficient. Interestingly, we find that sqk has a high density close to zero, indicating the sparsity of attention in nGPT. For su and sv, their distributions shift left as the context length increases, but shift right as the model size or the number of training tokens increases. Furthermore, these two factors become flatter when using larger learning rate. Lastly, for sz, we see higher mean values for longer context lengths, larger model sizes, and more training tokens, suggesting that the model may learn to use a lower temperature to make the final word distribution sharper.",
        "md": "In nGPT, we introduce a total six trainable parameters: the eigen learning rates αA and αM, along with the scaling factors sqk, su, sv and sz. Since these parameters are updated using gradients, we are interested in understanding their learned distributions after training. In Figure 14, we present the histograms of these saved weights combining across layers and analyze their distributions under different context lengths, model sizes, learning rates, and numbers of training tokens.\n\nFor αA and αM, we observe that their distributions remain stable across various learning rates. However, increasing the context length or the number of training tokens tends to shift the distributions to the right, indicating that the hidden state h requires more transformation from the Attention and MLP blocks to handle the larger inputs. Additionally, the mean values for these parameters tend to be smaller in larger models because we have more layers to update the hidden state.\n\nRegarding the scaling factors, sqk is quite stable under all the conditions, suggesting that a fixed value, similar to the softmax scaling factor in original transformer, may be sufficient. Interestingly, we find that sqk has a high density close to zero, indicating the sparsity of attention in nGPT. For su and sv, their distributions shift left as the context length increases, but shift right as the model size or the number of training tokens increases. Furthermore, these two factors become flatter when using larger learning rate. Lastly, for sz, we see higher mean values for longer context lengths, larger model sizes, and more training tokens, suggesting that the model may learn to use a lower temperature to make the final word distribution sharper.",
        "bBox": {
          "x": 107,
          "y": 135,
          "w": 396.42,
          "h": 9.96
        }
      }
    ],
    "status": "OK",
    "links": [
      {
        "text": "the histograms of these saved weights combining across layers and analyze their distributions under"
      }
    ]
  },
  {
    "page": 21,
    "text": "Normalized Transformer\n                                                            dimerent modcleles                                                     dlttcrnt totens\n                              ImAnad n\n                              Imejn-02%8                                                                                                   523 Imt4n-0 245\n                  dittornt Icnaths                                                                                                 dimerent tokons\n                                                                                                                                           a ImAnadD\n                              Imejn-019                                                                                                    523 Imejn-04Z\n                     ditorcnt lenaths                          dlHtcrcnt modee                   Jbsa                                 dlmerent tokenx\n                               Im*4n-1                                                                                                      573 Imejn-141\n                  Jitterent lcnnths                         olmerent modceles                          ditercnt Icamlna rete       ditorcnttokent\n                               Imaanen\n                               ImtJn-119                                                                                                    573 Imejn-1 41\n                                                            olmerent modce/ter                         dltterent Icamlng rotos\n                                                                                                                    0 3 Imtun-188           573 Imejn-1 83\n                  dlmerent lennth}                                                                                                 dltterent tokon\n                               Im Ane\n                               Imen-8r                                                                                                      573 Imejn-62 51\nFigure 14: The distribution of trainable eigen learning rates and scaling factors varies under different\ncontext lengths, model sizes, learning rates, and number of training tokens. If not specified, we use\ncontext length 1K, model size 0.5B, learning rate 2.0 × 10−3, and training tokens 52B as default.\n\n\n\n                                                                                       21",
    "md": "# Normalized Transformer\n\n![Figure 14: The distribution of trainable eigen learning rates and scaling factors varies under different context lengths, model sizes, learning rates, and number of training tokens.]\n\nThe image contains a grid of 24 graphs arranged in 6 rows and 4 columns. Each row represents a different aspect of the Normalized Transformer, and each column represents a different variable being tested. The graphs show various distributions and patterns, with different colors representing different parameters or conditions.\n\nHere's a breakdown of the columns:\n\n1. αw: different lengths\n2. αw: different model sizes\n3. αw: different learning rates\n4. αw: different tokens\n\nEach row seems to be testing a different component or scaling factor (α, β, γ, δ, ε, ζ).\n\nThe graphs display various distribution shapes including bell curves, bimodal distributions, and more complex patterns. They use different color schemes to differentiate between conditions, such as context lengths, model sizes, or number of tokens.\n\nAt the bottom of the image, there's a caption that reads:\n\nFigure 14: The distribution of trainable eigen learning rates and scaling factors varies under different context lengths, model sizes, learning rates, and number of training tokens. If not specified, we use context length 1K, model size 0.5B, learning rate 2.0 × 10^-3, and training tokens 52B as default.\n\nThis figure demonstrates how various parameters of the Normalized Transformer model change under different conditions, providing insights into the model's behavior and adaptability across different configurations.",
    "images": [
      {
        "name": "img_p20_1.png",
        "height": 147,
        "width": 192,
        "x": 108,
        "y": 163.97459999999998,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_2.png",
        "height": 147,
        "width": 192,
        "x": 205.56,
        "y": 163.97459999999998,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_3.png",
        "height": 147,
        "width": 192,
        "x": 303.12,
        "y": 163.97459999999998,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_4.png",
        "height": 147,
        "width": 192,
        "x": 400.56,
        "y": 163.97459999999998,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_5.png",
        "height": 147,
        "width": 192,
        "x": 108,
        "y": 237.6546,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_6.png",
        "height": 147,
        "width": 192,
        "x": 205.56,
        "y": 237.6546,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_7.png",
        "height": 147,
        "width": 192,
        "x": 303.12,
        "y": 237.6546,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_8.png",
        "height": 147,
        "width": 192,
        "x": 400.56,
        "y": 237.6546,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_9.png",
        "height": 147,
        "width": 192,
        "x": 108,
        "y": 311.4546,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_10.png",
        "height": 147,
        "width": 192,
        "x": 205.56,
        "y": 311.4546,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_11.png",
        "height": 147,
        "width": 192,
        "x": 303.12,
        "y": 311.4546,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_12.png",
        "height": 147,
        "width": 192,
        "x": 400.56,
        "y": 311.4546,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_13.png",
        "height": 147,
        "width": 192,
        "x": 108,
        "y": 385.1346,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_14.png",
        "height": 147,
        "width": 192,
        "x": 205.56,
        "y": 385.1346,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_15.png",
        "height": 147,
        "width": 192,
        "x": 303.12,
        "y": 385.1346,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_16.png",
        "height": 147,
        "width": 192,
        "x": 400.56,
        "y": 385.1346,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_17.png",
        "height": 147,
        "width": 192,
        "x": 108,
        "y": 458.8145999999999,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_18.png",
        "height": 147,
        "width": 192,
        "x": 205.56,
        "y": 458.8145999999999,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_19.png",
        "height": 147,
        "width": 192,
        "x": 303.12,
        "y": 458.8145999999999,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_20.png",
        "height": 147,
        "width": 192,
        "x": 400.56,
        "y": 458.8145999999999,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_21.png",
        "height": 147,
        "width": 192,
        "x": 108,
        "y": 532.4946,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_22.png",
        "height": 147,
        "width": 192,
        "x": 205.56,
        "y": 532.4946,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_23.png",
        "height": 147,
        "width": 192,
        "x": 303.12,
        "y": 532.4946,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "img_p20_24.png",
        "height": 147,
        "width": 192,
        "x": 400.56,
        "y": 532.4946,
        "original_width": 1650,
        "original_height": 1260
      },
      {
        "name": "page_21.jpg",
        "height": 792,
        "width": 612,
        "x": 0,
        "y": 0,
        "type": "full_page_screenshot"
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Normalized Transformer",
        "md": "# Normalized Transformer",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 99.32,
          "h": 9.96
        }
      },
      {
        "type": "text",
        "value": "![Figure 14: The distribution of trainable eigen learning rates and scaling factors varies under different context lengths, model sizes, learning rates, and number of training tokens.]\n\nThe image contains a grid of 24 graphs arranged in 6 rows and 4 columns. Each row represents a different aspect of the Normalized Transformer, and each column represents a different variable being tested. The graphs show various distributions and patterns, with different colors representing different parameters or conditions.\n\nHere's a breakdown of the columns:\n\n1. αw: different lengths\n2. αw: different model sizes\n3. αw: different learning rates\n4. αw: different tokens\n\nEach row seems to be testing a different component or scaling factor (α, β, γ, δ, ε, ζ).\n\nThe graphs display various distribution shapes including bell curves, bimodal distributions, and more complex patterns. They use different color schemes to differentiate between conditions, such as context lengths, model sizes, or number of tokens.\n\nAt the bottom of the image, there's a caption that reads:\n\nFigure 14: The distribution of trainable eigen learning rates and scaling factors varies under different context lengths, model sizes, learning rates, and number of training tokens. If not specified, we use context length 1K, model size 0.5B, learning rate 2.0 × 10^-3, and training tokens 52B as default.\n\nThis figure demonstrates how various parameters of the Normalized Transformer model change under different conditions, providing insights into the model's behavior and adaptability across different configurations.",
        "md": "![Figure 14: The distribution of trainable eigen learning rates and scaling factors varies under different context lengths, model sizes, learning rates, and number of training tokens.]\n\nThe image contains a grid of 24 graphs arranged in 6 rows and 4 columns. Each row represents a different aspect of the Normalized Transformer, and each column represents a different variable being tested. The graphs show various distributions and patterns, with different colors representing different parameters or conditions.\n\nHere's a breakdown of the columns:\n\n1. αw: different lengths\n2. αw: different model sizes\n3. αw: different learning rates\n4. αw: different tokens\n\nEach row seems to be testing a different component or scaling factor (α, β, γ, δ, ε, ζ).\n\nThe graphs display various distribution shapes including bell curves, bimodal distributions, and more complex patterns. They use different color schemes to differentiate between conditions, such as context lengths, model sizes, or number of tokens.\n\nAt the bottom of the image, there's a caption that reads:\n\nFigure 14: The distribution of trainable eigen learning rates and scaling factors varies under different context lengths, model sizes, learning rates, and number of training tokens. If not specified, we use context length 1K, model size 0.5B, learning rate 2.0 × 10^-3, and training tokens 52B as default.\n\nThis figure demonstrates how various parameters of the Normalized Transformer model change under different conditions, providing insights into the model's behavior and adaptability across different configurations.",
        "bBox": {
          "x": 108,
          "y": 36,
          "w": 396.25,
          "h": 9.96
        }
      }
    ],
    "status": "OK",
    "links": []
  }
]